{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Module description for wSAA classes\n",
    "output-file: wsaa.html\n",
    "title: wSAA\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9df2e7-fdc8-44e7-b16c-d8b15ed01db9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451e786a-e401-43d8-91e9-97c175eba23a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bd1d-ceaa-4b36-a42e-aaa2e4062c22",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagu/.conda/envs/dddex39/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex39/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex39/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex39/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex39/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n",
      "  else: warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='squared_error',\n",
       ">                        max_depth=None, min_samples_split=2,\n",
       ">                        min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
       ">                        max_features=1.0, max_leaf_nodes=None,\n",
       ">                        min_impurity_decrease=0.0, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "*A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "For a comparison between tree-based ensemble models see the example\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | squared_error | The function to measure the quality of a split. Supported criteria<br>are \"squared_error\" for the mean squared error, which is equal to<br>variance reduction as feature selection criterion and minimizes the L2<br>loss using the mean of each terminal node, \"friedman_mse\", which uses<br>mean squared error with Friedman's improvement score for potential<br>splits, \"absolute_error\" for the mean absolute error, which minimizes<br>the L1 loss using the median of each terminal node, and \"poisson\" which<br>uses reduction in Poisson deviance to find splits.<br>Training using \"absolute_error\" is significantly slower<br>than when using \"squared_error\".<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion.<br><br>.. versionadded:: 1.0<br>   Poisson criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | float | 1.0 | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `max(1, int(max_features * n_features_in_))` features are considered at each<br>  split.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None or 1.0, then `max_features=n_features`.<br><br>.. note::<br>    The default of 1.0 is equivalent to bagged trees and more<br>    randomness can be achieved by setting smaller values, e.g. 0.3.<br><br>.. versionchanged:: 1.1<br>    The default of `max_features` changed from `\"auto\"` to 1.0.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | Whether to use out-of-bag samples to estimate the generalization score.<br>By default, :func:`~sklearn.metrics.r2_score` is used.<br>Provide a callable with signature `metric(y_true, y_pred)` to use a<br>custom metric. Only available if `bootstrap=True`. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`Glossary <warm_start>` and<br>:ref:`gradient_boosting_warm_start` for details. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,<br>  `max_samples` should be in the interval `(0.0, 1.0]`.<br><br>.. versionadded:: 0.22 |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L27){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='squared_error',\n",
       ">                        max_depth=None, min_samples_split=2,\n",
       ">                        min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
       ">                        max_features=1.0, max_leaf_nodes=None,\n",
       ">                        min_impurity_decrease=0.0, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "*A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "For a comparison between tree-based ensemble models see the example\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | squared_error | The function to measure the quality of a split. Supported criteria<br>are \"squared_error\" for the mean squared error, which is equal to<br>variance reduction as feature selection criterion and minimizes the L2<br>loss using the mean of each terminal node, \"friedman_mse\", which uses<br>mean squared error with Friedman's improvement score for potential<br>splits, \"absolute_error\" for the mean absolute error, which minimizes<br>the L1 loss using the median of each terminal node, and \"poisson\" which<br>uses reduction in Poisson deviance to find splits.<br>Training using \"absolute_error\" is significantly slower<br>than when using \"squared_error\".<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion.<br><br>.. versionadded:: 1.0<br>   Poisson criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | float | 1.0 | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `max(1, int(max_features * n_features_in_))` features are considered at each<br>  split.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None or 1.0, then `max_features=n_features`.<br><br>.. note::<br>    The default of 1.0 is equivalent to bagged trees and more<br>    randomness can be achieved by setting smaller values, e.g. 0.3.<br><br>.. versionchanged:: 1.1<br>    The default of `max_features` changed from `\"auto\"` to 1.0.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | Whether to use out-of-bag samples to estimate the generalization score.<br>By default, :func:`~sklearn.metrics.r2_score` is used.<br>Provide a callable with signature `metric(y_true, y_pred)` to use a<br>custom metric. Only available if `bootstrap=True`. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`Glossary <warm_start>` and<br>:ref:`gradient_boosting_warm_start` for details. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,<br>  `max_samples` should be in the interval `(0.0, 1.0]`.<br><br>.. versionadded:: 0.22 |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RandomForestWSAA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfff03a",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc0602",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# #| export \n",
    "\n",
    "# # We attempt here to speed up the computation of the weights by interpreting every single\n",
    "# # tree as a lookup table. This way we don't have to compare the leaf-Indices arrays of each\n",
    "# # training sample and each test sample.\n",
    "# # Unfortunately, despite the fact that this strategy works very well for a single tree,\n",
    "# # it doesn't work for the whole forest because the structure of the output of the lookup \n",
    "# # tables per tree makes it difficult to aggregate the received weights per tree \n",
    "# # over all trees.\n",
    "\n",
    "# class RandomForestWSAA2(RandomForestRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "#     def fit(self, \n",
    "#             X: np.ndarray, # Feature matrix\n",
    "#             y: np.ndarray, # Target values\n",
    "#             **kwargs):\n",
    "\n",
    "#         super().fit(X = X, \n",
    "#                     y = y, \n",
    "#                     **kwargs)\n",
    "        \n",
    "#         self.yTrain = y\n",
    "        \n",
    "#         leafIndices = self.apply(X)\n",
    "\n",
    "#         indicesPerBinPerTree = list()\n",
    "\n",
    "#         for indexTree in range(self.n_estimators):\n",
    "#             leafIndicesPerTree = leafIndices[:, indexTree]\n",
    "\n",
    "#             indicesPerBin = defaultdict(list)\n",
    "\n",
    "#             for index, leafIndex in enumerate(leafIndicesPerTree):\n",
    "#                 indicesPerBin[leafIndex].append(index)\n",
    "\n",
    "#             indicesPerBinPerTree.append(indicesPerBin)\n",
    "        \n",
    "#         self.indicesPerBinPerTree = indicesPerBinPerTree\n",
    "\n",
    "        \n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def getWeights(self, \n",
    "#                    X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "#                    # Specifies structure of the returned density estimates. One of: \n",
    "#                    # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "#                    outputType: str='onlyPositiveWeights', \n",
    "#                    # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "#                    # density of each sample for scaling purposes.\n",
    "#                    scalingList: list=None, \n",
    "#                    ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "#         __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         leafIndicesPerTree = self.apply(X)\n",
    "        \n",
    "#         weightsDataList = list()\n",
    "\n",
    "#         for leafIndices in leafIndicesPerTree:\n",
    "            \n",
    "#             weights = np.zeros(self.yTrain.shape[0])\n",
    "\n",
    "#             for indexTree in range(len(leafIndices)):\n",
    "#                 indicesPosWeight = self.indicesPerBinPerTree[indexTree][leafIndices[indexTree]]\n",
    "\n",
    "#                 weightsNew = np.zeros(self.yTrain.shape[0])\n",
    "#                 np.put(weightsNew, indicesPosWeight, 1 / len(indicesPosWeight))\n",
    "                \n",
    "#                 weights = weights + weightsNew\n",
    "\n",
    "#             weights = weights / len(leafIndices)\n",
    "\n",
    "#             weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "#             weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "#         #---\n",
    "\n",
    "#         # Check if self.yTrain is a 2D array with more than one column.\n",
    "#         if len(self.yTrain.shape) > 1:\n",
    "#             if self.yTrain.shape[1] > 1:\n",
    "\n",
    "#                 if not outputType in ['all', 'onlyPositiveWeights', 'summarized']:\n",
    "#                     raise ValueError(\"outputType must be one of 'all', 'onlyPositiveWeights', 'summarized' for multivariate y.\")\n",
    "                \n",
    "#                 weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "#                                                                         outputType = outputType, \n",
    "#                                                                         y = self.yTrain, \n",
    "#                                                                         scalingList = scalingList,\n",
    "#                                                                         equalWeights = False) \n",
    "            \n",
    "#         else:\n",
    "#             weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "#                                                         outputType = outputType, \n",
    "#                                                         y = self.yTrain, \n",
    "#                                                         scalingList = scalingList,\n",
    "#                                                         equalWeights = False)\n",
    "            \n",
    "        \n",
    "                \n",
    "\n",
    "#         return weightsDataList\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def predict(self: BaseWeightsBasedEstimator, \n",
    "#                 X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "#                 probs: list, # Probabilities for which quantiles are computed.\n",
    "#                 outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "#                 # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "#                 # of each sample to rescale values.\n",
    "#                 scalingList: list=None, \n",
    "#                 ): \n",
    "        \n",
    "#         __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "#         return super(MetaEstimatorMixin, self).predict(X = X,\n",
    "#                                                        probs = probs, \n",
    "#                                                        scalingList = scalingList)\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def pointPredict(self,\n",
    "#                      X: np.ndarray, # Feature Matrix\n",
    "#                      **kwargs):\n",
    "#         \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "#         return super().predict(X = X,\n",
    "#                                **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811aafa",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA_LGBM\n",
       "\n",
       ">      RandomForestWSAA_LGBM (boosting_type:str='gbdt', num_leaves:int=31,\n",
       ">                             max_depth:int=-1, learning_rate:float=0.1,\n",
       ">                             n_estimators:int=100,\n",
       ">                             subsample_for_bin:int=200000, objective:Union[str,\n",
       ">                             Callable[[Optional[numpy.ndarray],numpy.ndarray],T\n",
       ">                             uple[numpy.ndarray,numpy.ndarray]],Callable[[Optio\n",
       ">                             nal[numpy.ndarray],numpy.ndarray,Optional[numpy.nd\n",
       ">                             array]],Tuple[numpy.ndarray,numpy.ndarray]],Callab\n",
       ">                             le[[Optional[numpy.ndarray],numpy.ndarray,Optional\n",
       ">                             [numpy.ndarray],Optional[numpy.ndarray]],Tuple[num\n",
       ">                             py.ndarray,numpy.ndarray]],NoneType]=None,\n",
       ">                             class_weight:Union[Dict,str,NoneType]=None,\n",
       ">                             min_split_gain:float=0.0,\n",
       ">                             min_child_weight:float=0.001,\n",
       ">                             min_child_samples:int=20, subsample:float=1.0,\n",
       ">                             subsample_freq:int=0, colsample_bytree:float=1.0,\n",
       ">                             reg_alpha:float=0.0, reg_lambda:float=0.0, random_\n",
       ">                             state:Union[int,numpy.random.mtrand.RandomState,No\n",
       ">                             neType]=None, n_jobs:Optional[int]=None,\n",
       ">                             importance_type:str='split', **kwargs)\n",
       "\n",
       "*LightGBM regressor.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA_LGBM\n",
       "\n",
       ">      RandomForestWSAA_LGBM (boosting_type:str='gbdt', num_leaves:int=31,\n",
       ">                             max_depth:int=-1, learning_rate:float=0.1,\n",
       ">                             n_estimators:int=100,\n",
       ">                             subsample_for_bin:int=200000, objective:Union[str,\n",
       ">                             Callable[[Optional[numpy.ndarray],numpy.ndarray],T\n",
       ">                             uple[numpy.ndarray,numpy.ndarray]],Callable[[Optio\n",
       ">                             nal[numpy.ndarray],numpy.ndarray,Optional[numpy.nd\n",
       ">                             array]],Tuple[numpy.ndarray,numpy.ndarray]],Callab\n",
       ">                             le[[Optional[numpy.ndarray],numpy.ndarray,Optional\n",
       ">                             [numpy.ndarray],Optional[numpy.ndarray]],Tuple[num\n",
       ">                             py.ndarray,numpy.ndarray]],NoneType]=None,\n",
       ">                             class_weight:Union[Dict,str,NoneType]=None,\n",
       ">                             min_split_gain:float=0.0,\n",
       ">                             min_child_weight:float=0.001,\n",
       ">                             min_child_samples:int=20, subsample:float=1.0,\n",
       ">                             subsample_freq:int=0, colsample_bytree:float=1.0,\n",
       ">                             reg_alpha:float=0.0, reg_lambda:float=0.0, random_\n",
       ">                             state:Union[int,numpy.random.mtrand.RandomState,No\n",
       ">                             neType]=None, n_jobs:Optional[int]=None,\n",
       ">                             importance_type:str='split', **kwargs)\n",
       "\n",
       "*LightGBM regressor.*"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RandomForestWSAA_LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471bba6-50ec-49a9-980d-1c10f4361938",
   "metadata": {},
   "source": [
    "## SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L224){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SampleAverageApproximation\n",
       "\n",
       ">      SampleAverageApproximation ()\n",
       "\n",
       "*SAA is a featureless approach that assumes the density of the target variable is given\n",
       "by assigning equal probability to each historical observation of said target variable.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L224){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SampleAverageApproximation\n",
       "\n",
       ">      SampleAverageApproximation ()\n",
       "\n",
       "*SAA is a featureless approach that assumes the density of the target variable is given\n",
       "by assigning equal probability to each historical observation of said target variable.*"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(SampleAverageApproximation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f57a0f-e4d4-4655-858c-d6319f087936",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# import lightgbm as lgb\n",
    "# from dddex.loadData import *\n",
    "# from datasetsDynamic.loadDataYaz import loadDataYaz\n",
    "# import ipdb\n",
    "# import inspect\n",
    "# from sklearn.base import RegressorMixin\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataBakery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50a254",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72300, 227)\n",
      "(72300, 7)\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 14,\n",
    "#                                                  daysToCut = 0,\n",
    "#                                                  normalizeDemand = True,\n",
    "#                                                  unstacked = True,\n",
    "#                                                  returnXY = True)\n",
    "\n",
    "# # RF = RandomForestRegressor(n_estimators = 10, n_jobs = 1, max_depth = 3)\n",
    "# # RF.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "# # Duplicate XTrain and yTrain m times\n",
    "# m = 100\n",
    "# XTrain = np.vstack([XTrain for i in range(m)])\n",
    "# yTrain = np.vstack([yTrain for i in range(m)])\n",
    "\n",
    "# print(XTrain.shape)\n",
    "# print(yTrain.shape)\n",
    "\n",
    "# # Add gaussian to XTrain and yTrain\n",
    "# XTrain = XTrain + np.random.normal(0, 0.1, XTrain.shape)\n",
    "# yTrain = yTrain + np.random.normal(0, 0.1, yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0a068",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# RFWSAA = RandomForestWSAA(n_estimators = 10, n_jobs = 1, max_depth = 4)\n",
    "# RFWSAA.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a01f60",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# RFWSAA2 = RandomForestWSAA2(n_estimators = 10, max_depth = 4, n_jobs = 1)\n",
    "# RFWSAA2.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4515e12",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3c0fe",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 2.92 s, total: 18.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# weights = RFWSAA.getWeights(X = XTrain[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58be9c9",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 1.44 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# weights2 = RFWSAA2.getWeights(X = XTrain[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb9dfd",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# RF.apply(XTrain).shape\n",
    "\n",
    "# indicesPerBinPerTree = list()\n",
    "\n",
    "# for indexTree, tree in enumerate(RF.estimators_):\n",
    "#     leafIndicesTrain = tree.apply(XTrain)\n",
    "\n",
    "#     indicesPerBin = defaultdict(list)\n",
    "\n",
    "#     for index, leafIndex in enumerate(leafIndicesTrain):\n",
    "#         indicesPerBin[leafIndex].append(index)\n",
    "\n",
    "#     indicesPerBinPerTree.append(indicesPerBin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2880-1b9e-4d07-ab84-4ba8a2bef226",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# RF = RandomForestWSAA_LGBM(max_depth = 2,\n",
    "#                            n_estimators = 10,\n",
    "#                            n_jobs = 1,\n",
    "#                            boosting_type = 'rf',\n",
    "#                            subsample_freq = 1,\n",
    "#                            subsample = 0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
