{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Module description for wSAA classes\n",
    "output-file: wsaa.html\n",
    "title: wSAA\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9df2e7-fdc8-44e7-b16c-d8b15ed01db9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "451e786a-e401-43d8-91e9-97c175eba23a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bd1d-ceaa-4b36-a42e-aaa2e4062c22",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kagu/.conda/envs/dddex/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Attributes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n",
      "  else: warn(msg)\n",
      "/home/kagu/.conda/envs/dddex/lib/python3.8/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n",
      "  else: warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='squared_error',\n",
       ">                        max_depth=None, min_samples_split=2,\n",
       ">                        min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
       ">                        max_features=1.0, max_leaf_nodes=None,\n",
       ">                        min_impurity_decrease=0.0, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | squared_error | The function to measure the quality of a split. Supported criteria<br>are \"squared_error\" for the mean squared error, which is equal to<br>variance reduction as feature selection criterion and minimizes the L2<br>loss using the mean of each terminal node, \"friedman_mse\", which uses<br>mean squared error with Friedman's improvement score for potential<br>splits, \"absolute_error\" for the mean absolute error, which minimizes<br>the L1 loss using the median of each terminal node, and \"poisson\" which<br>uses reduction in Poisson deviance to find splits.<br>Training using \"absolute_error\" is significantly slower<br>than when using \"squared_error\".<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion.<br><br>.. versionadded:: 1.0<br>   Poisson criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | float | 1.0 | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `max(1, int(max_features * n_features_in_))` features are considered at each<br>  split.<br>- If \"auto\", then `max_features=n_features`.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None or 1.0, then `max_features=n_features`.<br><br>.. note::<br>    The default of 1.0 is equivalent to bagged trees and more<br>    randomness can be achieved by setting smaller values, e.g. 0.3.<br><br>.. versionchanged:: 1.1<br>    The default of `max_features` changed from `\"auto\"` to 1.0.<br><br>.. deprecated:: 1.1<br>    The `\"auto\"` option was deprecated in 1.1 and will be removed<br>    in 1.3.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | Whether to use out-of-bag samples to estimate the generalization score.<br>Only available if bootstrap=True. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`Glossary <warm_start>` and<br>:ref:`gradient_boosting_warm_start` for details. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max_samples * X.shape[0]` samples. Thus,<br>  `max_samples` should be in the interval `(0.0, 1.0]`.<br><br>.. versionadded:: 0.22 |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RandomForestWSAA\n",
       "\n",
       ">      RandomForestWSAA (n_estimators=100, criterion='squared_error',\n",
       ">                        max_depth=None, min_samples_split=2,\n",
       ">                        min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
       ">                        max_features=1.0, max_leaf_nodes=None,\n",
       ">                        min_impurity_decrease=0.0, bootstrap=True,\n",
       ">                        oob_score=False, n_jobs=None, random_state=None,\n",
       ">                        verbose=0, warm_start=False, ccp_alpha=0.0,\n",
       ">                        max_samples=None)\n",
       "\n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_estimators | int | 100 | The number of trees in the forest.<br><br>.. versionchanged:: 0.22<br>   The default value of ``n_estimators`` changed from 10 to 100<br>   in 0.22. |\n",
       "| criterion | str | squared_error | The function to measure the quality of a split. Supported criteria<br>are \"squared_error\" for the mean squared error, which is equal to<br>variance reduction as feature selection criterion and minimizes the L2<br>loss using the mean of each terminal node, \"friedman_mse\", which uses<br>mean squared error with Friedman's improvement score for potential<br>splits, \"absolute_error\" for the mean absolute error, which minimizes<br>the L1 loss using the median of each terminal node, and \"poisson\" which<br>uses reduction in Poisson deviance to find splits.<br>Training using \"absolute_error\" is significantly slower<br>than when using \"squared_error\".<br><br>.. versionadded:: 0.18<br>   Mean Absolute Error (MAE) criterion.<br><br>.. versionadded:: 1.0<br>   Poisson criterion. |\n",
       "| max_depth | NoneType | None | The maximum depth of the tree. If None, then nodes are expanded until<br>all leaves are pure or until all leaves contain less than<br>min_samples_split samples. |\n",
       "| min_samples_split | int | 2 | The minimum number of samples required to split an internal node:<br><br>- If int, then consider `min_samples_split` as the minimum number.<br>- If float, then `min_samples_split` is a fraction and<br>  `ceil(min_samples_split * n_samples)` are the minimum<br>  number of samples for each split.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_samples_leaf | int | 1 | The minimum number of samples required to be at a leaf node.<br>A split point at any depth will only be considered if it leaves at<br>least ``min_samples_leaf`` training samples in each of the left and<br>right branches.  This may have the effect of smoothing the model,<br>especially in regression.<br><br>- If int, then consider `min_samples_leaf` as the minimum number.<br>- If float, then `min_samples_leaf` is a fraction and<br>  `ceil(min_samples_leaf * n_samples)` are the minimum<br>  number of samples for each node.<br><br>.. versionchanged:: 0.18<br>   Added float values for fractions. |\n",
       "| min_weight_fraction_leaf | float | 0.0 | The minimum weighted fraction of the sum total of weights (of all<br>the input samples) required to be at a leaf node. Samples have<br>equal weight when sample_weight is not provided. |\n",
       "| max_features | float | 1.0 | The number of features to consider when looking for the best split:<br><br>- If int, then consider `max_features` features at each split.<br>- If float, then `max_features` is a fraction and<br>  `max(1, int(max_features * n_features_in_))` features are considered at each<br>  split.<br>- If \"auto\", then `max_features=n_features`.<br>- If \"sqrt\", then `max_features=sqrt(n_features)`.<br>- If \"log2\", then `max_features=log2(n_features)`.<br>- If None or 1.0, then `max_features=n_features`.<br><br>.. note::<br>    The default of 1.0 is equivalent to bagged trees and more<br>    randomness can be achieved by setting smaller values, e.g. 0.3.<br><br>.. versionchanged:: 1.1<br>    The default of `max_features` changed from `\"auto\"` to 1.0.<br><br>.. deprecated:: 1.1<br>    The `\"auto\"` option was deprecated in 1.1 and will be removed<br>    in 1.3.<br><br>Note: the search for a split does not stop until at least one<br>valid partition of the node samples is found, even if it requires to<br>effectively inspect more than ``max_features`` features. |\n",
       "| max_leaf_nodes | NoneType | None | Grow trees with ``max_leaf_nodes`` in best-first fashion.<br>Best nodes are defined as relative reduction in impurity.<br>If None then unlimited number of leaf nodes. |\n",
       "| min_impurity_decrease | float | 0.0 | A node will be split if this split induces a decrease of the impurity<br>greater than or equal to this value.<br><br>The weighted impurity decrease equation is the following::<br><br>    N_t / N * (impurity - N_t_R / N_t * right_impurity<br>                        - N_t_L / N_t * left_impurity)<br><br>where ``N`` is the total number of samples, ``N_t`` is the number of<br>samples at the current node, ``N_t_L`` is the number of samples in the<br>left child, and ``N_t_R`` is the number of samples in the right child.<br><br>``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,<br>if ``sample_weight`` is passed.<br><br>.. versionadded:: 0.19 |\n",
       "| bootstrap | bool | True | Whether bootstrap samples are used when building trees. If False, the<br>whole dataset is used to build each tree. |\n",
       "| oob_score | bool | False | Whether to use out-of-bag samples to estimate the generalization score.<br>Only available if bootstrap=True. |\n",
       "| n_jobs | NoneType | None | The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,<br>:meth:`decision_path` and :meth:`apply` are all parallelized over the<br>trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`<br>context. ``-1`` means using all processors. See :term:`Glossary<br><n_jobs>` for more details. |\n",
       "| random_state | NoneType | None | Controls both the randomness of the bootstrapping of the samples used<br>when building trees (if ``bootstrap=True``) and the sampling of the<br>features to consider when looking for the best split at each node<br>(if ``max_features < n_features``).<br>See :term:`Glossary <random_state>` for details. |\n",
       "| verbose | int | 0 | Controls the verbosity when fitting and predicting. |\n",
       "| warm_start | bool | False | When set to ``True``, reuse the solution of the previous call to fit<br>and add more estimators to the ensemble, otherwise, just fit a whole<br>new forest. See :term:`Glossary <warm_start>` and<br>:ref:`gradient_boosting_warm_start` for details. |\n",
       "| ccp_alpha | float | 0.0 | Complexity parameter used for Minimal Cost-Complexity Pruning. The<br>subtree with the largest cost complexity that is smaller than<br>``ccp_alpha`` will be chosen. By default, no pruning is performed. See<br>:ref:`minimal_cost_complexity_pruning` for details.<br><br>.. versionadded:: 0.22 |\n",
       "| max_samples | NoneType | None | If bootstrap is True, the number of samples to draw from X<br>to train each base estimator.<br><br>- If None (default), then draw `X.shape[0]` samples.<br>- If int, then draw `max_samples` samples.<br>- If float, then draw `max_samples * X.shape[0]` samples. Thus,<br>  `max_samples` should be in the interval `(0.0, 1.0]`.<br><br>.. versionadded:: 0.22 |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(RandomForestWSAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db18d0f-bdb7-4f08-a8cf-ac07a49e7d2c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a240e3b-5acc-4f82-8f4d-29391f5692b7",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe067-f51e-4038-ae00-06bafa7ca014",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471bba6-50ec-49a9-980d-1c10f4361938",
   "metadata": {},
   "source": [
    "## SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L111){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SampleAverageApproximation\n",
       "\n",
       ">      SampleAverageApproximation ()\n",
       "\n",
       "SAA is a featureless approach that assumes the density of the target variable is given\n",
       "by assigning equal probability to each historical observation of said target variable."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/wSAA.py#L111){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SampleAverageApproximation\n",
       "\n",
       ">      SampleAverageApproximation ()\n",
       "\n",
       "SAA is a featureless approach that assumes the density of the target variable is given\n",
       "by assigning equal probability to each historical observation of said target variable."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(SampleAverageApproximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc67de5-0fcb-47b3-82dd-908cbedaae31",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9520933-6480-4e96-bace-8b9f164cd922",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e77e0d-5cb8-4c29-b356-0f0e61bccde0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f57a0f-e4d4-4655-858c-d6319f087936",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HC-Scheduling",
   "language": "python",
   "name": "hc-scheduling"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
