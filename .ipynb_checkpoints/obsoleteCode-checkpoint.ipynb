{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a09ff0-8cbb-4e1d-bb1f-f9d396169492",
   "metadata": {},
   "source": [
    "# LSx Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37539cbb-2cd7-4717-9ddb-53c3b8af70d8",
   "metadata": {},
   "source": [
    "## Packages + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc82c1-392a-4154-b971-c6d121ab6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict, Counter, deque\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import faiss\n",
    "\n",
    "from dddex.baseClasses import BaseLSx, BaseWeightsBasedEstimator\n",
    "from dddex.utils import restructureWeightsDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13ac5f-010c-4b72-ba90-bef86fe9341b",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on DRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7101af9-eda1-45c0-bf9f-fd1f16bd996c",
   "metadata": {},
   "source": [
    "The LSx approach based on Distributional Random Forest (DRF) has been dropped due to the drf package causing problems for new users during the installation of our dddex package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6ddd6-0a8e-4aac-8047-2d486606f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from drf import drf \n",
    "\n",
    "# class LevelSetKDEx_DRF(BaseWeightsBasedEstimator, BaseLSx):\n",
    "#     \"\"\"\n",
    "#     `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "#     The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "#     as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "#     recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "#     a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "#     would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "#     of this test sample.    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "#                  binSize: int=100, # Size of the bins created while running fit.\n",
    "#                  ):\n",
    "        \n",
    "#         super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "#         # Check if binSize is integer\n",
    "#         if not isinstance(binSize, (int, np.int32, np.int64)):\n",
    "#             raise ValueError(\"'binSize' must be an integer!\")\n",
    "\n",
    "#         self.binSize = binSize\n",
    "        \n",
    "#         self.yTrain = None\n",
    "#         self.yPredTrain = None\n",
    "#         self.drf = None\n",
    "#         self.fitted = False\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def fit(self: LevelSetKDEx_DRF, \n",
    "#             X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "#             y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "#             ):\n",
    "#         \"\"\"\n",
    "#         Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "#         according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "#         `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "#         heavy lifting.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Checks\n",
    "#         if not isinstance(self.binSize, (int, np.int32, np.int64)):\n",
    "#             raise ValueError(\"'binSize' must be an integer!\")\n",
    "            \n",
    "#         if self.binSize > y.shape[0]:\n",
    "#             raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "#         if X.shape[0] != y.shape[0]:\n",
    "#             raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         try:\n",
    "#             yPred = self.estimator.predict(X)\n",
    "            \n",
    "#         except NotFittedError:\n",
    "#             try:\n",
    "#                 self.estimator.fit(X = X, y = y)                \n",
    "#             except:\n",
    "#                 raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "#             else:\n",
    "#                 yPred = self.estimator.predict(X)\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         yPred = pd.DataFrame(yPred)\n",
    "#         y = pd.Series(y)\n",
    "\n",
    "#         DRF = drf(min_node_size = self.binSize, num_trees = 100, num_features = 1, honesty = False, sample_fraction = 0.5, response_scaling = False, mtry = 1, num_threads = 16)\n",
    "#         DRF.fit(X = yPred, Y = y)\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "#         # problems later on.\n",
    "#         self.yTrain = y.ravel()\n",
    "        \n",
    "#         self.yPredTrain = yPred\n",
    "#         self.drf = DRF\n",
    "#         self.fitted = True\n",
    "        \n",
    "#     #---\n",
    "    \n",
    "#     def getWeights(self: LevelSetKDEx_DRF, \n",
    "#                    X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "#                    # Specifies structure of the returned density estimates. One of: \n",
    "#                    # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "#                    outputType: str='onlyPositiveWeights', \n",
    "#                    # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "#                    # density of each sample for scaling purposes.\n",
    "#                    scalingList: list=None, \n",
    "#                    ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "#         # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "#         __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "#         if not self.fitted:\n",
    "#             raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "#                                  \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         yPred = self.estimator.predict(X)\n",
    "#         yPred = pd.DataFrame(yPred)\n",
    "        \n",
    "#         weightsArray = self.drf.predict(yPred).weights\n",
    "#         weightsList = list(weightsArray)\n",
    "#         weightsDataList = [(weights[weights > 0], np.where(weights > 0)[0]) for weights in weightsList]\n",
    "\n",
    "#         weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "#                                                      outputType = outputType, \n",
    "#                                                      y = self.yTrain,\n",
    "#                                                      scalingList = scalingList,\n",
    "#                                                      equalWeights = True)\n",
    "        \n",
    "#         return weightsDataList\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bfc33-c825-4ae7-9875-2667970c3982",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Level-Set Approach based on Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372ff552-4e28-4e74-a78a-21d3baf8a6f1",
   "metadata": {},
   "source": [
    "I simply can't see any need for a method like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671f37a-6106-4629-b1a5-f9aec24f54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelSetKDEx_clustering(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 nClusters: int=10, # Number of clusters to form as well as number of centroids to generate.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(nClusters, (np.int32, np.int64)):\n",
    "            nClusters = int(nClusters)\n",
    "        \n",
    "        elif not isinstance(nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "                \n",
    "        self.nClusters = nClusters\n",
    "\n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.kmeans = None\n",
    "        self.clusterDict = None\n",
    "        self.clusterSizes = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.nClusters is None:\n",
    "            raise ValueError(\"'nClusters' must be specified to fit the LSx estimator!\")\n",
    "        \n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(self.nClusters, (np.int32, np.int64)):\n",
    "            self.nClusters = int(self.nClusters)\n",
    "        \n",
    "        elif not isinstance(self.nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "        \n",
    "        # Check if nClusters is positive\n",
    "        if self.nClusters <= 0:\n",
    "            raise ValueError(\"'nClusters' must be positive!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # Reshape yPred to 2D array with shape (nSamples, 1) and convert to float32.\n",
    "        yPredMod = yPred.reshape(-1, 1).astype(np.float32)\n",
    "        \n",
    "        kmeans = faiss.Kmeans(d = 1, k = self.nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "        # self.centers = kmeans.centroids\n",
    "        \n",
    "        clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        # Get the cluster labels for each sample in yPred as a dict with keys being the cluster labels\n",
    "        # and values being the indices of the samples in yPred that belong to that cluster.\n",
    "        # Create another dict with keys being the cluster labels and values being the size of each cluster.\n",
    "        clusterDict = defaultdict(list)\n",
    "        clusterSizes = defaultdict(int)\n",
    "\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            clusterDict[cluster].append(index)\n",
    "            clusterSizes[cluster] += 1\n",
    "        \n",
    "        clusterSizes = pd.Series(clusterSizes)\n",
    "       \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.kmeans = kmeans\n",
    "        self.clusterDict = clusterDict\n",
    "        self.clusterSizes = clusterSizes\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        # Reshape yPred to 2D array with shape (nSamples, 1) and convert to float32.\n",
    "        yPred = yPred.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        # Get cluster labels of yPred\n",
    "        clusterLabels = self.kmeans.assign(yPred)[1]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / self.clusterSizes[cluster], self.clusterSizes[cluster]), np.array(self.clusterDict[cluster], dtype = 'uintc')) \n",
    "                            for cluster in clusterLabels]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7be57-4668-4bbd-b189-69286202e3c9",
   "metadata": {},
   "source": [
    "# LSx Multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88093080-b8b4-460c-9872-be958684d301",
   "metadata": {},
   "source": [
    "## Packages + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480dad5-e9ba-4a2a-a22c-c1a5b6b05fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from dddex.baseClasses import BaseLSx, BaseWeightsBasedEstimator_multivariate\n",
    "from dddex.levelSetKDEx_univariate import generateBins\n",
    "from dddex.wSAA import SampleAverageApproximation, RandomForestWSAA, RandomForestWSAA_LGBM\n",
    "from dddex.utils import restructureWeightsDataList_multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb24b6-358d-4442-84fd-943df019a2f6",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cafcae-d718-4172-bde2-3ac3a2a81c4c",
   "metadata": {},
   "source": [
    "LevelSetKDEx_multivariate is an older version of the currently used partition-based LSx model for multivariate point predictors. Both approaches use faiss-kMeans to create a partition of the target space based on the point forecasts. The difference between the two approaches is how we merge clusters in the subsequent steps to ensure that each cluster is big enough. Here, we simply merge each cluster that is too small (here defined as clusterSize < binSize / 2) with the next cluster and generate a bigger one. In the newer version, we don't actually merge them, but instead only take the observations of the next cluster into account when computing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef236b-bb2e-40b6-b2f2-1011d84d5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelSetKDEx_multivariate(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=None, # Size of the bins created while running fit.\n",
    "                 # Determines behaviour of method `getWeights`. If False, all weights receive the same  \n",
    "                 # value. If True, the distance of the point forecasts is taking into account.\n",
    "                 equalBins: bool=False,\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "        \n",
    "        # Check if binSize is int\n",
    "        if not isinstance(binSize, int):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "        \n",
    "        # Check if equalBins is bool\n",
    "        if not isinstance(equalBins, bool):\n",
    "            raise ValueError(\"'equalBins' must be a boolean!\")\n",
    "        \n",
    "        self.equalBins = equalBins\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.binSize is None:\n",
    "            raise ValueError(\"'binSize' must be specified to fit the LSx estimator!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # Compute desired number of clusters dependend on binSize and number of samples\n",
    "        nClusters = int(np.ceil(yPred.shape[0] / self.binSize))\n",
    "\n",
    "        # Modify yPred to be compatible with faiss\n",
    "        yPredMod = yPred.astype(np.float32)\n",
    "        \n",
    "        # Train kmeans model based on the faiss library\n",
    "        kmeans = faiss.Kmeans(d = yPredMod.shape[1], k = nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "\n",
    "        # Get cluster centers created by faiss. IMPORTANT NOTE: not all clusters are used! We will handle that further below.\n",
    "        centersAll = kmeans.centroids\n",
    "        \n",
    "        # Compute the cluster assignment for each sample\n",
    "        if self.equalBins:\n",
    "            clusterAssignments = self._getEqualSizedClusters(y = yPredMod)            \n",
    "        else:\n",
    "            clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        # Based on the clusters and cluster assignments, we can now compute the indices belonging to each bin / cluster\n",
    "        indicesPerBin = defaultdict(list)\n",
    "        binSizes = defaultdict(int)\n",
    "\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            indicesPerBin[cluster].append(index)\n",
    "            binSizes[cluster] += 1\n",
    "\n",
    "        #---\n",
    "\n",
    "        clustersUsed = np.array(list(indicesPerBin.keys()))\n",
    "        clustersOrdered = np.sort(clustersUsed)\n",
    "\n",
    "        centers = centersAll[clustersOrdered]\n",
    "        indicesPerBin = [indicesPerBin[cluster] for cluster in clustersOrdered]\n",
    "        binSizes = np.array([binSizes[cluster] for cluster in clustersOrdered])\n",
    "\n",
    "        #---\n",
    "\n",
    "        # Merge clusters that are too small (i.e. contain less than binSize / 2 samples).\n",
    "        # clustersTooSmall is the array of all clusters that are too small.\n",
    "        threshold = self.binSize / 2\n",
    "        binsTooSmall = np.where(binSizes < threshold)[0]\n",
    "        \n",
    "        if len(binsTooSmall) > 0:\n",
    "\n",
    "            # remove all centers from centersOld that are part of clustersTooSmall\n",
    "            centersNew = np.delete(centers, binsTooSmall, axis = 0)\n",
    "            centersTooSmall = centers[binsTooSmall]\n",
    "            centersNew_oldIndices = np.delete(np.arange(len(centers)), binsTooSmall)\n",
    "\n",
    "            KDTreeNew = KDTree(centersNew)\n",
    "            clustersToMerge = KDTreeNew.query(centersTooSmall)[1]\n",
    "\n",
    "            for i, clusterToMerge in enumerate(clustersToMerge):\n",
    "                indicesPerBin[centersNew_oldIndices[clusterToMerge]].extend(indicesPerBin[binsTooSmall[i]])\n",
    "\n",
    "            # Remove the indices given by clustersTooSmall from indicesPerBin by deleting the list entry\n",
    "            indicesPerBin = [np.array(indices) for binIndex, indices in enumerate(indicesPerBin) if binIndex not in binsTooSmall]\n",
    "            binSizes = [len(indices) for indices in indicesPerBin]\n",
    "            binSizes = pd.Series(binSizes)\n",
    "\n",
    "            self.centers = centersNew\n",
    "            self.binSizes = binSizes\n",
    "            self.kmeans = KDTreeNew\n",
    "        \n",
    "        else:\n",
    "            self.centers = centers\n",
    "            self.binSizes = pd.Series(binSizes)\n",
    "            self.kmeans = KDTree(self.centers)\n",
    "\n",
    "            # Transform the indices given by indicesPerBin into numpy arrays\n",
    "            indicesPerBin = [np.array(indices) for indices in indicesPerBin]\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.fitted = True\n",
    "        \n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def _getEqualSizedClusters(self,\n",
    "                               y,\n",
    "                               ):\n",
    "            \n",
    "        centers = self.centers.reshape(-1, 1, y.shape[-1]).repeat(self.binSize, 1).reshape(-1, y.shape[-1])\n",
    "\n",
    "        distance_matrix = cdist(y, centers)\n",
    "        clusterAssignments = linear_sum_assignment(distance_matrix)[1]//self.binSize\n",
    "\n",
    "        return clusterAssignments\n",
    "\n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X).astype(np.float32)\n",
    "        \n",
    "        if len(yPred.shape) == 1:\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        if self.equalBins:\n",
    "            binPerPred = self._getEqualSizedClusters(y = yPred)\n",
    "            \n",
    "        else:\n",
    "            binPerPred = self.kmeans.query(yPred)[1]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        neighborsList = [self.indicesPerBin[binIndex] for binIndex in binPerPred]\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef55b4-11aa-44e9-8d0f-61a9834b61d1",
   "metadata": {},
   "source": [
    "# wSAA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783ea5f-0c6f-42cb-9dfa-546df475ca60",
   "metadata": {},
   "source": [
    "## Packages + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8776970-08fa-45b4-913e-63dc0e21a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from lightgbm.sklearn import LGBMModel\n",
    "from dddex.baseClasses import BaseWeightsBasedEstimator\n",
    "from dddex.utils import restructureWeightsDataList, restructureWeightsDataList_multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c174f-4fd0-48d8-a2a2-02f9cb727424",
   "metadata": {},
   "source": [
    "## Random Forest Speed Up by Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc5e99-b395-40a7-9033-e0f0804a4c32",
   "metadata": {},
   "source": [
    "The weights calculation of single trees can be speed up massively by viewing the tree as a partition algorithm. As such, we can simply use a lookup table for the calculation of the weights once we know into which leaf-node a new observation falls. Here, we tried to apply this idea to Random Forest as well. Sadly, it turns out that the assembling of the weights of each single tree into a single set of weights is more computationally expensive than the old way of calculating the weights for a Random Forest model. Maybe we can come up with a solution in the future. It might be worth looking into the R code of the Distributional Random Forest package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3d34b-1dde-492d-a4c0-778eb763f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attempt here to speed up the computation of the weights by interpreting every single\n",
    "# tree as a lookup table. This way we don't have to compare the leaf-Indices arrays of each\n",
    "# training sample and each test sample.\n",
    "# Unfortunately, despite the fact that this strategy works very well for a single tree,\n",
    "# it doesn't work for the whole forest because the structure of the output of the lookup \n",
    "# tables per tree makes it difficult to aggregate the received weights per tree \n",
    "# over all trees.\n",
    "\n",
    "class RandomForestWSAA_speedup(RandomForestRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix\n",
    "            y: np.ndarray, # Target values\n",
    "            **kwargs):\n",
    "\n",
    "        super().fit(X = X, \n",
    "                    y = y, \n",
    "                    **kwargs)\n",
    "        \n",
    "        self.yTrain = y\n",
    "        \n",
    "        leafIndices = self.apply(X)\n",
    "\n",
    "        indicesPerBinPerTree = list()\n",
    "\n",
    "        for indexTree in range(self.n_estimators):\n",
    "            leafIndicesPerTree = leafIndices[:, indexTree]\n",
    "\n",
    "            indicesPerBin = defaultdict(list)\n",
    "\n",
    "            for index, leafIndex in enumerate(leafIndicesPerTree):\n",
    "                indicesPerBin[leafIndex].append(index)\n",
    "\n",
    "            indicesPerBinPerTree.append(indicesPerBin)\n",
    "        \n",
    "        self.indicesPerBinPerTree = indicesPerBinPerTree\n",
    "\n",
    "        \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        leafIndicesPerTree = self.apply(X)\n",
    "        \n",
    "        weightsDataList = list()\n",
    "\n",
    "        for leafIndices in leafIndicesPerTree:\n",
    "            \n",
    "            weights = np.zeros(self.yTrain.shape[0])\n",
    "\n",
    "            for indexTree in range(len(leafIndices)):\n",
    "                indicesPosWeight = self.indicesPerBinPerTree[indexTree][leafIndices[indexTree]]\n",
    "\n",
    "                weightsNew = np.zeros(self.yTrain.shape[0])\n",
    "                np.put(weightsNew, indicesPosWeight, 1 / len(indicesPosWeight))\n",
    "                \n",
    "                weights = weights + weightsNew\n",
    "\n",
    "            weights = weights / len(leafIndices)\n",
    "\n",
    "            weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "            weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "        #---\n",
    "\n",
    "        # Check if self.yTrain is a 2D array with more than one column.\n",
    "        if len(self.yTrain.shape) > 1:\n",
    "            if self.yTrain.shape[1] > 1:\n",
    "\n",
    "                if not outputType in ['all', 'onlyPositiveWeights', 'summarized']:\n",
    "                    raise ValueError(\"outputType must be one of 'all', 'onlyPositiveWeights', 'summarized' for multivariate y.\")\n",
    "                \n",
    "                weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                        outputType = outputType, \n",
    "                                                                        y = self.yTrain, \n",
    "                                                                        scalingList = scalingList,\n",
    "                                                                        equalWeights = False) \n",
    "            \n",
    "        else:\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                        outputType = outputType, \n",
    "                                                        y = self.yTrain, \n",
    "                                                        scalingList = scalingList,\n",
    "                                                        equalWeights = False)\n",
    "            \n",
    "        \n",
    "                \n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self: BaseWeightsBasedEstimator, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ): \n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "        return super(MetaEstimatorMixin, self).predict(X = X,\n",
    "                                                       probs = probs, \n",
    "                                                       scalingList = scalingList)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self,\n",
    "                     X: np.ndarray, # Feature Matrix\n",
    "                     **kwargs):\n",
    "        \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "        return super().predict(X = X,\n",
    "                               **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46a9f5-681b-4d67-b046-1f7642ebeb4b",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f685fe-ca26-4e8b-b8f4-e912374ee4f9",
   "metadata": {},
   "source": [
    "## Packages + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013011a-f0e7-472a-b0e6-66a9f69d11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from dddex.baseClasses import BaseLSx\n",
    "from dddex.wSAA import SampleAverageApproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f7bd9-a042-4071-9465-8f86bc267035",
   "metadata": {},
   "source": [
    "The whole idea of using the Wasserstein Distance between the validation points and the computed densities turned out to be mathematically bullshit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0c2d3-81b4-48ee-87e9-65df26ec354a",
   "metadata": {},
   "source": [
    "## CV - Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caec39e-e97d-4a4f-92e5-0be2c2bfdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityCrossValidation:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 # An object with a `predict` method that must (!) have an argument called `probs`\n",
    "                 # that specifies which quantiles to predict. Further, `estimator` needs\n",
    "                 # a `set_params` and `fit` method.\n",
    "                 estimator, \n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGrid: dict, \n",
    "                 randomSearch: bool=False, # Whether to use randomized search or grid search\n",
    "                 # Number of parameter settings that are sampled if `randomSearch == True`. \n",
    "                 # n_iter trades off runtime vs quality of the solution.\n",
    "                 nIter: int=None,\n",
    "                 p: int=1, # The order of the wasserstein distance to evaluate each hyperparameter settings.\n",
    "                 n_jobs: int=None, # Number of jobs to run in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None, \n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        # if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "        #     raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")               \n",
    "        \n",
    "        if not isinstance(p, int):\n",
    "            raise ValueError(\"`p` must be an integer between 1 and inf!\")\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        if randomSearch:\n",
    "            self.parameterGrid = list(ParameterSampler(param_distributions = parameterGrid,\n",
    "                                                       n_iter = nIter,\n",
    "                                                       random_state = random_state).__iter__())\n",
    "            \n",
    "            self.randomSearch = True\n",
    "            self.nIter = nIter\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGrid = ParameterGrid(parameterGrid)\n",
    "            self.randomSearch = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimator = copy.deepcopy(estimator)\n",
    "        self.cvFolds = cvFolds\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestEstimator = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1e22e-a372-47ef-b1b5-928d047a4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def fit(self: DensityCrossValidation, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScore_wasserstein)(estimator = copy.deepcopy(self.estimator),\n",
    "    #                                                                                  parameterGrid = self.parameterGrid,\n",
    "    #                                                                                  cvFold = cvFold,\n",
    "    #                                                                                  p = self.p,\n",
    "    #                                                                                  X = X,\n",
    "    #                                                                                  y = y) for cvFold in self.cvFolds)\n",
    "    \n",
    "    scoresPerFold = [getFoldScore_wasserstein(estimator = copy.deepcopy(self.estimator),\n",
    "                                              parameterGrid = self.parameterGrid,\n",
    "                                              cvFold = cvFold,\n",
    "                                              p = self.p,\n",
    "                                              y = y,\n",
    "                                              X = X) for cvFold in self.cvFolds]\n",
    "    \n",
    "    # RESULTS\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    # BEST PARAMETER SETTING\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    self.bestParams = paramsBest\n",
    "    \n",
    "    # REFITTING ESTIMATOR WITH BEST PARAMETERS\n",
    "    estimator = copy.deepcopy(self.estimator)\n",
    "    estimator.set_params(**paramsBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d878cc-375d-4690-baca-9e4737daac36",
   "metadata": {},
   "source": [
    "### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e68cf-0eb3-40fe-8bd9-dd840ffd8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScore_wasserstein(estimator, parameterGrid, cvFold, p, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    densitiesSAA = SAA_fold.getWeights(X = XTestFold, outputType = 'onlyPositiveWeightsValues')\n",
    "    \n",
    "    wassersteinDistSAA = getWassersteinDistances(densities = densitiesSAA,\n",
    "                                                 yTest = yTestFold,\n",
    "                                                 p = p).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Necessary to ensure compatability with wSAA-models etc.\n",
    "    try:\n",
    "        estimator.refitPointEstimator(X = XTrainFold, \n",
    "                                      y = yTrainFold)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    costsPerParam = defaultdict(dict)\n",
    "\n",
    "    for params in parameterGrid:\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "\n",
    "        estimator.fit(X = XTrainFold,\n",
    "                      y = yTrainFold)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        densities = estimator.getWeights(X = XTestFold,\n",
    "                                         outputType = 'onlyPositiveWeightsValues')\n",
    "        \n",
    "        wassersteinDist = getWassersteinDistances(densities = densities, \n",
    "                                                  yTest = yTestFold, \n",
    "                                                  p = p).sum()\n",
    "        \n",
    "        if wassersteinDistSAA > 0:\n",
    "            wassersteinRatio = wassersteinDist / wassersteinDistSAA\n",
    "        else:\n",
    "            if wassersteinDist == 0:\n",
    "                wassersteinRatio = 0\n",
    "            else:\n",
    "                wassersteinRatio = 1\n",
    "                \n",
    "        costsPerParam[tuple(params.values())] = {'wassersteinRatio': wassersteinRatio}\n",
    "\n",
    "    #---\n",
    "    # s = pd.Series(list(d.values()),index=pd.MultiIndex.from_tuples(d.keys()))\n",
    "    costsDf = pd.DataFrame.from_dict(costsPerParam, orient = 'index')\n",
    "    costsDf.index.names = list(params.keys())\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96406a-4b08-43b2-97c9-9083ebc78a2d",
   "metadata": {},
   "source": [
    "## CV - Combined - Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56543aa5-e198-4ed9-a0a3-c861e587de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityCrossValidationLSx:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 estimatorLSx, # A Level-Set based model.\n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with LSx parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridLSx: dict,\n",
    "                 # dict or list of dicts with parameters names (`str`) of the point predictor as keys\n",
    "                 # and distributions or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridEstimator: dict,\n",
    "                 randomSearchLSx: bool=False, # Whether to use randomized search or grid search for the LSx parameters.\n",
    "                 # Whether to use randomized search or grid search for the point predictor parameters.\n",
    "                 randomSearchEstimator: bool=False, \n",
    "                 # Number of parameter settings of the LSx model that are sampled if `randomSearchLSx == True`. \n",
    "                 # LSx parameter settings are usually relatively cheap to evaluate, so all sampled LSx paramater settings\n",
    "                 # are evaluated for each point predictor parameter setting.\n",
    "                 nIterLSx: int=None,\n",
    "                 # Number of parameter settings of the underlying point predictor that are sampled if \n",
    "                 # `randomSearchEstimator == True`. nIterEstimator trades off runtime vs quality of the solution.\n",
    "                 nIterEstimator: int=None,\n",
    "                 p: int=1, # The order of the wasserstein distance to evaluate each hyperparameter settings.\n",
    "                 n_jobs: int=None, # number of folds being computed in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None,\n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "            raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")\n",
    "            \n",
    "        if len(parameterGridEstimator) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the point predictor. If you want to only evaluate\"\n",
    "                             \"parameter settings of the LSx-estimator itself, use the class `QuantileCrossValidation` instead or\"\n",
    "                             \"provide a fixed parameter setting for the point predictor via `parameterGridEstimator`.\")\n",
    "            \n",
    "        if len(parameterGridLSx) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the LSx model! If you want to only evaluate\"\n",
    "                             \"parameter settings of the point predictor, use standard cross-validation classes or instead\"\n",
    "                             \"provide a fixed parameter setting for the LS model via `parameterGridLSx`.\")\n",
    "            \n",
    "        if not isinstance(p, int):\n",
    "            raise ValueError(\"`p` must be an integer between 1 and inf!\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if randomSearchLSx:\n",
    "            self.parameterGridLSx = list(ParameterSampler(param_distributions = parameterGridLSx,\n",
    "                                                          n_iter = nIterLSx,\n",
    "                                                          random_state = random_state).__iter__())\n",
    "            self.randomSearchLSx = True\n",
    "            self.nIterLsx = nIterLSx\n",
    "            self.random_state = random_state\n",
    "        \n",
    "        else:\n",
    "            self.parameterGridLSx = ParameterGrid(parameterGridLSx)\n",
    "            self.randomSearchLSx = False\n",
    "            \n",
    "        \n",
    "        if randomSearchEstimator:\n",
    "            \n",
    "            self.parameterGridEstimator = list(ParameterSampler(param_distributions = parameterGridEstimator,\n",
    "                                                                n_iter = nIterEstimator,\n",
    "                                                                random_state = random_state).__iter__())\n",
    "            self.randomSearchEstimator = True\n",
    "            self.nIterEstimator = nIterEstimator\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGridEstimator = ParameterGrid(parameterGridEstimator)\n",
    "            self.randomSearchEstimator = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimatorLSx = copy.deepcopy(estimatorLSx)\n",
    "        \n",
    "        self.cvFolds = cvFolds\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestEstimator = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59827510-daf2-497c-b9e9-a4b45d2a9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def fit(self: DensityCrossValidationLSx, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScoreLSx_wasserstein)(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "                                                                                        parameterGridLSx = self.parameterGridLSx, \n",
    "                                                                                        parameterGridEstimator = self.parameterGridEstimator,\n",
    "                                                                                        cvFold = cvFold,\n",
    "                                                                                        p = self.p,\n",
    "                                                                                        y = y,\n",
    "                                                                                        X = X) for cvFold in self.cvFolds)\n",
    "    \n",
    "    # scoresPerFold = [getFoldScoreLSx_wasserstein(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "    #                                              parameterGridLSx = self.parameterGridLSx,\n",
    "    #                                              parameterGridEstimator = self.parameterGridEstimator,\n",
    "    #                                              cvFold = cvFold,\n",
    "    #                                              p = self.p,\n",
    "    #                                              y = y,\n",
    "    #                                              X = X) for cvFold in self.cvFolds]\n",
    "\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS OVER ALL PROBS\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    \n",
    "    paramsLSxNames = self.estimatorLSx._get_param_names()\n",
    "    paramsLSxBest = {paramName: value for paramName, value in paramsBest.items() if paramName in paramsLSxNames}\n",
    "    paramsEstimatorBest = {paramName: value for paramName, value in paramsBest.items() if not paramName in paramsLSxNames}\n",
    "    \n",
    "    self.bestParams = paramsBest\n",
    "    self.bestParamsLSx = paramsLSxBest\n",
    "    self.bestParamsEstimator = paramsEstimatorBest\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    estimatorLSx = copy.deepcopy(self.estimatorLSx)\n",
    "    \n",
    "    estimator = clone(estimatorLSx.estimator)\n",
    "    estimator.set_params(**paramsEstimatorBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "    \n",
    "    estimatorLSx.set_params(**paramsLSxBest,\n",
    "                            estimator = estimator)\n",
    "    estimatorLSx.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimatorLSx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bcb8e-1124-4b9c-ab67-273523a3dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(CrossValidationLSx_combined.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb91a522-ed16-45bb-9c79-36a7ab395035",
   "metadata": {},
   "source": [
    "### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549121a-2046-40dd-89d3-79cf041a10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScoreLSx_wasserstein(estimatorLSx, parameterGridLSx, parameterGridEstimator, cvFold, p, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    densitiesSAA = SAA_fold.getWeights(X = XTestFold, outputType = 'onlyPositiveWeightsValues')\n",
    "    \n",
    "    wassersteinDistSAA = getWassersteinDistances(densities = densitiesSAA,\n",
    "                                                 yTest = yTestFold,\n",
    "                                                 p = p).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    costsDfList = list()\n",
    "    \n",
    "    for paramsEstimator in parameterGridEstimator:\n",
    "        \n",
    "        estimatorLSx.refitPointEstimator(X = XTrainFold, \n",
    "                                         y = yTrainFold,\n",
    "                                         **paramsEstimator)\n",
    "\n",
    "        #---\n",
    "\n",
    "        costsPerParamLSx = defaultdict(dict)\n",
    "\n",
    "        for paramsLSx in parameterGridLSx:\n",
    "\n",
    "            estimatorLSx.set_params(**paramsLSx)\n",
    "\n",
    "            estimatorLSx.fit(X = XTrainFold,\n",
    "                             y = yTrainFold)\n",
    "\n",
    "            densities = estimatorLSx.getWeights(X = XTestFold,\n",
    "                                                outputType = 'onlyPositiveWeightsValues')\n",
    "        \n",
    "            wassersteinDist = getWassersteinDistances(densities = densities, \n",
    "                                                      yTest = yTestFold, \n",
    "                                                      p = p).sum()\n",
    "\n",
    "            if wassersteinDistSAA > 0:\n",
    "                wassersteinRatio = wassersteinDist / wassersteinDistSAA\n",
    "            else:\n",
    "                if wassersteinDist == 0:\n",
    "                    wassersteinRatio = 0\n",
    "                else:\n",
    "                    wassersteinRatio = 1\n",
    "\n",
    "            costsPerParamLSx[tuple(paramsLSx.values())] = {'wassersteinRatio': wassersteinRatio}\n",
    "\n",
    "        #---\n",
    "        \n",
    "        costsDf = pd.DataFrame.from_dict(costsPerParamLSx, orient = 'index')\n",
    "        \n",
    "        paramsLSxNames = list(paramsLSx.keys())\n",
    "        costsDf.index.names = paramsLSxNames\n",
    "\n",
    "        costsDf = costsDf.reset_index(drop = False)\n",
    "        for paramName, value in paramsEstimator.items():\n",
    "            costsDf[paramName] = value\n",
    "\n",
    "        paramNames = paramsLSxNames + list(paramsEstimator.keys())\n",
    "        costsDf = costsDf.set_index(paramNames)\n",
    "        \n",
    "        costsDfList.append(costsDf)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    costsDf = pd.concat(costsDfList, axis = 0)\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9da4f-3533-45b3-9104-9ced4625bde5",
   "metadata": {},
   "source": [
    "#### Get Wasserstein Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368820fb-9598-4bd3-9d16-d8458ed42693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWassersteinDistances(densities, yTest, p):\n",
    "    \n",
    "    wassersteinDists = list()\n",
    "    \n",
    "    for i in range(yTest.shape[0]):\n",
    "        y = yTest[i]\n",
    "        values = densities[i][1]\n",
    "        probs = densities[i][0]\n",
    "\n",
    "        if len(values.shape) == 1:\n",
    "            values = values.reshape(-1, 1)\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        wassersteinDists.append(np.sum(probs * np.sum(np.abs(values - y)**p, axis = 1))**(1/p))\n",
    "\n",
    "    return np.array(wassersteinDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393b1bc-7882-4795-95d7-feb485e1437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# def getWassersteinDistances(densities, yTest, p):\n",
    "    \n",
    "#     # CHECK IF TARGET VALUES ARE ONE- OR MULTI-DIMENSIONAL\n",
    "#     # In the case of one-dimensional arrays, a faster algorithm that\n",
    "#     # doesn't iterate over the test observations can be used.\n",
    "#     if len(densities[0][1].shape) == 1:\n",
    "        \n",
    "#         lenList = list()\n",
    "#         valuesList = list()\n",
    "#         probsList = list()\n",
    "        \n",
    "#         for probs, values in densities:\n",
    "#             lenList.append(values.shape[0])\n",
    "#             probsList.append(probs)\n",
    "#             valuesList.append(values)\n",
    "            \n",
    "#         lenMax = max(lenList)   \n",
    "#         insertionCheck = np.arange(lenMax) < np.array(lenList)[:, None]\n",
    "\n",
    "#         probsArray = np.zeros(shape = (len(probsList), lenMax))\n",
    "#         valuesArray = np.zeros(shape = (len(valuesList), lenMax))\n",
    "\n",
    "#         probsArray[insertionCheck] = np.concatenate(probsList, axis = 0)\n",
    "#         valuesArray[insertionCheck] = np.concatenate(valuesList, axis = 0)\n",
    "#         yTest = yTest.reshape(-1, 1)\n",
    "\n",
    "#         wassersteinDists = np.sum(probsArray * np.abs(valuesArray - yTest)**p, axis = 1)**(1/p)\n",
    "        \n",
    "#         return wassersteinDists\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     else:\n",
    "#         wassersteinDists = list()\n",
    "    \n",
    "#         for i in range(yTest.shape[0]):\n",
    "#             y = yTest[i]\n",
    "#             values = densities[i][1]\n",
    "#             probs = densities[i][0]\n",
    "\n",
    "#             if len(values.shape) == 1:\n",
    "#                 values = values.reshape(-1, 1)\n",
    "#                 y = y.reshape(-1, 1)\n",
    "\n",
    "#             wassersteinDists.append(np.sum(probs * np.sum(np.abs(values - y)**p, axis = 1))**(1/p))\n",
    "\n",
    "#         return np.array(wassersteinDists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec05653-e675-4ac9-8907-7bbdf7abaefe",
   "metadata": {},
   "source": [
    "# Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d8229-e042-447c-b26a-6afa7f176d39",
   "metadata": {},
   "source": [
    "# Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62884dce-f86d-4d75-90bf-9b07f2f345e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE DIMENSIONAL TARGET VALUES\n",
    "densities = [(np.array([0.4, 0.6]), np.array([2, 4])), \n",
    "             (np.array([0.5, 0.5]), np.array([3, 6]))]\n",
    "\n",
    "y = np.array([3, 4])\n",
    "\n",
    "wassersteinDists = getWassersteinDistances(densities = densities, \n",
    "                                           yTest = y, \n",
    "                                           p = 1)\n",
    "\n",
    "distsByHand = np.array([0.4 * 1 + 0.6 * 1, 0.5 * 1 + 0.5 * 2])\n",
    "\n",
    "assert np.allclose(wassersteinDists, distsByHand)\n",
    "\n",
    "#---\n",
    "\n",
    "# MULTI DIMENSIONAL TARGET VALUES\n",
    "densities = [(np.array([0.4, 0.6]), np.array([[2, 4], [4, 4]])), \n",
    "             (np.array([0.5, 0.5]), np.array([[3, 6], [2, 3]]))]\n",
    "\n",
    "y = np.array([[3, 4], \n",
    "              [4, 2]])\n",
    "\n",
    "wassersteinDists = getWassersteinDistances(densities = densities, \n",
    "                                           yTest = y,\n",
    "                                           p = 1)\n",
    "\n",
    "distsByHand = np.array([0.4 * (1 + 0) + 0.6 * (1 + 0), \n",
    "                        0.5 * (1 + 4) + 0.5 * (2 + 1)])\n",
    "\n",
    "assert np.allclose(wassersteinDists, distsByHand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb006a6-32cd-4c31-9bb2-774c4532a558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1330b-4c33-45c5-a1f6-df7e0763b3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex39",
   "language": "python",
   "name": "dddex39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
