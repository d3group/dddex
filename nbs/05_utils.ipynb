{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93fcaa-9d2d-49ea-8cc3-b7e7ebd4b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d834faa-e24c-4542-9070-b8ec8562cc63",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101f7c6-4c66-481f-b580-a8dda687c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294fa26-4c16-435e-9d6a-b7537d8c4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24f328-6b14-4ae8-8034-c24c4a128b28",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e0e08-4fd9-4644-8048-d9f0948600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import copy\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3cf6b-c0ef-4763-a49c-e1c26479622e",
   "metadata": {},
   "source": [
    "## Restructure Weights Data List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828150c-4075-4501-8e91-f107c9c90aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def restructureWeightsDataList(weightsDataList, outputType = 'onlyPositiveWeights', y = None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    # CHECKS AND WARNINGS\n",
    "    if not scalingList is None:\n",
    "        \n",
    "        if len(scalingList) > len(weightsDataList):\n",
    "            warnings.warn(\"'scalingList' is longer than 'weightsDataList'\")\n",
    "            \n",
    "        elif len(scalingList) > len(weightsDataList):\n",
    "            raise ValueError(\"'scalingList' mustn't be shorter than 'weightsDataList'\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeightsValues':\n",
    "        \n",
    "        weightsDataListNew = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weights, values = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                values = values * scalingList[i]\n",
    "                \n",
    "            weightsDataListNew.append((weights, values))\n",
    "            \n",
    "        return weightsDataListNew\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, yUnique = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                              yWeightPos = yWeightPos,\n",
    "                                                              equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yUnique = yUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, yUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistribution':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            indicesSort = np.argsort(yWeightPos)\n",
    "            \n",
    "            weightsPosSorted = weightsPos[indicesSort]\n",
    "            yWeightPosSorted = yWeightPos[indicesSort]\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsPosSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yWeightPosSorted = yWeightPosSorted * scalingList[i]\n",
    "            \n",
    "            distributionDataList.append((cumulativeProbs, yWeightPosSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'cumulativeDistributionSummarized':\n",
    "        \n",
    "        distributionDataList = list()\n",
    "        \n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarizedSorted, yPosWeightUniqueSorted = summarizeWeightsData(weightsPos = weightsPos, \n",
    "                                                                                   yWeightPos = yWeightPos,\n",
    "                                                                                   equalWeights = equalWeights)\n",
    "            \n",
    "            cumulativeProbs = np.cumsum(weightsSummarizedSorted)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yPosWeightUniqueSorted = yPosWeightUniqueSorted * scalingList[i]\n",
    "                \n",
    "            distributionDataList.append((cumulativeProbs, yPosWeightUniqueSorted))\n",
    "            \n",
    "        return distributionDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba02cf86-4954-4f80-bff1-23a9980a6688",
   "metadata": {},
   "source": [
    "### Summarize Weights Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd1e41-454a-4921-a5e8-49c6278afd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarizeWeightsData(weightsPos, yWeightPos, equalWeights = False):\n",
    "    \n",
    "    if equalWeights:\n",
    "        \n",
    "        counterDict = Counter(yWeightPos)\n",
    "        yUniqueSorted = np.sort(list(counterDict.keys()))\n",
    "\n",
    "        weightsSummarizedSorted = np.array([counterDict[value] / len(yWeightPos) for value in yUniqueSorted])\n",
    "    \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "\n",
    "        for i, yValue in enumerate(yWeightPos):\n",
    "            duplicationDict[yValue].append(i)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        yUnique = list()\n",
    "\n",
    "        for value, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            yUnique.append(value)\n",
    "\n",
    "        weightsSummarized, yUnique = np.array(weightsSummarized), np.array(yUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        indicesSort = np.argsort(yUnique)\n",
    "        weightsSummarizedSorted, yUniqueSorted = weightsSummarized[indicesSort], yUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, yUniqueSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0aedc-4f0c-447f-b801-8a6d005c18c4",
   "metadata": {},
   "source": [
    "## Restructure Weights Data List - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd65a4d-7b73-436d-ac68-3365e899810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def restructureWeightsDataList_multivariate(weightsDataList, outputType = 'onlyPositiveWeights', y = None, scalingList = None, equalWeights = False):\n",
    "    \n",
    "    # CHECKS AND WARNINGS\n",
    "    if not scalingList is None:\n",
    "        \n",
    "        if len(scalingList) > len(weightsDataList):\n",
    "            warnings.warn(\"'scalingList' is longer than 'weightsDataList'\")\n",
    "            \n",
    "        elif len(scalingList) > len(weightsDataList):\n",
    "            raise ValueError(\"'scalingList' mustn't be shorter than 'weightsDataList'\")\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if outputType == 'all':\n",
    "        \n",
    "        weightsDataListAll = list()\n",
    "        \n",
    "        for weights, indicesPosWeight in weightsDataList:\n",
    "            weightsAll = np.zeros(len(y))\n",
    "            weightsAll[indicesPosWeight] = weights\n",
    "            weightsDataListAll.append(weightsAll)\n",
    "        \n",
    "        return weightsDataListAll\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'onlyPositiveWeights':\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    elif outputType == 'summarized':\n",
    "        \n",
    "        weightsDataListSummarized = list()\n",
    "\n",
    "        for i in range(len(weightsDataList)):\n",
    "            weightsPos, yWeightPos = weightsDataList[i][0], y[weightsDataList[i][1]]\n",
    "            \n",
    "            weightsSummarized, yUnique = summarizeWeightsData_multivariate(weightsPos = weightsPos, \n",
    "                                                                           yWeightPos = yWeightPos,\n",
    "                                                                           equalWeights = equalWeights)\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                yUnique = yUnique * scalingList[i]\n",
    "                \n",
    "            weightsDataListSummarized.append((weightsSummarized, yUnique))\n",
    "            \n",
    "        return weightsDataListSummarized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb36ac-771a-4664-8551-15d3a8ed63c2",
   "metadata": {},
   "source": [
    "### Summarize Weights Data - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b0bd1-a5ac-47f0-80cb-f9f9469985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarizeWeightsData_multivariate(weightsPos, yWeightPos, equalWeights = False):\n",
    "    \n",
    "    uniqueRes = np.unique(yWeightPos, return_counts = True, return_inverse = True, return_index = True, axis = 0)\n",
    "    \n",
    "    if equalWeights:\n",
    "        \n",
    "        weightsSummarizedSorted = np.array([uniqueRes[3][i] / len(yWeightPos) for i in range(len(uniqueRes[3]))])\n",
    "        yUniqueSorted = uniqueRes[0]\n",
    "        \n",
    "    else:\n",
    "        duplicationDict = defaultdict(list)\n",
    "        for index, indexUnique in enumerate(uniqueRes[2]):\n",
    "            duplicationDict[indexUnique].append(index)\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsSummarized = list()\n",
    "        yUnique = list()\n",
    "\n",
    "        for indexUnique, indices in duplicationDict.items():        \n",
    "\n",
    "            weightsSummarized.append(weightsPos[indices].sum())\n",
    "            yUnique.append(uniqueRes[0][indexUnique])\n",
    "\n",
    "        weightsSummarized, yUnique = np.array(weightsSummarized), np.array(yUnique)\n",
    "\n",
    "        #---\n",
    "\n",
    "        # indicesSort = np.argsort(yUnique)\n",
    "        # weightsSummarizedSorted, yUniqueSorted = weightsSummarized[indicesSort], yUnique[indicesSort]\n",
    "    \n",
    "    return weightsSummarizedSorted, yUniqueSorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d688792-2db3-476c-a73a-f1088f4880ee",
   "metadata": {},
   "source": [
    "## Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940af8bb-aa80-4f46-adef-c3fd6a424a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generateFinalOutput(dataOriginal, \n",
    "                        dataDecisions, \n",
    "                        targetVariable = 'demand', \n",
    "                        mergeOn = None, \n",
    "                        variablesToAdd = None, \n",
    "                        scaleBy = None, \n",
    "                        includeTraining = False, \n",
    "                        sortBy = None,\n",
    "                        longFormat = False,\n",
    "                        **kwargs):\n",
    "    \n",
    "    dataOriginal = dataOriginal.rename(columns = {targetVariable: 'actuals'})\n",
    "    \n",
    "    if not scaleBy is None:\n",
    "        if not isinstance(scaleBy, str): \n",
    "            raise ValueError(\"'scaleBy' has to a string specifying a single feature to scale the target variable!\")\n",
    "        elif scaleBy in dataOriginal.columns:\n",
    "            dataOriginal['actuals'] = dataOriginal['actuals'] * dataOriginal[scaleBy]\n",
    "        else:\n",
    "            raise ValueError(f\"The specified feature {scaleBy} is not part of 'dataOriginal'!\")\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Adding additional data to the matrix dataDecisions\n",
    "    # NOTE: This function is written to be useable for all datasets we currently use. \n",
    "    colsToAdd = ['id', 'sku_code_prefix', 'sku_code', 'SKU_API',\n",
    "                 'actuals', 'revenue', 'label',\n",
    "                 'adi', 'adi_sku', 'adi_product', 'cv2', 'cvDemand_sku', 'cvDemand_product']\n",
    "    \n",
    "    if isinstance(mergeOn, list):\n",
    "        colsToAdd = colsToAdd + mergeOn\n",
    "    else:\n",
    "        colsToAdd.append(mergeOn)\n",
    "    \n",
    "    if isinstance(variablesToAdd, list):\n",
    "        colsToAdd = colsToAdd + variablesToAdd\n",
    "    else:\n",
    "        colsToAdd.append(variablesToAdd)\n",
    "        \n",
    "    if isinstance(sortBy, list):\n",
    "        colsToAdd = colsToAdd + sortBy\n",
    "    else:\n",
    "        colsToAdd.append(sortBy)\n",
    "    \n",
    "    colsToAdd = pd.Series(colsToAdd)\n",
    "    colsToAdd = colsToAdd[colsToAdd.isin(dataOriginal.columns)] \n",
    "    colsToAdd = colsToAdd.unique()\n",
    "    \n",
    "    dataTestInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'test', colsToAdd].reset_index(drop = True)\n",
    "    \n",
    "    if not longFormat:\n",
    "\n",
    "        if mergeOn is None:\n",
    "            dataResults = pd.concat([dataTestInfoToAdd, dataDecisions.reset_index(drop = True)], axis = 1)\n",
    "        else:\n",
    "            dataResults = pd.merge(dataTestInfoToAdd, dataDecisions, on = mergeOn)\n",
    "\n",
    "        #---\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dataDecisionsStacked = dataDecisions.stack().reset_index().set_index('level_0')\n",
    "        dataDecisionsStacked.rename(columns = {'level_1': 'decisionType'}, inplace = True)\n",
    "        \n",
    "        dataDecisionsStacked.rename(columns = {0: 'decisions'}, inplace = True)\n",
    "        dataDecisionsStacked.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        numberOfDecisionTypes = len(dataDecisionsStacked['decisionType'].unique())\n",
    "        \n",
    "        infoDuplicatedDf = dataTestInfoToAdd.loc[dataTestInfoToAdd.index.repeat(numberOfDecisionTypes)]\n",
    "        infoDuplicatedDf.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        dataResults = pd.concat([infoDuplicatedDf, dataDecisionsStacked], axis = 1)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    infoToAdd = pd.DataFrame(kwargs, index = [0])\n",
    "    infoToAdd['label'] = 'test'\n",
    "    dataResults = pd.merge(dataResults, infoToAdd, on = 'label')\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if includeTraining:\n",
    "        dataTrainInfoToAdd = dataOriginal.loc[dataOriginal['label'] == 'train', colsToAdd].reset_index(drop = True)\n",
    "        dataResults = pd.concat([dataTrainInfoToAdd, dataResults], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    if not sortBy is None:\n",
    "        \n",
    "        if not all([sortByCol in dataResults.columns for sortByCol in sortBy]):\n",
    "            raise ValueError(\"Columns specified by 'sortBy' must be part of 'dataOriginal'!\")\n",
    "        else:\n",
    "            dataResults.sort_values(by = sortBy, axis = 0, inplace = True, ignore_index = True)\n",
    "    \n",
    "    return dataResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bac87-dc5d-47c7-a984-a32431bf0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
