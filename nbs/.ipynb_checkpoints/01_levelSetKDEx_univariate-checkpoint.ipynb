{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61516f21-8606-49d3-8907-c0190315ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd6b76-2644-458b-8436-da808dd134c7",
   "metadata": {},
   "source": [
    "# Level-Set Based Kernel Density Estimation\n",
    "> Defining the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` which turn any point predictor into a conditional kernel density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7668fd2-5361-43a1-ba2b-42506a2419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp levelSetKDEx_univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cef21-4158-4307-b898-6bc03398a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95d3a6-a5e0-46de-b191-d1de0281f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict, Counter, deque\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import faiss\n",
    "from drf import drf \n",
    "\n",
    "from dddex.baseClasses import BaseLSx, BaseWeightsBasedEstimator\n",
    "from dddex.utils import restructureWeightsDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8315b-c072-4e14-b477-5856b5d922fe",
   "metadata": {},
   "source": [
    "In the following we define the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` where KDE is short for 'Kernel Density Estimator' and the 'x' is supposed to signal that both classes can be defined based on any arbitrary point predictor. The name 'LevelSet' stems from the fact that every approach presented in this notebook interprets the values of the point forecasts as a similarity measure between samples. The point predictor is specified by the argument `estimator` and must have a `.predict()`-method and should have been trained before hand. \n",
    "\n",
    "Both classes `LevelSetKDEx` and `LevelSetKDEx_kNN` fulfill the same task: By first running `.fit(XTrain, yTrain)` and then calling `.generateWeights(XTest)`, they both output an estimation of the conditional density of every sample specified by 'XTest'. The basic idea for both approaches is also identical: Suppose we have a single test sample at hand. At first, we compare the value of the point prediction of this sample and the values of the point predictions of the training samples computed via `estimator.predict(XTrain)` and `estimator.predict(XTest)`, respectively. Based on this comparison, we select 'binSize'-many training samples that we deem the most similar to the test sample at hand. The concrete way we select the training samples constitutes the only difference between `LevelSetKDEx` and `LevelSetKDEx_kNN`. Finally, the empirical distribution of the y-values of these training samples then acts as our estimation of the conditional distribution.\n",
    "\n",
    "Further details on how both approaches work approaches can be found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a75d0-8a2c-4482-b372-6e67bf394d86",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Bin Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cebd8e-52d0-4a7f-af8f-75af78972824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=100, # Size of the bins created while running fit.\n",
    "                 # Determines behaviour of method `getWeights`. If False, all weights receive the same  \n",
    "                 # value. If True, the distance of the point forecasts is taking into account.\n",
    "                 weightsByDistance: bool=False, \n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # Check if binSize is integer\n",
    "        if not isinstance(binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "\n",
    "        # Check if weightsByDistance is bool\n",
    "        if not isinstance(weightsByDistance, bool):\n",
    "            raise ValueError(\"'weightsByDistance' must be a boolean!\")\n",
    "\n",
    "        self.binSize = binSize\n",
    "        self.weightsByDistance = weightsByDistance\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if not isinstance(self.binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        indicesPerBin, lowerBoundPerBin = generateBins(binSize = self.binSize,\n",
    "                                                       yPred = yPred)\n",
    "\n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.lowerBoundPerBin = lowerBoundPerBin\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        \n",
    "        binPerPred = np.searchsorted(a = self.lowerBoundPerBin, v = yPred, side = 'right') - 1\n",
    "        neighborsList = [self.indicesPerBin[binIndex] for binIndex in binPerPred]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if self.weightsByDistance:\n",
    "\n",
    "            predDistances = [np.abs(self.yPredTrain[neighborsList[i]] - yPred[i]) for i in range(len(neighborsList))]\n",
    "\n",
    "            weightsDataList = list()\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                distances = predDistances[i]\n",
    "                distancesCloseZero = np.isclose(distances, 0)\n",
    "                \n",
    "                if np.any(distancesCloseZero):\n",
    "                    indicesCloseZero = neighborsList[i][np.where(distancesCloseZero)[0]]\n",
    "                    weightsDataList.append((np.repeat(1 / len(indicesCloseZero), len(indicesCloseZero)),\n",
    "                                            indicesCloseZero))\n",
    "                    \n",
    "                else:                                 \n",
    "                    inverseDistances = 1 / distances\n",
    "\n",
    "                    weightsDataList.append((inverseDistances / inverseDistances.sum(), \n",
    "                                            np.array(neighborsList[i], dtype = 'uintc')))\n",
    "            \n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = False)\n",
    "        \n",
    "        else:\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors, dtype = 'uintc')) \n",
    "                               for neighbors in neighborsList]\n",
    "\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def solveKernelGLS(self,\n",
    "                       X,\n",
    "                       sigma,\n",
    "                       c):\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        binPerPred = np.searchsorted(a = self.lowerBoundPerBin, v = yPred, side = 'right') - 1\n",
    "        \n",
    "        binVectors = [(binPerPred == i).reshape(-1, 1) * 1 for i in range(len(self.lowerBoundPerBin))]\n",
    "        binVectorsToSlice = [np.where(binVector)[0] for binVector in binVectors]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        def getNewSolution(u, x, y):\n",
    "    \n",
    "            if len(u.shape) == 1:\n",
    "                u.reshape(-1, 1)\n",
    "\n",
    "            uSlice = np.where(u == 1)[0]\n",
    "\n",
    "            ux = np.sum(x[uSlice, :], axis = 0, keepdims = True)\n",
    "            xNew = x - np.matmul(y, ux) / (1 + np.sum(y[uSlice]))\n",
    "\n",
    "            return xNew\n",
    "        \n",
    "        def solveGLS_initial(sigma, u, c):\n",
    "    \n",
    "            if len(u.shape) == 1:\n",
    "                u = u.reshape(-1, 1)\n",
    "\n",
    "            if len(c.shape) == 1:\n",
    "                c = c.reshape(-1, 1)\n",
    "\n",
    "            x = c * sigma**(-1)\n",
    "            y = u * sigma**(-1)\n",
    "\n",
    "            return getNewSolution(u = u, x = x, y = y)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yDict = dict()\n",
    "\n",
    "        # Loop for A_k^-1 y = u_k\n",
    "        for k in range(len(binVectors)):\n",
    "\n",
    "            # Loop for A_j^-1 y = u_k\n",
    "            for j in range(k):\n",
    "\n",
    "                if j == 0:\n",
    "                    yDict[(0, k)] = solveGLS_initial(sigma = sigma, u = binVectors[0], c = binVectors[k])\n",
    "\n",
    "                else:\n",
    "                    yDict[j, k] = getNewSolution(u = binVectors[j], x = yDict[(j - 1, k)], y = yDict[(j - 1, j)])   \n",
    "\n",
    "            #---\n",
    "\n",
    "            if k == 0:\n",
    "                x = solveGLS_initial(sigma = sigma, u = binVectors[0], c = c)\n",
    "            else:\n",
    "                x = getNewSolution(u = binVectors[k], x = x, y = yDict[(k - 1, k)])\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getKernelVectorProduct(self,\n",
    "                               X1,\n",
    "                               c,\n",
    "                               X2 = None,\n",
    "                               ):\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "\n",
    "        yPred1 = self.estimator.predict(X1)\n",
    "        binPerPred1 = np.searchsorted(a = self.lowerBoundPerBin, v = yPred1, side = 'right') - 1\n",
    "        \n",
    "        binVectors1 = [(binPerPred1 == i).reshape(-1, 1) * 1 for i in range(len(self.lowerBoundPerBin))]\n",
    "        binVectorsToSlice1 = [np.where(binVector)[0] for binVector in binVectors1]\n",
    "        \n",
    "        if X2 is None:\n",
    "            binVectors2 = binVectors1\n",
    "            binVectorsToSlice2 = binVectorsToSlice1\n",
    "        \n",
    "        else:\n",
    "            yPred2 = self.estimator.predict(X2)\n",
    "            binPerPred2 = np.searchsorted(a = self.lowerBoundPerBin, v = yPred2, side = 'right') - 1\n",
    "\n",
    "            binVectors2 = [(binPerPred2 == i).reshape(-1, 1) * 1 for i in range(len(self.lowerBoundPerBin))]\n",
    "            binVectorsToSlice2 = [np.where(binVector)[0] for binVector in binVectors2]\n",
    "        \n",
    "        if len(c.shape) == 1:\n",
    "            c = c.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        n = X1.shape[0]\n",
    "        m = c.shape[1]\n",
    "        \n",
    "        resList = list()\n",
    "        \n",
    "        for i in range(len(self.lowerBoundPerBin)):\n",
    "            uc = np.sum(c[binVectorsToSlice2[i], :], axis = 0, keepdims = True)\n",
    "            \n",
    "            kernelProduct = np.zeros(shape = (n, m))\n",
    "            kernelProduct[binVectorsToSlice1[i], :] = uc\n",
    "            \n",
    "            resList.append(kernelProduct)\n",
    "        \n",
    "        vectorProduct = np.sum(resList, axis = 0)\n",
    "        \n",
    "        return vectorProduct\n",
    "            \n",
    "    #---\n",
    "    \n",
    "    def getGaussianPosterior(self,\n",
    "                             XTrain,\n",
    "                             yTrain,\n",
    "                             XTest,\n",
    "                             sigma):\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPredTrain = self.estimator.predict(XTrain)\n",
    "        binPerPredTrain = np.searchsorted(a = self.lowerBoundPerBin, v = yPredTrain, side = 'right') - 1\n",
    "        \n",
    "        binVectorsTrain = [(binPerPredTrain == i).reshape(-1, 1) * 1 for i in range(len(self.lowerBoundPerBin))]\n",
    "        binVectorsToSliceTrain = [np.where(binVector)[0] for binVector in binVectorsTrain]\n",
    "        \n",
    "        yPredTest = self.estimator.predict(XTest)\n",
    "        binPerPredTest = np.searchsorted(a = self.lowerBoundPerBin, v = yPredTest, side = 'right') - 1\n",
    "        \n",
    "        binVectorsTest = [(binPerPredTest == i).reshape(-1, 1) * 1 for i in range(len(self.lowerBoundPerBin))]\n",
    "        binVectorsToSliceTest = [np.where(binVector)[0] for binVector in binVectorsTest]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        n = binVectorsTrain[0].shape[0]\n",
    "        k = binVectorsTest[0].shape[0]\n",
    "        \n",
    "        kernelProductList = list()\n",
    "        \n",
    "        for i in range(len(self.lowerBoundPerBin)):\n",
    "            x = self.solveKernelGLS(X = XTrain,\n",
    "                                    sigma = sigma,\n",
    "                                    c = binVectorsTrain[i])\n",
    "            \n",
    "            kernelProduct = np.zeros(shape = (n, k))\n",
    "            kernelProduct[:, binVectorsToSliceTest[i]] = x\n",
    "            kernelProductList.append(kernelProduct)\n",
    "            \n",
    "        kernelProduct = sum(kernelProductList)\n",
    "        \n",
    "        # x = self.solveGLSKernel(X = XTrain,\n",
    "        #                         sigma = sigma,\n",
    "        #                         c = kernelProduct)\n",
    "        \n",
    "        covRightSide = self.getKernelVectorProduct(X1 = XTest,\n",
    "                                                   X2 = XTrain,\n",
    "                                                   c = kernelProduct)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        covPrior = np.zeros(shape = (k, k))\n",
    "        \n",
    "        for i in range(len(self.lowerBoundPerBin)):\n",
    "            v = binVectorsToSliceTest[i]\n",
    "            covPrior[v[:, None], v] = 1\n",
    "        \n",
    "        covPosterior = covPrior - covRightSide\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(yTrain.shape) == 1:\n",
    "            yTrain = yTrain.reshape(-1, 1)\n",
    "        \n",
    "        x = self.solveKernelGLS(X = XTrain, \n",
    "                                sigma = sigma,\n",
    "                                c = yTrain)\n",
    "        \n",
    "        meanPosterior = self.getKernelVectorProduct(X1 = XTest,\n",
    "                                                    X2 = XTrain,\n",
    "                                                    c = x)\n",
    "        \n",
    "        return meanPosterior, covPosterior\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdb5bf-f39d-470f-b7cc-8679e733b2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx.fit\n",
       "\n",
       ">      LevelSetKDEx.fit (X:numpy.ndarray, y:numpy.ndarray)\n",
       "\n",
       "Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
       "according to their value. Samples are recursively sorted into bins until each bin contains\n",
       "`binSize` many samples. For details, checkout the function `generateBins` which does the\n",
       "heavy lifting.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | np.ndarray | Feature matrix used by `estimator` to predict `y`. |\n",
       "| y | np.ndarray | 1-dimensional target variable corresponding to the feature matrix `X`. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx.fit\n",
       "\n",
       ">      LevelSetKDEx.fit (X:numpy.ndarray, y:numpy.ndarray)\n",
       "\n",
       "Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
       "according to their value. Samples are recursively sorted into bins until each bin contains\n",
       "`binSize` many samples. For details, checkout the function `generateBins` which does the\n",
       "heavy lifting.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | np.ndarray | Feature matrix used by `estimator` to predict `y`. |\n",
       "| y | np.ndarray | 1-dimensional target variable corresponding to the feature matrix `X`. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LevelSetKDEx.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca538d-96ff-48df-9e29-8573a6442a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L123){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx.getWeights\n",
       "\n",
       ">      LevelSetKDEx.getWeights (X:numpy.ndarray,\n",
       ">                               outputType:str='onlyPositiveWeights',\n",
       ">                               scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L123){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx.getWeights\n",
       "\n",
       ">      LevelSetKDEx.getWeights (X:numpy.ndarray,\n",
       ">                               outputType:str='onlyPositiveWeights',\n",
       ">                               scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LevelSetKDEx.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661e147-5acc-4593-b3a5-e93f2d82447f",
   "metadata": {},
   "source": [
    "#### Generate Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d9778-557b-4618-80a8-c440a81b2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generateBins(binSize: int, # Size of the bins of values of `yPred` being grouped together.\n",
    "                 yPred: np.ndarray, # 1-dimensional array of predicted values.\n",
    "                 ):\n",
    "    \"Used to generate the bin-structure used by `LevelSetKDEx` to compute density estimations.\"\n",
    "    \n",
    "    predIndicesSort = np.argsort(yPred)\n",
    "    yPredSorted = yPred[predIndicesSort]\n",
    "\n",
    "    currentBinSize = 0\n",
    "    binIndex = 0\n",
    "    trainIndicesLeft = len(yPred)\n",
    "    indicesPerBin = defaultdict(list)\n",
    "    lowerBoundPerBin = dict()\n",
    "    \n",
    "    for i in range(len(yPred)):\n",
    "        \n",
    "        if i == 0:\n",
    "            lowerBoundPerBin[binIndex] = np.NINF\n",
    "            \n",
    "        currentBinSize += 1\n",
    "        trainIndicesLeft -= 1\n",
    "\n",
    "        indicesPerBin[binIndex].append(predIndicesSort[i])\n",
    "        \n",
    "        if trainIndicesLeft < binSize:\n",
    "            indicesPerBin[binIndex].extend(predIndicesSort[np.arange(i+1, len(yPred), 1)])\n",
    "            break\n",
    "\n",
    "        if currentBinSize >= binSize and yPredSorted[i] < yPredSorted[i+1]:\n",
    "            lowerBoundPerBin[binIndex + 1] = (yPredSorted[i] + yPredSorted[i+1]) / 2\n",
    "            binIndex += 1\n",
    "            currentBinSize = 0\n",
    "           \n",
    "    indicesPerBin = {binIndex: np.array(indices, dtype = 'uintc') for binIndex, indices in indicesPerBin.items()}\n",
    "    \n",
    "    lowerBoundPerBin = pd.Series(lowerBoundPerBin)\n",
    "    lowerBoundPerBin.index.name = 'binIndex'\n",
    "    \n",
    "    return indicesPerBin, lowerBoundPerBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b9d07-e1af-49e1-8709-5575002f1bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L392){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### generateBins\n",
       "\n",
       ">      generateBins (binSize:int, yPred:numpy.ndarray)\n",
       "\n",
       "Used to generate the bin-structure used by `LevelSetKDEx` to compute density estimations.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| binSize | int | Size of the bins of values of `yPred` being grouped together. |\n",
       "| yPred | np.ndarray | 1-dimensional array of predicted values. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L392){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### generateBins\n",
       "\n",
       ">      generateBins (binSize:int, yPred:numpy.ndarray)\n",
       "\n",
       "Used to generate the bin-structure used by `LevelSetKDEx` to compute density estimations.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| binSize | int | Size of the bins of values of `yPred` being grouped together. |\n",
       "| yPred | np.ndarray | 1-dimensional array of predicted values. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(generateBins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a53ad6",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on DRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefe1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_DRF(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=100, # Size of the bins created while running fit.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # Check if binSize is integer\n",
    "        if not isinstance(binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.drf = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx_DRF, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if not isinstance(self.binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = pd.DataFrame(yPred)\n",
    "        y = pd.Series(y)\n",
    "\n",
    "        DRF = drf(min_node_size = self.binSize, num_trees = 100, num_features = 1, honesty = False, sample_fraction = 0.5, response_scaling = False, mtry = 1, num_threads = 16)\n",
    "        DRF.fit(X = yPred, Y = y)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        self.yTrain = y.ravel()\n",
    "        \n",
    "        self.yPredTrain = yPred\n",
    "        self.drf = DRF\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self: LevelSetKDEx_DRF, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        yPred = pd.DataFrame(yPred)\n",
    "        \n",
    "        weightsArray = self.drf.predict(yPred).weights\n",
    "        weightsList = list(weightsArray)\n",
    "        weightsDataList = [(weights[weights > 0], np.where(weights > 0)[0]) for weights in weightsList]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd550e-ae6e-44fe-a91b-f0ffee0096bc",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b41f2-b012-428e-a18c-6fa6c20690ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_kNN(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "     `LevelSetKDEx_kNN` turns any point predictor that has a .predict-method \n",
    "    into an estimator of the condititional density of the underlying distribution.\n",
    "    The basic idea of each level-set based approach is to interprete the point forecast\n",
    "    generated by the underlying point predictor as a similarity measure of samples.\n",
    "    In the case of the `LevelSetKDEx_kNN` defined here, for every new samples\n",
    "    'binSize'-many training samples are computed whose point forecast is closest\n",
    "    to the point forecast of the new sample.\n",
    "    The resulting empirical distribution of these 'nearest' training samples are \n",
    "    viewed as our estimation of the conditional distribution of each the new sample \n",
    "    at hand.\n",
    "    \n",
    "    NOTE: In contrast to the standard `LevelSetKDEx`, it is possible to apply\n",
    "    `LevelSetKDEx_kNN` to arbitrary dimensional point predictors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=100, # Size of the bins created while running fit.\n",
    "                 # Determines behaviour of method `getWeights`. If False, all weights receive the same  \n",
    "                 # value. If True, the distance of the point forecasts is taking into account.\n",
    "                 weightsByDistance: bool=False, \n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # Check if binSize is integer\n",
    "        if not isinstance(binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "\n",
    "        # Check if weightsByDistance is bool\n",
    "        if not isinstance(weightsByDistance, bool):\n",
    "            raise ValueError(\"'weightsByDistance' must be a boolean!\")\n",
    "        \n",
    "        self.binSize = binSize\n",
    "        self.weightsByDistance = weightsByDistance\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.nearestNeighborsOnPreds = None\n",
    "        self.fitted = False\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx_kNN` model by applying the nearest neighbors algorithm to the point\n",
    "        predictions of the samples specified by `X` based on `estimator`. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if not isinstance(self.binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "            \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "            \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "        except NotFittedError:\n",
    "            # warnings.warn(\"The object 'estimator' must have been fitted already!\"\n",
    "            #               \"'estimator' will be fitted with 'X' and 'y' to enable point predicting!\",\n",
    "            #               stacklevel = 2)\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "\n",
    "        #---\n",
    "        \n",
    "        yPred_reshaped = np.reshape(yPred, newshape = (len(yPred), 1))\n",
    "        \n",
    "        nn = NearestNeighbors(algorithm = 'kd_tree')\n",
    "        nn.fit(X = yPred_reshaped)\n",
    "\n",
    "        #---\n",
    "\n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.nearestNeighborsOnPreds = nn\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx_kNN instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "            \n",
    "        #---\n",
    "\n",
    "        nn = self.nearestNeighborsOnPreds\n",
    "\n",
    "        #---\n",
    "\n",
    "        yPred = self.estimator.predict(X)   \n",
    "        yPred_reshaped = np.reshape(yPred, newshape = (len(yPred), 1))\n",
    "\n",
    "        distancesMatrix, neighborsMatrix = nn.kneighbors(X = yPred_reshaped, \n",
    "                                                         n_neighbors = self.binSize + 1)\n",
    "\n",
    "        #---\n",
    "\n",
    "        neighborsList = list(neighborsMatrix[:, 0:self.binSize])\n",
    "        distanceCheck = np.where(distancesMatrix[:, self.binSize - 1] == distancesMatrix[:, self.binSize])\n",
    "        indicesToMod = distanceCheck[0]\n",
    "\n",
    "        for index in indicesToMod:\n",
    "            distanceExtremePoint = np.absolute(yPred[index] - self.yPredTrain[neighborsMatrix[index, self.binSize-1]])\n",
    "\n",
    "            neighborsByRadius = nn.radius_neighbors(X = yPred_reshaped[index:index + 1], \n",
    "                                                    radius = distanceExtremePoint, return_distance = False)[0]\n",
    "            neighborsList[index] = neighborsByRadius\n",
    "\n",
    "        #---\n",
    "        \n",
    "        if self.weightsByDistance:\n",
    "            binSizesReal = [len(neighbors) for neighbors in neighborsList]\n",
    "            binSizeMax = max(binSizesReal)\n",
    "            \n",
    "            distancesMatrix, neighborsMatrix = nn.kneighbors(X = yPred_reshaped, \n",
    "                                                             n_neighbors = binSizeMax)\n",
    "            \n",
    "            weightsDataList = list()\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                distances = distancesMatrix[i, 0:binSizesReal[i]]\n",
    "                distancesCloseZero = np.isclose(distances, 0)\n",
    "                \n",
    "                if np.any(distancesCloseZero):\n",
    "                    indicesCloseZero = neighborsMatrix[i, np.where(distancesCloseZero)[0]]\n",
    "                    weightsDataList.append((np.repeat(1 / len(indicesCloseZero), len(indicesCloseZero)),\n",
    "                                            indicesCloseZero))\n",
    "                    \n",
    "                else:                                 \n",
    "                    inverseDistances = 1 / distances\n",
    "\n",
    "                    weightsDataList.append((inverseDistances / inverseDistances.sum(), \n",
    "                                            np.array(neighborsList[i], dtype = 'uintc')))\n",
    "            \n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = False)\n",
    "            \n",
    "        else:\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors, dtype = 'uintc')) \n",
    "                               for neighbors in neighborsList]\n",
    "\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9daa50-ef7b-4858-b343-b212f07a02b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx_kNN.fit\n",
       "\n",
       ">      LevelSetKDEx_kNN.fit (X:numpy.ndarray, y:numpy.ndarray)\n",
       "\n",
       "Fit `LevelSetKDEx_kNN` model by applying the nearest neighbors algorithm to the point\n",
       "predictions of the samples specified by `X` based on `estimator`.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | np.ndarray | Feature matrix used by `estimator` to predict `y`. |\n",
       "| y | np.ndarray | 1-dimensional target variable corresponding to the feature matrix `X`. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L478){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx_kNN.fit\n",
       "\n",
       ">      LevelSetKDEx_kNN.fit (X:numpy.ndarray, y:numpy.ndarray)\n",
       "\n",
       "Fit `LevelSetKDEx_kNN` model by applying the nearest neighbors algorithm to the point\n",
       "predictions of the samples specified by `X` based on `estimator`.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| X | np.ndarray | Feature matrix used by `estimator` to predict `y`. |\n",
       "| y | np.ndarray | 1-dimensional target variable corresponding to the feature matrix `X`. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LevelSetKDEx_kNN.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f4603-b305-464c-a67b-45ff7be439ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx_kNN.getWeights\n",
       "\n",
       ">      LevelSetKDEx_kNN.getWeights (X:numpy.ndarray,\n",
       ">                                   outputType:str='onlyPositiveWeights',\n",
       ">                                   scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/levelSetKDEx_univariate.py#L533){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LevelSetKDEx_kNN.getWeights\n",
       "\n",
       ">      LevelSetKDEx_kNN.getWeights (X:numpy.ndarray,\n",
       ">                                   outputType:str='onlyPositiveWeights',\n",
       ">                                   scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(LevelSetKDEx_kNN.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10822979-f3a5-4612-bfed-c6cca4b341a6",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c8fe1-6e5d-47fd-a0bb-13376e4149e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_NN(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "     `LevelSetKDEx_kNN` turns any point predictor that has a .predict-method \n",
    "    into an estimator of the condititional density of the underlying distribution.\n",
    "    The basic idea of each level-set based approach is to interprete the point forecast\n",
    "    generated by the underlying point predictor as a similarity measure of samples.\n",
    "    In the case of the `LevelSetKDEx_kNN` defined here, for every new samples\n",
    "    'binSize'-many training samples are computed whose point forecast is closest\n",
    "    to the point forecast of the new sample.\n",
    "    The resulting empirical distribution of these 'nearest' training samples are \n",
    "    viewed as our estimation of the conditional distribution of each the new sample \n",
    "    at hand.\n",
    "    \n",
    "    NOTE: In contrast to the standard `LevelSetKDEx`, it is possible to apply\n",
    "    `LevelSetKDEx_kNN` to arbitrary dimensional point predictors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=100, # Size of the bins created while running fit.\n",
    "                 # Setting 'efficientRAM = TRUE' is only necessary when there are roughly umore than 200k training observations to avoid\n",
    "                 # an overusage of RAM. This setting causes the run-time of the algorithm of the weights computation to linearly depend on \n",
    "                 # 'binSize'. Because of that the algorithm becomes quite slow for 'binSize' > 10k'.\n",
    "                 efficientRAM: bool=False,\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # Check if binSize is integer\n",
    "        if not isinstance(binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "\n",
    "        # Check if efficient RAM is boolean\n",
    "        if not isinstance(efficientRAM, bool):\n",
    "            raise ValueError(\"'efficientRAM' must be a boolean!\")\n",
    "        \n",
    "        self.binSize = binSize\n",
    "        self.weightsByDistance = False\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.fitted = False\n",
    "        self.efficientRAM = efficientRAM\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx_kNN` model by applying the nearest neighbors algorithm to the point\n",
    "        predictions of the samples specified by `X` based on `estimator`. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if not isinstance(self.binSize, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "            \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "            \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "\n",
    "        #---\n",
    "        \n",
    "        neighborsDict, neighborsRemoved, neighborsAdded = getNeighbors(binSize = self.binSize,\n",
    "                                                                       yPred = yPred)\n",
    "\n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.neighborsDictTrain = neighborsDict\n",
    "        self._neighborsRemoved = neighborsRemoved\n",
    "        self._neighborsAdded = neighborsAdded\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None,\n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx_kNN instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        \n",
    "        neighborsDictTest = getNeighborsTest(binSize = self.binSize,\n",
    "                                             yPred = yPred,\n",
    "                                             yPredTrain = self.yPredTrain,\n",
    "                                             neighborsDictTrain = self.neighborsDictTrain)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = getKernelValues(yPred = yPred,\n",
    "                                          yPredTrain = self.yPredTrain,\n",
    "                                          neighborsDictTest = neighborsDictTest,\n",
    "                                          neighborsDictTrain = self.neighborsDictTrain,\n",
    "                                          neighborsRemoved = self._neighborsRemoved,\n",
    "                                          neighborsAdded = self._neighborsAdded,\n",
    "                                          binSize = self.binSize,\n",
    "                                          efficientRAM = self.efficientRAM)\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7a117-4626-4d16-b690-77632727bc7b",
   "metadata": {},
   "source": [
    "### Get Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8de776-19cf-47ca-b11e-032056fbfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def getNeighbors(binSize: int, # Size of the bins of values of `yPred` being grouped together.\n",
    "                 yPred: np.ndarray, # 1-dimensional array of predicted values.\n",
    "                 ):\n",
    "    \"Used to generate the neighboorhoods used by `LevelSetKDEx` to compute density estimations.\"\n",
    "    \n",
    "    duplicationDict = defaultdict(list)\n",
    "    counterDict = defaultdict(int)\n",
    "    \n",
    "    for index, value in enumerate(yPred):\n",
    "        duplicationDict[value].append(index)\n",
    "        counterDict[value] += 1\n",
    "    \n",
    "    duplicationDict = dict(duplicationDict)\n",
    "    counterDict = dict(counterDict)\n",
    "    \n",
    "    yPredUnique = np.sort(list(duplicationDict.keys()))\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Here we initiate our search for the nearest neighbors by creating the neighborhood of the lowest point prediction.\n",
    "    # VARIABLES:\n",
    "    # a) 'neighbors': A Collection.deque object that keeps track of the training indices which are the nearest neighbors of \n",
    "    # the current observation. A deque object allows us to very efficiently remove values from the left and right side of the last.\n",
    "    # b) 'neighborsMaxIter': The index of yPredUnique that has to be considered next (and hasn't yet been considered).\n",
    "    # neighborsMaxIter has to be set to len(yPredUnique), when we iterated over all yPredUnique.\n",
    "    #\n",
    "    # PROCEDURE\n",
    "    # 1) We simply start at the lowest predicted value, iterate from here on over the other predicted values and keep\n",
    "    # adding indices of neighbors by using 'duplicationDict', whose keys are the predicted values\n",
    "    # and whose entries are the indices of the training instances that all share the same point prediction.\n",
    "    # 2) 'neighborsMaxIter' is eventually being set, so we can use this information during the further search down below. \n",
    "    \n",
    "    neighborsPerPred = dict()\n",
    "    neighbors = deque()\n",
    "    \n",
    "    for k in range(len(yPredUnique)):\n",
    "        \n",
    "        if len(neighbors) < binSize:\n",
    "            neighbors.extend(duplicationDict[yPredUnique[k]])\n",
    "            \n",
    "        else:\n",
    "            neighborsMaxIter = k\n",
    "            break\n",
    "        \n",
    "        if k == (len(yPredUnique) - 1):\n",
    "            neighborsMaxIter = len(yPredUnique)\n",
    "    \n",
    "    neighborsPerPred[yPredUnique[0]] = np.array(neighbors, dtype = 'uintc')\n",
    "    neighborsUnchangedLoop = True\n",
    "    neighborsUnchangedLoop = True\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    neighborsRemoved = [0]\n",
    "    neighborsAdded = [0]\n",
    "    \n",
    "    for i in range(1, len(yPredUnique)):\n",
    "        \n",
    "        removeCounter = 0\n",
    "        addCounter = 0\n",
    "            \n",
    "        predCurrent = yPredUnique[i]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # CHECK AND CLEAN CURRENT NEIGHBORHOOD BEFORE STARTING THE LOOP\n",
    "        # \n",
    "        # One very obvious case where we need such a cleaning step is the following:\n",
    "        # Assume yPredUnique = [1, 2, 3] and binsize = 2. In this case the loop below over k in \n",
    "        # range(neighborsMaxIter, len(yPredUnique), 1) creates the neighborhood of indices [0, 1, 2] (all indices) \n",
    "        # for 2. As we have iterated over all possible neighbors, below loop isn't considered anymore. When we now\n",
    "        # consider the prediction 3, its neighborhood is supposed to consist of [1, 2]. For that reason we must check\n",
    "        # if it is possible to remove the left-side of the neighborhood in case 'neighbors' is bigger than bin-size.\n",
    "        # \n",
    "        # So what are we doing here?\n",
    "        # 1) We enter the loop, when the number of currently detected neighbors is higher than binSize. This can\n",
    "        # naturally occur, when the the outer neighbors contain identical predicted values.\n",
    "        # 2) We first compute the distance of the current prediction to the two outer predictions.\n",
    "        # 3) When the current and left-most prediction are not identical and further apart than distanceToMax,\n",
    "        # then we look up how many indices belong to the left-most prediction. If this is not the case, we do nothing \n",
    "        # and leave the while loop.\n",
    "        # 4) If the number of neighbors would still be higher than binSize after removing all left-most neighbors,\n",
    "        # we remove all left-most neighbors and go back to step (3). Otherwise we stop.\n",
    "        \n",
    "        neighborsUnchangedCheck = True\n",
    "        \n",
    "        if len(neighbors) > binSize:\n",
    "            \n",
    "            checkNeeded = True\n",
    "            while checkNeeded:\n",
    "                \n",
    "                distanceToMin = predCurrent - yPred[neighbors[0]]\n",
    "                distanceToMax = yPred[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "                if distanceToMin > 0 and distanceToMin > distanceToMax:\n",
    "                    countIdenticalMin = counterDict[yPred[neighbors[0]]]\n",
    "                \n",
    "                    if len(neighbors) - countIdenticalMin >= binSize:\n",
    "                        removeCounter += countIdenticalMin\n",
    "\n",
    "                        for p in range(countIdenticalMin):\n",
    "                            neighbors.popleft()\n",
    "                            \n",
    "                        neighborsUnchangedCheck = False\n",
    "                        \n",
    "                    else:\n",
    "                        checkNeeded = False\n",
    "                else:\n",
    "                    checkNeeded = False\n",
    "\n",
    "        #---\n",
    "        \n",
    "        # After checking and cleaning 'neighbors', there are 3 different scenarios how 'neighbors' looks like:\n",
    "        # 1) 'neighbors' is either smaller than or equal to 'binSize' and no removal of indices has been conducted \n",
    "        #\n",
    "        # 2) 'neighbors' is bigger than 'binSize' and no removal of the left-most preds has been conducted because either\n",
    "        # a) the current prediction and left-most prediction are identical\n",
    "        # b) distanceToMin is smaller than or equal to distanceToMax\n",
    "        # c) removing the left most preds would have made neighbors smaller than binSize\n",
    "        #\n",
    "        # 3) 'neighbors' is bigger than or equal to 'binSize' and as many left-most preds have been removed until the removal\n",
    "        # of further preds would have made neighbors smaller than 'binSize'.\n",
    "        \n",
    "        # Now we compute distanceMin and distanceMax and start looking for new nearest neighbors beginning with 'neighborsMaxIter',\n",
    "        # which is obviously the next bigger pred to the currently highest pred that is part of 'neighbors'.\n",
    "        #\n",
    "        # We have to consider 5 different cases:\n",
    "        # 1) 'neighbors' is smaller than 'binSize' --> the new pred is always added\n",
    "        # 2) The new pred is closer to the current prediction than the left-most prediction --> the left-most predictions are\n",
    "        # all removed and the new predictions are added.\n",
    "        # 3) The new prediction is exactly as far away as the left-most prediction --> new predictions are added without any removal\n",
    "        # 4) The new prediction is as far away as the currently right-most prediction of 'neighbors' --> new predictions are added\n",
    "        # without any removal\n",
    "        # 5) None of the above 4 cases apply --> break and set 'neighborsMaxIter' to the current index k of 'yPredUnique'.\n",
    "        # \n",
    "        # NOTE: Can case (4) ever happen? I mean, we are iterating over 'yPredUnique'!?!?!\n",
    "\n",
    "        distanceToMin = predCurrent - yPred[neighbors[0]]\n",
    "        distanceToMax = yPred[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "        \n",
    "        # If the neighbors-object has not been changed since the last considered point\n",
    "        # prediction for which we computed the nearest neighbors, we simply reuse the same\n",
    "        # neighbors object in RAM (see below).\n",
    "        neighborsUnchangedLoop = True\n",
    "        \n",
    "        for k in range(neighborsMaxIter, len(yPredUnique), 1):\n",
    "            \n",
    "            predNew = yPredUnique[k]\n",
    "            distance = predNew - predCurrent \n",
    "            \n",
    "            if len(neighbors) < binSize:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                \n",
    "                distanceToMax = yPred[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "                addCounter += counterDict[predNew]\n",
    "                neighborsUnchangedLoop = False\n",
    "                \n",
    "            elif distance < distanceToMin:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                addCounter += counterDict[predNew]\n",
    "                \n",
    "                countIdenticalMin = counterDict[yPred[neighbors[0]]]\n",
    "                for p in range(countIdenticalMin): \n",
    "                    neighbors.popleft()\n",
    "                    \n",
    "                removeCounter += countIdenticalMin\n",
    "                distanceToMin = predCurrent - yPred[neighbors[0]]\n",
    "                distanceToMax = yPred[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "                neighborsUnchangedLoop = False\n",
    "                \n",
    "            elif distance == distanceToMin:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                \n",
    "                distanceToMax = yPred[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "                addCounter += counterDict[predNew]\n",
    "                neighborsUnchangedLoop = False\n",
    "                \n",
    "            elif distance == distanceToMax:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                addCounter += counterDict[predNew]\n",
    "                neighborsUnchangedLoop = False\n",
    "                \n",
    "            # We only ever end up here, if 'predNew' hasn't been added to 'neighbors'.\n",
    "            else:\n",
    "                    \n",
    "                neighborsMaxIter = k\n",
    "                break\n",
    "\n",
    "            # If we end up down here, it means that all train preds have sucessuflly been added to the \n",
    "            # current neighborhood. For that reason, neighborsMaxIter has to be set to len(yPred) in order\n",
    "            # to stop the code from starting the loop.\n",
    "            # We only end up here, if the highest point prediction has been added during this iteration.\n",
    "            # We have to treat this is a special case using the 'addedHighestPredDuringIteration' variable.\n",
    "            if k == (len(yPredUnique) - 1):\n",
    "                neighborsMaxIter = len(yPredUnique)\n",
    "                \n",
    "        # Ideas to solve RAM problems:\n",
    "        # 1) Save neighbors into Sparse Array (neighbors have to be padded in some cases)\n",
    "        # 2) Don't save redundant neighbors objects (already done here)\n",
    "        # 3) Save neighbors Objects in Memory\n",
    "        \n",
    "        if neighborsUnchangedCheck and neighborsUnchangedLoop:\n",
    "            neighborsPerPred[predCurrent] = neighborsPerPred[yPredUnique[i-1]]\n",
    "        else:\n",
    "            neighborsPerPred[predCurrent] = np.array(neighbors, dtype = 'uintc')\n",
    "\n",
    "        neighborsRemoved.append(removeCounter)\n",
    "        neighborsAdded.append(addCounter)\n",
    "        \n",
    "    #---\n",
    " \n",
    "    return neighborsPerPred, np.array(neighborsRemoved), np.array(neighborsAdded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdbc68-a9ce-4384-a9f3-00b7a6adcf33",
   "metadata": {},
   "source": [
    "### Get Neighbor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980d802-9d12-49aa-adfc-a4377c8dc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def getNeighborsTest(binSize: int, # Size of the bins of values of `yPred` being grouped together.\n",
    "                     yPred: np.ndarray, # 1-dimensional array of predicted values.\n",
    "                     yPredTrain: np.ndarray, # 1-dimensional array of predicted train values.\n",
    "                     # Dict containing the neighbors of all train samples. Keys are the train predictions.\n",
    "                     neighborsDictTrain: dict, \n",
    "                     ):\n",
    "    \"Used to generate the neighboorhoods used by `LevelSetKDEx` to compute density estimations.\"\n",
    "    \n",
    "    duplicationDict = defaultdict(list)\n",
    "    counterDict = defaultdict(int)\n",
    "    \n",
    "    for index, value in enumerate(yPredTrain):\n",
    "        duplicationDict[value].append(index)\n",
    "        counterDict[value] += 1\n",
    "    \n",
    "    yPredTrainUnique = np.sort(list(duplicationDict.keys()))\n",
    "    yPredUnique = np.unique(yPred)\n",
    "    \n",
    "    yPredTrainUniqueRanking = {value: index for index, value in enumerate(yPredTrainUnique)}\n",
    "    \n",
    "    trainIndicesClosest = np.searchsorted(a = yPredTrainUnique, v = yPredUnique, side = 'right') - 1\n",
    "    \n",
    "    # Needed if any yPred value is lower than all yPredTrain values\n",
    "    trainIndicesClosest = np.clip(a = trainIndicesClosest, a_min = 0, a_max = None) \n",
    "    yPredTrainClosest = yPredTrainUnique[trainIndicesClosest]\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    neighborsPerPred = dict()\n",
    "\n",
    "    for i, predCurrent in enumerate(yPredUnique):\n",
    "        \n",
    "        neighbors = deque(neighborsDictTrain[yPredTrainClosest[i]])\n",
    "        \n",
    "        neighborsMaxIndex = yPredTrainUniqueRanking[yPredTrain[neighbors[len(neighbors) - 1]]]\n",
    "        \n",
    "        distanceToMin = predCurrent - yPredTrain[neighbors[0]]\n",
    "        \n",
    "        # Check and Clean current neighborhood before starting the loop\n",
    "        if len(neighbors) > binSize:\n",
    "            \n",
    "            checkNeeded = True\n",
    "            while checkNeeded:\n",
    "                \n",
    "                distanceToMin = predCurrent - yPredTrain[neighbors[0]]\n",
    "                distanceToMax = yPredTrain[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "                if distanceToMin > 0 and distanceToMin > distanceToMax:\n",
    "                    countIdenticalValuesLeftSide = counterDict[yPredTrain[neighbors[0]]]\n",
    "\n",
    "                    if len(neighbors) - countIdenticalValuesLeftSide >= binSize:\n",
    "                        for p in range(countIdenticalValuesLeftSide):\n",
    "                            neighbors.popleft()\n",
    "                    else:\n",
    "                        checkNeeded = False\n",
    "                else:\n",
    "                    checkNeeded = False\n",
    "\n",
    "        #---\n",
    "\n",
    "        distanceToMin = predCurrent - yPredTrain[neighbors[0]]\n",
    "        distanceToMax = yPredTrain[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "        for k in range(neighborsMaxIndex + 1, len(yPredTrainUnique), 1):\n",
    "            predNew = yPredTrainUnique[k]\n",
    "            distance = predNew - predCurrent \n",
    "                \n",
    "            if len(neighbors) < binSize:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                distanceToMax = yPredTrain[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "            elif distance < distanceToMin:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                \n",
    "                for p in range(counterDict[yPredTrain[neighbors[0]]]): \n",
    "                    neighbors.popleft()\n",
    "\n",
    "                distanceToMin = predCurrent - yPredTrain[neighbors[0]]\n",
    "                distanceToMax = yPredTrain[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "            elif distance == distanceToMin:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "                distanceToMax = yPredTrain[neighbors[len(neighbors) - 1]] - predCurrent\n",
    "\n",
    "            elif distance == distanceToMax:\n",
    "                neighbors.extend(duplicationDict[predNew])\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        neighborsPerPred[predCurrent] = np.array(neighbors, dtype = 'uintc')\n",
    "        \n",
    "    #---\n",
    " \n",
    "    return neighborsPerPred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7168278-74bf-4af3-ba23-525d490d2bfb",
   "metadata": {},
   "source": [
    "### Get Kernel Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e08de-2803-4c80-a715-f98be3557de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Setting 'efficientRAM = TRUE' is only necessary when there are roughly umore than 200k training observations.\n",
    "# This setting causes the run-time of the algorithm to linearly depend on the 'binSize', which can become quite\n",
    "# slow for bin-sizes above 10k.\n",
    "\n",
    "def getKernelValues(yPred,\n",
    "                    yPredTrain,\n",
    "                    neighborsDictTest,\n",
    "                    neighborsDictTrain,\n",
    "                    neighborsRemoved,\n",
    "                    neighborsAdded,\n",
    "                    binSize,\n",
    "                    efficientRAM = False):\n",
    "    \n",
    "    duplicationDict = defaultdict(list)\n",
    "    counterDict = defaultdict(int)\n",
    "    \n",
    "    for index, value in enumerate(yPredTrain):\n",
    "        duplicationDict[value].append(index)\n",
    "        counterDict[value] += 1\n",
    "    \n",
    "    yPredTrainUnique = np.sort(list(neighborsDictTrain.keys()))\n",
    "    trainIndicesClosest = np.searchsorted(a = yPredTrainUnique, v = yPred, side = 'right') - 1\n",
    "    \n",
    "    # Needed if any yPred value is lower than all yPredTrain values\n",
    "    trainIndicesClosest = np.clip(a = trainIndicesClosest, a_min = 0, a_max = None)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    kernelValuesList = list()\n",
    "    weightsDataList = list()\n",
    "    \n",
    "    for i in range(len(yPred)):\n",
    "        \n",
    "        trainIndexClosest = trainIndicesClosest[i]\n",
    "        neighbors = neighborsDictTest[yPred[i]]\n",
    "        sizeBin = len(neighbors)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if trainIndexClosest + 1 <= len(yPredTrainUnique) - 1:\n",
    "            neighborsTrainClosest = neighborsDictTrain[yPredTrainUnique[trainIndexClosest+1]]\n",
    "            sharedNeighborsClosest = len(set(neighbors) & set(neighborsTrainClosest))\n",
    "            \n",
    "            if trainIndexClosest + 1 == len(yPredTrainUnique) - 1:\n",
    "                kernelValuesRight = np.expand_dims(2 * sharedNeighborsClosest / (sizeBin + len(neighborsTrainClosest)), axis = 0)\n",
    "                \n",
    "            else:\n",
    "                removeCum = np.concatenate([np.arange(1), neighborsRemoved[trainIndexClosest+2:len(yPredTrainUnique)]], axis = 0).cumsum()\n",
    "                addCum = np.concatenate([np.arange(1), neighborsAdded[trainIndexClosest+2:len(yPredTrainUnique)]], axis = 0).cumsum()\n",
    "\n",
    "                sharedNeighborsRight = np.clip(a = sharedNeighborsClosest - removeCum, a_min = 0, a_max = None)\n",
    "                binSizesRight = len(neighborsTrainClosest) - removeCum + addCum\n",
    "\n",
    "                kernelValuesRight = 2 * sharedNeighborsRight / (sizeBin + binSizesRight)\n",
    "        \n",
    "        else:\n",
    "            kernelValuesRight = np.arange(0)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        # trainIndexClosest == -1 means that the test pred is lower than any train pred\n",
    "        if trainIndexClosest >= 0:\n",
    "            neighborsTrainClosest = neighborsDictTrain[yPredTrainUnique[trainIndexClosest]]\n",
    "            sharedNeighborsClosest = len(set(neighbors) & set(neighborsTrainClosest))\n",
    "            \n",
    "            if trainIndexClosest == 0:\n",
    "                kernelValuesLeft = np.expand_dims(2 * sharedNeighborsClosest / (sizeBin + len(neighborsTrainClosest)), axis = 0)\n",
    "            \n",
    "            else:\n",
    "                neighborsRemovedFlip = np.flip(neighborsRemoved[1:trainIndexClosest+1])\n",
    "                addCum = np.concatenate([np.arange(1), neighborsRemovedFlip], axis = 0).cumsum()\n",
    "\n",
    "                neighborsAddedFlip = np.flip(neighborsAdded[1:trainIndexClosest+1])\n",
    "                removeCum = np.concatenate([np.arange(1), neighborsAddedFlip], axis = 0).cumsum()\n",
    "\n",
    "                sharedNeighborsLeft = np.clip(a = sharedNeighborsClosest - removeCum, a_min = 0, a_max = None)\n",
    "                binSizesLeft = len(neighborsTrainClosest) - removeCum + addCum\n",
    "\n",
    "                kernelValuesLeft = np.flip(2 * sharedNeighborsLeft / (sizeBin + binSizesLeft))\n",
    "                    \n",
    "        else:\n",
    "            kernelValuesLeft = np.arange(0)\n",
    "\n",
    "        #---\n",
    "        \n",
    "        kernelValuesUnique = np.concatenate([kernelValuesLeft, kernelValuesRight], axis = 0)\n",
    "        \n",
    "        if efficientRAM:\n",
    "            weightsList = []\n",
    "            indicesList = []\n",
    "\n",
    "            for index in np.where(kernelValuesUnique > 0)[0]:\n",
    "                indices = duplicationDict[yPredTrainUnique[index]]\n",
    "                indicesList.extend(indices)\n",
    "\n",
    "                weight = kernelValuesUnique[index]\n",
    "                weightsList.extend([weight for i in range(len(indices))])\n",
    "            \n",
    "            weightsArray = np.array(weightsList) / sum(weightsList)            \n",
    "            weightsDataList.append((weightsArray, np.array(indicesList, dtype = 'uintc')))\n",
    "        \n",
    "        else:       \n",
    "            kernelValuesList.append(kernelValuesUnique)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    if efficientRAM:\n",
    "        return weightsDataList\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        kernelMatrixUnique = np.array(kernelValuesList)\n",
    "        kernelMatrix = np.zeros(shape = (len(yPred), len(yPredTrain)))\n",
    "\n",
    "        for index, predTrain in enumerate(yPredTrainUnique):\n",
    "            kernelMatrix[:, duplicationDict[predTrain]] = kernelMatrixUnique[:, [index]]\n",
    "\n",
    "        for i in range(len(yPred)):\n",
    "            indices = np.where(kernelMatrix[i,:] > 0)[0]\n",
    "            weights = kernelMatrix[i, indices]\n",
    "            weights = weights / weights.sum()\n",
    "            weightsDataList.append((weights, indices))\n",
    "\n",
    "        return weightsDataList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc893f",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9679bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_clustering(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 nClusters: int=10, # Number of clusters to form as well as number of centroids to generate.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(nClusters, (np.int32, np.int64)):\n",
    "            nClusters = int(nClusters)\n",
    "        \n",
    "        elif not isinstance(nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "                \n",
    "        self.nClusters = nClusters\n",
    "\n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.kmeans = None\n",
    "        self.clusterDict = None\n",
    "        self.clusterSizes = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.nClusters is None:\n",
    "            raise ValueError(\"'nClusters' must be specified to fit the LSx estimator!\")\n",
    "        \n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(self.nClusters, (np.int32, np.int64)):\n",
    "            self.nClusters = int(self.nClusters)\n",
    "        \n",
    "        elif not isinstance(self.nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "        \n",
    "        # Check if nClusters is positive\n",
    "        if self.nClusters <= 0:\n",
    "            raise ValueError(\"'nClusters' must be positive!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # Reshape yPred to 2D array with shape (nSamples, 1) and convert to float32.\n",
    "        yPredMod = yPred.reshape(-1, 1).astype(np.float32)\n",
    "        \n",
    "        kmeans = faiss.Kmeans(d = 1, k = self.nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "        # self.centers = kmeans.centroids\n",
    "        \n",
    "        clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        # Get the cluster labels for each sample in yPred as a dict with keys being the cluster labels\n",
    "        # and values being the indices of the samples in yPred that belong to that cluster.\n",
    "        # Create another dict with keys being the cluster labels and values being the size of each cluster.\n",
    "        clusterDict = defaultdict(list)\n",
    "        clusterSizes = defaultdict(int)\n",
    "\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            clusterDict[cluster].append(index)\n",
    "            clusterSizes[cluster] += 1\n",
    "        \n",
    "        clusterSizes = pd.Series(clusterSizes)\n",
    "       \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.kmeans = kmeans\n",
    "        self.clusterDict = clusterDict\n",
    "        self.clusterSizes = clusterSizes\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        # Reshape yPred to 2D array with shape (nSamples, 1) and convert to float32.\n",
    "        yPred = yPred.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "        # Get cluster labels of yPred\n",
    "        clusterLabels = self.kmeans.assign(yPred)[1]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / self.clusterSizes[cluster], self.clusterSizes[cluster]), np.array(self.clusterDict[cluster], dtype = 'uintc')) \n",
    "                            for cluster in clusterLabels]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_clustering2(BaseWeightsBasedEstimator, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 nClusters: int=10, # Number of clusters to form as well as number of centroids to generate.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(nClusters, (np.int32, np.int64)):\n",
    "            nClusters = int(nClusters)\n",
    "        \n",
    "        elif not isinstance(nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "                \n",
    "        self.nClusters = nClusters\n",
    "\n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.kmeans = None\n",
    "        self.clusterDict = None\n",
    "        self.clusterSizes = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.nClusters is None:\n",
    "            raise ValueError(\"'nClusters' must be specified to fit the LSx estimator!\")\n",
    "        \n",
    "        # nClusters must either be of type int, np.int64 or np.int32\n",
    "        if isinstance(self.nClusters, (np.int32, np.int64)):\n",
    "            self.nClusters = int(self.nClusters)\n",
    "        \n",
    "        elif not isinstance(self.nClusters, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "\n",
    "        # Check if nClusters is positive\n",
    "        if self.nClusters <= 0:\n",
    "            raise ValueError(\"'nClusters' must be positive!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "\n",
    "        # Cluster yPred into nClusters many clusters\n",
    "        kmeans = KMeans(n_clusters = self.nClusters, random_state = 0, n_init = 10).fit(yPred.reshape(-1, 1))\n",
    "\n",
    "        # Get the cluster labels for each sample in yPred as a dict with keys being the cluster labels\n",
    "        # and values being the indices of the samples in yPred that belong to that cluster.\n",
    "        # Create another dict with keys being the cluster labels and values being the size of each cluster.\n",
    "        clusterDict = defaultdict(list)\n",
    "        clusterSizes = defaultdict(int)\n",
    "\n",
    "        for index, cluster in enumerate(kmeans.labels_):\n",
    "            clusterDict[cluster].append(index)\n",
    "            clusterSizes[cluster] += 1\n",
    "       \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.kmeans = kmeans\n",
    "        self.clusterDict = clusterDict\n",
    "        self.clusterSizes = clusterSizes\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        \n",
    "        # Get cluster labels of yPred\n",
    "        clusterLabels = self.kmeans.predict(yPred.reshape(-1, 1))\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / self.clusterSizes[label], self.clusterSizes[label]), np.array(self.clusterDict[label], dtype = 'uintc')) \n",
    "                            for label in clusterLabels]\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain,\n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81936321-7fe7-4b27-91e3-a4a1444bd29e",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "\n",
    "# import ipdb\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from dddex.loadData import *\n",
    "# from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time \n",
    "# import psutil\n",
    "# import os\n",
    "# import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523b02c-ce7b-4969-81e7-d09e60db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataBakery(returnXY = True)\n",
    "\n",
    "# LGBM = LGBMRegressor(n_jobs = 1).fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbeb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# LSKDEx_drf = LevelSetKDEx_DRF(estimator = LGBM, binSize = 100)\n",
    "# LSKDEx_drf.fit(XTrain, yTrain)\n",
    "\n",
    "# weightsDataList = LSKDEx_drf.getWeights(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68573e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659721851348877\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "\n",
    "# import time\n",
    "# start = time.time()\n",
    "# LSKDEx = LevelSetKDEx(estimator = LGBM, binSize = 100)\n",
    "\n",
    "# LSKDEx.fit(XTrain, yTrain)\n",
    "# weights = LSKDEx.getWeights(XTest)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb537e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bb0dd13",
   "metadata": {},
   "source": [
    "# LSx based on DRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import ipdb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from dddex.loadData import *\n",
    "from dddex.wSAA import RandomForestWSAA, SampleAverageApproximation\n",
    "import time \n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from drf import drf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "data, XTrain, yTrain, XTest, yTest = loadDataYaz(returnXY = True)\n",
    "\n",
    "LGBM = LGBMRegressor(n_jobs = 1).fit(XTrain, yTrain)\n",
    "yPredTrain = LGBM.predict(XTrain)\n",
    "yPredTest = LGBM.predict(XTest)\n",
    "\n",
    "yPredTrain = pd.DataFrame(yPredTrain)\n",
    "yPredTest = pd.DataFrame(yPredTest)\n",
    "yTrain = pd.Series(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bddde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "DRF = drf(min_node_size = 100, num_trees = 500, num_features = 1, honesty = False, sample_fraction = 0.5, response_scaling = False, mtry = 1, num_threads = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "DRF.fit(yPredTrain, yTrain)\n",
    "weights = DRF.predict(yPredTest).weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    366.000000\n",
       "mean       0.002732\n",
       "std        0.002423\n",
       "min        0.000020\n",
       "25%        0.000590\n",
       "50%        0.001994\n",
       "75%        0.004667\n",
       "max        0.009120\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "# Get statistic of weights of first row#\n",
    "weightsRow = weights[0]\n",
    "pd.Series(weightsRow[weightsRow > 0]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29717899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# LSKDEx = LevelSetKDEx_clustering2(estimator = LGBM, nClusters = 100)\n",
    "\n",
    "# LSKDEx.fit(XTrain, yTrain)\n",
    "# weights = LSKDEx.getWeights(XTest)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9c86d-137e-4d85-a4dc-211987b98589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/kagu/SID/data/dataSID.csv'\n",
    "# data = pd.read_csv(path)\n",
    "\n",
    "# ids = data.id.unique()[0:30]\n",
    "# filtering = [ID in ids for ID in data.id]\n",
    "# data = data[filtering]\n",
    "\n",
    "# X = np.array(data.drop(['demand', 'date', 'id', 'label'], axis = 1))\n",
    "# Y = np.array(data['demand'])\n",
    "\n",
    "# indicesTrain = data['label'] == 'train'\n",
    "# indicesTest = data['label'] == 'test'\n",
    "\n",
    "# XTrain = X[indicesTrain]\n",
    "# yTrain = Y[indicesTrain]\n",
    "\n",
    "# XTest = X[indicesTest]\n",
    "# yTest = Y[indicesTest]\n",
    "\n",
    "# dataTrain = data[indicesTrain]\n",
    "# dataTest = data[indicesTest]\n",
    "\n",
    "# scalingList = dataTest['scalingValue'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6e066-7688-4d99-9dff-b7b2b0ce8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9e826-dadc-45ba-872f-a66e4fd72c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = psutil.Process(os.getpid())\n",
    "# print(f\"Memory used by Jupyter notebook: {process.memory_info().rss / 2**20:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764f024-c9fd-429b-b1f9-fe4503e0ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM = LGBMRegressor(boosting_type = 'gbdt',\n",
    "#                      n_jobs = 1)\n",
    "\n",
    "# LGBM.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54beaf70-9c79-4704-82d9-5a284e0823ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# LSKDEx = LevelSetKDEx(estimator = LGBM, binSize = 5000)\n",
    "# LSKDEx.fit(XTrain, yTrain)\n",
    "# print(time.time() - start)\n",
    "\n",
    "# yPredTrain = LSKDEx.yPredTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523732cb-67da-467e-acf2-186df68d5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredTrain[LSKDEx.neighborsDictTrain[list(LSKDEx.neighborsDictTrain.keys())[100]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322eb337-a075-4f47-aa2e-780b1a05fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredTrain[LSKDEx.neighborsDictTrain[np.array(list(LSKDEx.neighborsDictTrain.keys()))[-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388de86-a5f8-41d8-90d8-aecb3e576ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = LSKDEx.getWeights(XTest, efficientRAM = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1a641-e3f3-49da-9dbe-9a9f60467161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = LSKDEx.predict(XTest,\n",
    "#                      probs = [0.1, 0.5], \n",
    "#                      scalingList = scalingList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53d7c1-72a5-4525-9b13-e6db6dd9f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = psutil.Process(os.getpid())\n",
    "# print(f\"Memory used by Jupyter notebook: {process.memory_info().rss / 2**20:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3ba66-1f21-44e0-b3d9-183b8c44d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTrainMod = XTrain[0:10000]\n",
    "# yTrainMod = yTrain[0:10000]\n",
    "# XTestMod = XTest[0:10]\n",
    "\n",
    "# yPredTrainMod = LGBM.predict(XTrainMod)\n",
    "# yPredTestMod = LGBM.predict(XTestMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36723cd7-309b-462d-8d26-829b1854f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# LSKDEx = LevelSetKDEx(estimator = LGBM, binSize = 100, weightsByDistance = False)\n",
    "# LSKDEx.fit(XTrainMod, yTrainMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ccaac-40eb-4023-a71d-50c5914dc940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# #| hide\n",
    "\n",
    "# res = LSKDEx.solveKernelGLS(X = XTrainMod, sigma = 0.5, c = yPredTrainMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a97a0-0779-4d0f-ae4c-f2ca0a61ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# #| hide\n",
    "# res = LSKDEx.getKernelVectorProduct(X1 = XTrainMod, c = yPredTrainMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17f57e-d639-4d71-a0cf-652aff9ebebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "# mean, cov = LSKDEx.getGaussianPosterior(XTrain = XTrainMod, \n",
    "#                                         XTest = XTestMod,\n",
    "#                                         yTrain = yTrainMod,\n",
    "#                                         sigma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e6185-06ec-4182-8719-5a75d2712d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPredTest = LSKDEx.estimator.predict(XTest)\n",
    "# binPerPredTest = np.searchsorted(a = LSKDEx.lowerBoundPerBin, v = yPredTest, side = 'right') - 1\n",
    "\n",
    "# binVectorsTest = [(binPerPredTest == i).reshape(-1, 1) * 1 for i in range(len(LSKDEx.lowerBoundPerBin))]\n",
    "# binVectorsToSliceTest = [np.where(binVector)[0] for binVector in binVectorsTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2273a6-a5c4-4e5f-bf9f-f9fe5a6f432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = binVectorsToSliceTest[2]\n",
    "# cov[v[:, None],  v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8dbd5-5255-4646-86b9-494611148717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d65fd-a8b4-4649-aea5-e73d04723d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(np.ravel(mean)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2db3cb-f6d6-406d-be91-5bcccf5acb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(yTest).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebeec7-8a14-46f7-8c85-c19fa6bde752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = np.concatenate([np.arange(5000)] * 2, axis = 0)\n",
    "# yPredTrain = np.concatenate([np.arange(50000)] * 2, axis = 0)\n",
    "# binSize = 200\n",
    "\n",
    "# neighborsDictTrain, neighborsRemoved, neighborsAdded = generateNeighborhoodsUnique(binSize = binSize,\n",
    "#                                                                                yPred = yPredTrain)\n",
    "\n",
    "# neighborsDictTest = generateNeighborhoodsTestUnique(binSize = binSize,\n",
    "#                                                 yPred = yPred,\n",
    "#                                                 yPredTrain = yPredTrain,\n",
    "#                                                 neighborsDictTrain = neighborsDictTrain)\n",
    "\n",
    "# start = time.time()\n",
    "# kernelValuesList = getKernelValues(binSize = binSize,\n",
    "#                                    yPred = yPred,\n",
    "#                                    yPredTrain = yPredTrain,\n",
    "#                                    neighborsDictTest = neighborsDictTest,\n",
    "#                                    neighborsDictTrain = neighborsDictTrain,\n",
    "#                                    neighborsRemoved = neighborsRemoved,\n",
    "#                                    neighborsAdded = neighborsAdded)\n",
    "# print(time.time() - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
