{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Class\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp baseClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "from fastcore.script import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import copy\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Level-Set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseLSx:\n",
    "    \"\"\"\n",
    "    Base class for the Level-Set based approaches. This class is not supposed to be used directly.\n",
    "    Use derived classes instead.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a `fit` and `predict` method (implementing the scikit-learn estimator interface).\n",
    "                 ):\n",
    "        \n",
    "        if not (hasattr(estimator, 'predict') and callable(estimator.predict)):\n",
    "            raise ValueError(\"'estimator' has to have a 'predict'-method!\")\n",
    "            \n",
    "        self.estimator = copy.deepcopy(estimator)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self: BaseLSx, \n",
    "                     # Feature matrix for which point predictions are computed based\n",
    "                     # on the point forecasting model specified via `estimator`.\n",
    "                     X: np.ndarray \n",
    "                     ):\n",
    "        \n",
    "        return self.estimator.predict(X)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def refitPointEstimator(self: BaseLSx, \n",
    "                            X: np.ndarray, # Input feature matrix\n",
    "                            y: np.ndarray, # Target values\n",
    "                            **kwargs):\n",
    "        \n",
    "        try:\n",
    "            self.estimator.set_params(**kwargs)\n",
    "        except:\n",
    "            raise NotImplementedError(\"The 'estimator' object has no 'set_params' method, so the\"\n",
    "                                      \"provided parameters via **kwargs can't be set!\")\n",
    "        else:\n",
    "            setattr(self, 'estimator', self.estimator.fit(X = X, y = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(BaseLSx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(BaseLSx.pointPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(BaseLSx.refitPointEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights-Based Predictor - Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseWeightsBasedEstimator(BaseEstimator):\n",
    "    \"\"\" \n",
    "    Base class that implements the 'prediction'-method for approaches based \n",
    "    on a reweighting of the empirical distribution. This class is not supposed\n",
    "    to be used directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None,\n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \"\"\"\n",
    "        Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
    "        of the returned list depends on the specified value of `outputType`:\n",
    "        \n",
    "        - **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
    "          of each training sample.\n",
    "        - **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
    "          one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
    "          Note: This is the most memory and computationally efficient output type.\n",
    "        - **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
    "          corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
    "        - **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
    "          one the corresponding value of `yTrain`.\n",
    "        - **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
    "          the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self : BaseWeightsBasedEstimator, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None,\n",
    "                ) -> np.ndarray: \n",
    "        \n",
    "        \"\"\"\n",
    "        Predict p-quantiles based on a reweighting of the empirical distribution function.\n",
    "        If the number of observations of either `X` or `yTrain` is above a certain threshold, \n",
    "        the results aren't calculated in one go, but rather by sequentially computing the\n",
    "        predictions for 100 different subsets of the observations of `X`. We do this to\n",
    "        avoid because intermediary objects like `distributionDataList` are stored in \n",
    "        '(X.shape[0], XTrain.shape[0])' space in the worst case. Hence, this can cause the \n",
    "        program to crash in extreme cases.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # CHECKS\n",
    "        if isinstance(probs, int) or isinstance(probs, float):\n",
    "            if probs >= 0 and probs <= 1:\n",
    "                probs = [probs]\n",
    "            else:\n",
    "                raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")           \n",
    "                 \n",
    "        if any([prob > 1 or prob < 0 for prob in probs]):\n",
    "            raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")\n",
    "            \n",
    "        try:\n",
    "            probs = np.array(probs)\n",
    "        except:\n",
    "            raise ValueError(\"Can't convert `probs` to 1-dimensional array.\")\n",
    "            \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if (X.shape[0] > 100) and ((X.shape[0] > 25000) or (self.yTrain.shape[0] > 25000)):\n",
    "            \n",
    "            subsetSize = int(X.shape[0] / 100)\n",
    "            splitPoints = [subsetSize * i for i in range(100)]\n",
    "            splitPoints.append(X.shape[0])\n",
    "            \n",
    "        else:\n",
    "            splitPoints = [0, X.shape[0]]\n",
    "            \n",
    "        \n",
    "        quantilesDfList = list()\n",
    "        \n",
    "        for i in range(len(splitPoints) - 1):\n",
    "            \n",
    "            XSubset = X[splitPoints[i]:splitPoints[i+1], :]\n",
    "            \n",
    "            if not scalingList is None:\n",
    "                scalingListSubset = scalingList[splitPoints[i]:splitPoints[i+1]]\n",
    "            else:\n",
    "                scalingListSubset = None\n",
    "            \n",
    "            distributionDataList = self.getWeights(X = XSubset,\n",
    "                                                   outputType = 'cumulativeDistribution',\n",
    "                                                   scalingList = scalingListSubset)        \n",
    "\n",
    "            quantilesList = list()\n",
    "\n",
    "            for probsDistribution, valuesDistribution in distributionDataList:\n",
    "\n",
    "                # A tolerance term of 10^-8 is substracted from prob to account for rounding errors due to numerical precision.\n",
    "                quantileIndices = np.searchsorted(a = probsDistribution, v = probs - 10**-8, side = 'left')\n",
    "                quantilesList.append(valuesDistribution[quantileIndices])\n",
    "\n",
    "            quantilesDf = pd.DataFrame(quantilesList)\n",
    "            quantilesDf.columns = probs\n",
    "            \n",
    "            quantilesDfList.append(quantilesDf)\n",
    "            \n",
    "        quantilesDf = pd.concat(quantilesDfList, axis = 0).reset_index(drop = True)\n",
    "        \n",
    "        return quantilesDf\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def sampleScenarios(self,\n",
    "                        X: np.ndarray, # Feature matrix for which samples are computed.\n",
    "                        n: int,\n",
    "                        # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                        # density of each sample for scaling purposes.\n",
    "                        scalingList: list=None,\n",
    "                        ) -> np.ndarray: # array-like of shape (n_samples_in, n_scenarios)\n",
    "        \n",
    "        distributionData = self.getWeights(X = X,\n",
    "                                           outputType = 'cumulativeDistribution',\n",
    "                                           scalingList = scalingList)\n",
    "        \n",
    "        sampleList = list()\n",
    "        for probs, values in distributionData:\n",
    "            randomProbs = uniform(size = n)\n",
    "            randomProbs = randomProbs.reshape(n, 1)\n",
    "            sample = values[np.argmax(probs >= randomProbs, axis = 1)]\n",
    "\n",
    "            sampleList.append(sample)\n",
    "        \n",
    "        sampleMatrix = np.concatenate([sampleList], axis = 0)\n",
    "        \n",
    "        return sampleMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/baseClasses.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BaseWeightsBasedEstimator.getWeights\n",
       "\n",
       ">      BaseWeightsBasedEstimator.getWeights (X:numpy.ndarray,\n",
       ">                                            outputType:str='onlyPositiveWeights\n",
       ">                                            ', scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/baseClasses.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BaseWeightsBasedEstimator.getWeights\n",
       "\n",
       ">      BaseWeightsBasedEstimator.getWeights (X:numpy.ndarray,\n",
       ">                                            outputType:str='onlyPositiveWeights\n",
       ">                                            ', scalingList:list=None)\n",
       "\n",
       "Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
       "of the returned list depends on the specified value of `outputType`:\n",
       "\n",
       "- **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
       "  of each training sample.\n",
       "- **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
       "  Note: This is the most memory and computationally efficient output type.\n",
       "- **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
       "  corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "- **cumDistribution**: A tuple. The first element of the tuple represents the probabilities and the second \n",
       "  one the corresponding value of `yTrain`.\n",
       "- **cumDistributionSummarized**: A tuple. The first element of the tuple represents the probabilities and \n",
       "  the second one the corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional density estimates are computed. |\n",
       "| outputType | str | onlyPositiveWeights | Specifies structure of the returned density estimates. One of: <br>'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized' |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the estimated <br>density of each sample for scaling purposes. |\n",
       "| **Returns** | **list** |  | **List whose elements are the conditional density estimates for the samples specified by `X`.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseWeightsBasedEstimator.getWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/baseClasses.py#L108){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BaseWeightsBasedEstimator.predict\n",
       "\n",
       ">      BaseWeightsBasedEstimator.predict (X:numpy.ndarray, probs:list,\n",
       ">                                         scalingList:list=None)\n",
       "\n",
       "Predict p-quantiles based on a reweighting of the empirical distribution function.\n",
       "If the number of observations of either `X` or `yTrain` is above a certain threshold, \n",
       "the results aren't calculated in one go, but rather by sequentially computing the\n",
       "predictions for 100 different subsets of the observations of `X`. We do this to\n",
       "avoid because intermediary objects like `distributionDataList` are stored in \n",
       "'(X.shape[0], XTrain.shape[0])' space in the worst case. Hence, this can cause the \n",
       "program to crash in extreme cases.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional quantiles are computed. |\n",
       "| probs | list |  | Probabilities for which quantiles are computed. |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the predictions<br>of each sample to rescale values. |\n",
       "| **Returns** | **np.ndarray** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/kaiguender/dddex/blob/main/dddex/baseClasses.py#L108){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### BaseWeightsBasedEstimator.predict\n",
       "\n",
       ">      BaseWeightsBasedEstimator.predict (X:numpy.ndarray, probs:list,\n",
       ">                                         scalingList:list=None)\n",
       "\n",
       "Predict p-quantiles based on a reweighting of the empirical distribution function.\n",
       "If the number of observations of either `X` or `yTrain` is above a certain threshold, \n",
       "the results aren't calculated in one go, but rather by sequentially computing the\n",
       "predictions for 100 different subsets of the observations of `X`. We do this to\n",
       "avoid because intermediary objects like `distributionDataList` are stored in \n",
       "'(X.shape[0], XTrain.shape[0])' space in the worst case. Hence, this can cause the \n",
       "program to crash in extreme cases.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| X | np.ndarray |  | Feature matrix for which conditional quantiles are computed. |\n",
       "| probs | list |  | Probabilities for which quantiles are computed. |\n",
       "| scalingList | list | None | Optional. List with length X.shape[0]. Values are multiplied to the predictions<br>of each sample to rescale values. |\n",
       "| **Returns** | **np.ndarray** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(BaseWeightsBasedEstimator.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights-Based Predictor - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseWeightsBasedEstimator_multivariate(BaseEstimator):\n",
    "    \"\"\" \n",
    "    Base class that implements the 'prediction'-method for approaches based \n",
    "    on a reweighting of the empirical distribution. This class is not supposed\n",
    "    to be used directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None,\n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \"\"\"\n",
    "        Computes estimated conditional density for each sample specified by `X`. The concrete structure of each element \n",
    "        of the returned list depends on the specified value of `outputType`:\n",
    "        \n",
    "        - **all**: An array with the same length as the number of training samples. Each entry represents the probability \n",
    "          of each training sample.\n",
    "        - **onlyPositiveWeights**: A tuple. The first element of the tuple represents the probabilities and the second \n",
    "          one the indices of the corresponding training sample. Only probalities greater than zero are returned. \n",
    "          Note: This is the most memory and computationally efficient output type.\n",
    "        - **summarized**: A tuple. The first element of the tuple represents the probabilities and the second one the \n",
    "          corresponding value of `yTrain`. The probabilities corresponding to identical values of `yTrain` are aggregated.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
