{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72025364-2e34-4aba-87de-ff5a8b382900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d902e5c-d369-4a96-b340-468c7f8c6cd7",
   "metadata": {},
   "source": [
    "# Level-Set Based Kernel Density Estimation for multivariate Predictors\n",
    "> Defining the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` which turn any point predictor into a conditional kernel density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a04071-5847-4a40-b5a6-f017de16b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp levelSetKDEx_multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8cef21-4158-4307-b898-6bc03398a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d95d3a6-a5e0-46de-b191-d1de0281f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from collections import defaultdict, Counter, deque\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from dddex.baseClasses import BaseLSx, BaseWeightsBasedEstimator_multivariate\n",
    "from dddex.wSAA import SampleAverageApproximation\n",
    "from dddex.utils import restructureWeightsDataList_multivariate\n",
    "from datasetsDynamic.loadDataYaz import loadDataYaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c92985-188d-484f-b270-521f5d2497bb",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f6a9c3-f82d-47c3-b09d-aad3d49f2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_multivariate(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=None, # Size of the bins created while running fit.\n",
    "                 # Determines behaviour of method `getWeights`. If False, all weights receive the same  \n",
    "                 # value. If True, the distance of the point forecasts is taking into account.\n",
    "                 equalBins: bool=False,\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "        \n",
    "        # Check if binSize is int\n",
    "        if not isinstance(binSize, int):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "        \n",
    "        # Check if equalBins is bool\n",
    "        if not isinstance(equalBins, bool):\n",
    "            raise ValueError(\"'equalBins' must be a boolean!\")\n",
    "        \n",
    "        self.equalBins = equalBins\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.binSize is None:\n",
    "            raise ValueError(\"'binSize' must be specified to fit the LSx estimator!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        nClusters = int(np.ceil(yPred.shape[0] / self.binSize))\n",
    "        yPredMod = yPred.astype(np.float32)\n",
    "        \n",
    "        kmeans = faiss.Kmeans(d = yPredMod.shape[1], k = nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "        self.centers = kmeans.centroids\n",
    "        \n",
    "        if self.equalBins:\n",
    "            clusterAssignments = self._getEqualSizedClusters(y = yPredMod)            \n",
    "            \n",
    "        else:\n",
    "            clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        \n",
    "        indicesPerBin = defaultdict(list)\n",
    "        clusterSizes = defaultdict(int)\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            indicesPerBin[cluster].append(index)\n",
    "            clusterSizes[cluster] += 1\n",
    "        \n",
    "        clusterSizes = pd.Series(clusterSizes)\n",
    "        \n",
    "\n",
    "        clustersTooSmall = clusterSizes.index[np.where(clusterSizes < self.binSize / 2)[0]]\n",
    "        \n",
    "        if len(clustersTooSmall) > 0:\n",
    "            centersToMerge = self.centers[clustersTooSmall]\n",
    "            centersRemaining = np.delete(self.centers, clustersTooSmall, axis = 0)\n",
    "\n",
    "            nearestCenterSearch = KDTree(centersRemaining)\n",
    "            clusterMergeDict = dict(zip(clustersTooSmall, nearestCenterSearch.query(centersToMerge)[1]))\n",
    "            \n",
    "            for clusterToMerge, newCluster in clusterMergeDict.items():\n",
    "                indicesPerBin[newCluster].extend(indicesPerBin[clusterToMerge])\n",
    "                indicesPerBin[clusterToMerge] = None\n",
    "        \n",
    "        else:\n",
    "            clusterMergeDict = None\n",
    "        \n",
    "        indicesPerBin = {binIndex: np.array(indices) for binIndex, indices in indicesPerBin.items()}\n",
    "        \n",
    "        #---\n",
    "\n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.kmeans = kmeans\n",
    "        self.clusterMergeDict = clusterMergeDict\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def _getEqualSizedClusters(self,\n",
    "                               y,\n",
    "                               ):\n",
    "            \n",
    "        centers = self.centers.reshape(-1, 1, y.shape[-1]).repeat(self.binSize, 1).reshape(-1, y.shape[-1])\n",
    "\n",
    "        distance_matrix = cdist(y, centers)\n",
    "        clusterAssignments = linear_sum_assignment(distance_matrix)[1]//self.binSize\n",
    "\n",
    "        return clusterAssignments\n",
    "\n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X).astype(np.float32)\n",
    "        \n",
    "        if len(yPred.shape) == 1:\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        if self.equalBins:\n",
    "            binPerPred = self._getEqualSizedClusters(y = yPred)\n",
    "            \n",
    "        else:\n",
    "            binPerPred = self.kmeans.assign(yPred)[1]\n",
    "            \n",
    "            binPerPredUnique = np.unique(binPerPred)\n",
    "            clustersToMerge = np.array(list(self.clusterMergeDict.keys()))\n",
    "            clustersToMod = clustersToMerge[[clusterToMerge in binPerPredUnique for clusterToMerge in clustersToMerge]]\n",
    "\n",
    "            if len(clustersToMod) > 0:\n",
    "                binPerPred = np.select([cluster == binPerPred for cluster in clustersToMod], \n",
    "                                       [self.clusterMergeDict[cluster] for cluster in clustersToMod],\n",
    "                                       binPerPred)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        neighborsList = [self.indicesPerBin[binIndex] for binIndex in binPerPred]\n",
    "                \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "332347e2-6360-4271-8142-05b9f4d82d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8dc43-8377-41b9-a5a7-aa5904038841",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import ipdb\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34541b1d-b3a9-4310-ae57-0918cbd18c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "data, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 14,\n",
    "                                                 daysToCut = 0,\n",
    "                                                 normalizeDemand = True,\n",
    "                                                 unstacked = True,\n",
    "                                                 returnXY = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d36f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=10, n_jobs=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=10, n_jobs=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=10, n_jobs=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestRegressor(n_estimators = 10, n_jobs = 1)\n",
    "\n",
    "RF.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff9c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSKDEx = LevelSetKDEx_multivariate(estimator = RF, binSize = 100, equalBins = True)\n",
    "\n",
    "LSKDEx.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f4aad-73e0-41f2-866d-a0e9b5fd4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestWSAA("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2880-1b9e-4d07-ab84-4ba8a2bef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "LGBM = LGBMRegressor(boosting_type = 'gbdt',\n",
    "                     n_jobs = 1)\n",
    "\n",
    "LGBM.fit(X = XTrain, y = yTrain)\n",
    "yPredTrain = LGBM.predict(XTrain)\n",
    "yPredTest = LGBM.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36723cd7-309b-462d-8d26-829b1854f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "| hide\n",
    "\n",
    "LSKDEx = LevelSetKDEx_multivariate(estimator = LGBM, binSize = 100, equalBins = True)\n",
    "LSKDEx.fit(XTrain, yTrain)\n",
    "test = LSKDEx.getWeights(XTest, outputType = 'summarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d2588-8328-4c6c-9a79-72b3f1ade827",
   "metadata": {},
   "outputs": [],
   "source": [
    "| hide\n",
    "\n",
    "LSKDEx = LevelSetKDEx_multivariate(estimator = LGBM, binSize = 100, equalBins = False)\n",
    "LSKDEx.fit(XTrain, yTrain)\n",
    "test2 = LSKDEx.getWeights(XTest, outputType = 'summarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a589d-e58c-4fdd-9450-86d62b746eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
