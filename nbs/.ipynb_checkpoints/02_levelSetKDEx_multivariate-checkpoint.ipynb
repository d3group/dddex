{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72025364-2e34-4aba-87de-ff5a8b382900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d902e5c-d369-4a96-b340-468c7f8c6cd7",
   "metadata": {},
   "source": [
    "# Level-Set Based Kernel Density Estimation for multivariate Predictors\n",
    "> Defining the classes `LevelSetKDEx` and `LevelSetKDEx_kNN` which turn any point predictor into a conditional kernel density estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a04071-5847-4a40-b5a6-f017de16b62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp levelSetKDEx_multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8cef21-4158-4307-b898-6bc03398a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95d3a6-a5e0-46de-b191-d1de0281f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from collections import defaultdict, Counter, deque\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "from dddex.baseClasses import BaseLSx, BaseWeightsBasedEstimator_multivariate\n",
    "from dddex.levelSetKDEx_univariate import generateBins\n",
    "from dddex.wSAA import SampleAverageApproximation, RandomForestWSAA, RandomForestWSAA_LGBM\n",
    "from dddex.utils import restructureWeightsDataList_multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c92985-188d-484f-b270-521f5d2497bb",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_multivariate(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. The point forecasts of the training samples are sorted and \n",
    "    recursively assigned to a bin until the size of the current bin reaches `binSize` many samples. Then\n",
    "    a new bin is created and so on. For a new test sample we check into which bin its point prediction\n",
    "    would have fallen and interpret the training samples of that bin as the empirical distribution function\n",
    "    of this test sample.    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 binSize: int=None, # Size of the bins created while running fit.\n",
    "                 # Determines behaviour of method `getWeights`. If False, all weights receive the same  \n",
    "                 # value. If True, the distance of the point forecasts is taking into account.\n",
    "                 equalBins: bool=False,\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "        \n",
    "        # Check if binSize is int\n",
    "        if not isinstance(binSize, int):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "        \n",
    "        # Check if equalBins is bool\n",
    "        if not isinstance(equalBins, bool):\n",
    "            raise ValueError(\"'equalBins' must be a boolean!\")\n",
    "        \n",
    "        self.equalBins = equalBins\n",
    "        self.binSize = binSize\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.binSize is None:\n",
    "            raise ValueError(\"'binSize' must be specified to fit the LSx estimator!\")\n",
    "            \n",
    "        if self.binSize > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # Compute desired number of clusters dependend on binSize and number of samples\n",
    "        nClusters = int(np.ceil(yPred.shape[0] / self.binSize))\n",
    "\n",
    "        # Modify yPred to be compatible with faiss\n",
    "        yPredMod = yPred.astype(np.float32)\n",
    "        \n",
    "        # Train kmeans model based on the faiss library\n",
    "        kmeans = faiss.Kmeans(d = yPredMod.shape[1], k = nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "\n",
    "        # Get cluster centers created by faiss. IMPORTANT NOTE: not all clusters are used! We will handle that further below.\n",
    "        centersAll = kmeans.centroids\n",
    "        \n",
    "        # Compute the cluster assignment for each sample\n",
    "        if self.equalBins:\n",
    "            clusterAssignments = self._getEqualSizedClusters(y = yPredMod)            \n",
    "        else:\n",
    "            clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        # Based on the clusters and cluster assignments, we can now compute the indices belonging to each bin / cluster\n",
    "        indicesPerBin = defaultdict(list)\n",
    "        binSizes = defaultdict(int)\n",
    "\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            indicesPerBin[cluster].append(index)\n",
    "            binSizes[cluster] += 1\n",
    "\n",
    "        #---\n",
    "\n",
    "        clustersUsed = np.array(list(indicesPerBin.keys()))\n",
    "        clustersOrdered = np.sort(clustersUsed)\n",
    "\n",
    "        centers = centersAll[clustersOrdered]\n",
    "        indicesPerBin = [indicesPerBin[cluster] for cluster in clustersOrdered]\n",
    "        binSizes = np.array([binSizes[cluster] for cluster in clustersOrdered])\n",
    "\n",
    "        #---\n",
    "\n",
    "        # Merge clusters that are too small (i.e. contain less than binSize / 2 samples).\n",
    "        # clustersTooSmall is the array of all clusters that are too small.\n",
    "        threshold = self.binSize / 2\n",
    "        binsTooSmall = np.where(binSizes < threshold)[0]\n",
    "        \n",
    "        if len(binsTooSmall) > 0:\n",
    "\n",
    "            # remove all centers from centersOld that are part of clustersTooSmall\n",
    "            centersNew = np.delete(centers, binsTooSmall, axis = 0)\n",
    "            centersTooSmall = centers[binsTooSmall]\n",
    "            centersNew_oldIndices = np.delete(np.arange(len(centers)), binsTooSmall)\n",
    "\n",
    "            KDTreeNew = KDTree(centersNew)\n",
    "            clustersToMerge = KDTreeNew.query(centersTooSmall)[1]\n",
    "\n",
    "            for i, clusterToMerge in enumerate(clustersToMerge):\n",
    "                indicesPerBin[centersNew_oldIndices[clusterToMerge]].extend(indicesPerBin[binsTooSmall[i]])\n",
    "\n",
    "            # Remove the indices given by clustersTooSmall from indicesPerBin by deleting the list entry\n",
    "            indicesPerBin = [np.array(indices) for binIndex, indices in enumerate(indicesPerBin) if binIndex not in binsTooSmall]\n",
    "            binSizes = [len(indices) for indices in indicesPerBin]\n",
    "            binSizes = pd.Series(binSizes)\n",
    "\n",
    "            self.centers = centersNew\n",
    "            self.binSizes = binSizes\n",
    "            self.kmeans = KDTreeNew\n",
    "        \n",
    "        else:\n",
    "            self.centers = centers\n",
    "            self.binSizes = pd.Series(binSizes)\n",
    "            self.kmeans = KDTree(self.centers)\n",
    "\n",
    "            # Transform the indices given by indicesPerBin into numpy arrays\n",
    "            indicesPerBin = [np.array(indices) for indices in indicesPerBin]\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.fitted = True\n",
    "        \n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def _getEqualSizedClusters(self,\n",
    "                               y,\n",
    "                               ):\n",
    "            \n",
    "        centers = self.centers.reshape(-1, 1, y.shape[-1]).repeat(self.binSize, 1).reshape(-1, y.shape[-1])\n",
    "\n",
    "        distance_matrix = cdist(y, centers)\n",
    "        clusterAssignments = linear_sum_assignment(distance_matrix)[1]//self.binSize\n",
    "\n",
    "        return clusterAssignments\n",
    "\n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X).astype(np.float32)\n",
    "        \n",
    "        if len(yPred.shape) == 1:\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        if self.equalBins:\n",
    "            binPerPred = self._getEqualSizedClusters(y = yPred)\n",
    "            \n",
    "        else:\n",
    "            binPerPred = self.kmeans.query(yPred)[1]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        neighborsList = [self.indicesPerBin[binIndex] for binIndex in binPerPred]\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f41ff",
   "metadata": {},
   "source": [
    "## LSx Multivariate Version with Theoretical Asymptotic Optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_multivariate_opt(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. \n",
    "    In this version of the LSx algorithm, we are grouping the point predictions of the samples specified via `X`\n",
    "    based on a k-means clustering algorithm. The number of clusters is determined by the `nClusters` parameter.  \n",
    "    In order to ensure theoretical asymptotic optimality of the algorithm, it has to be ensured that the number\n",
    "    of training observations receiving positive weight is at least minClusterSize, while minClusterSize has to be \n",
    "    an element of o(N) meaning minClusterSize / N -> 0 as N -> infinity.\n",
    "    To ensure this, each cluster is checked for its size and clusters being smaller than minClusterSize have to be\n",
    "    modified. For every cluster that is too small, we are recurvely searching for the closest other cluster until\n",
    "    the size of the combined cluster is at least minClusterSize. The clusters are not actually merged in the traditional\n",
    "    sense, though. Instead, we are creating new overlapping sets of samples that are used to compute the weights. \n",
    "    Let's say we have three clusters A, B and C, minClusterSize = 10, the sizes of the clusters are 4, 4 and 20. Furthermore,\n",
    "    assume B is closest to A and C closest to B. The set of indices are given then as follows:\n",
    "    A: A + B + C\n",
    "    B: B + C\n",
    "    C: C\n",
    "    This way it is ensured that the number of training observations receiving positive weight is at least 10 for every cluster. \n",
    "    At the same time, the above algorithm ensure that the distance of the samples receiving positive weight \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 nClusters: int=None, # Number of clusters being created while running fit.\n",
    "                 minClusterSize: int=None, # Minimum size of a cluster. If a cluster is smaller than this value, it will be merged with another cluster.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "        \n",
    "        # Check if binSize is int\n",
    "        if not isinstance(nClusters, int):\n",
    "            raise ValueError(\"'nClusters' must be an integer!\")\n",
    "        \n",
    "        # Check if minClusterSize is int\n",
    "        if not isinstance(minClusterSize, int):\n",
    "            raise ValueError(\"'minClusterSize' must be an integer!\")\n",
    "        \n",
    "        self.nClusters = nClusters\n",
    "        self.minClusterSize = minClusterSize\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to a k-means clustering algorithm. The number of clusters is determined by the `nClusters` parameter.\n",
    "        In order to ensure theoretical asymptotic optimality of the algorithm, it has to be ensured that the number\n",
    "        of training observations receiving positive weight is at least minClusterSize, while minClusterSize has to be\n",
    "        an element of o(N) meaning minClusterSize / N -> 0 as N -> infinity.\n",
    "        The specifics on how the clusters are created can be found in the class documentation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.nClusters is None:\n",
    "            raise ValueError(\"'nClusters' must be specified to fit the LSx estimator!\")\n",
    "        \n",
    "        if self.minClusterSize is None:\n",
    "            raise ValueError(\"'minClusterSize' must be specified to fit the LSx estimator!\")\n",
    "            \n",
    "        if self.nClusters > y.shape[0]:\n",
    "            raise ValueError(\"'nClusters' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if self.minClusterSize > y.shape[0]:\n",
    "            raise ValueError(\"'minClusterSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "        \n",
    "        #---\n",
    "\n",
    "        # Modify yPred to be compatible with faiss\n",
    "        yPredMod = yPred.astype(np.float32)\n",
    "        \n",
    "        # Train kmeans model based on the faiss library\n",
    "        kmeans = faiss.Kmeans(d = yPredMod.shape[1], k = self.nClusters)\n",
    "        kmeans.train(yPredMod)\n",
    "\n",
    "        # Get cluster centers created by faiss. IMPORTANT NOTE: not all clusters are used! We will handle that further below.\n",
    "        centers = kmeans.centroids\n",
    "        clusters = np.arange(centers.shape[0])\n",
    "        \n",
    "        # Compute the cluster assignment for each sample\n",
    "        clusterAssignments = kmeans.assign(yPredMod)[1]\n",
    "        \n",
    "        # Based on the clusters and cluster assignments, we can now compute the indices belonging to each bin / cluster\n",
    "        indicesPerBin = [[] for i in range(self.nClusters)]\n",
    "        clusterSizes = [0 for i in range(self.nClusters)]\n",
    "\n",
    "        for index, cluster in enumerate(clusterAssignments):\n",
    "            indicesPerBin[cluster].append(index)\n",
    "            clusterSizes[cluster] += 1\n",
    "\n",
    "        clusterSizes = np.array(clusterSizes)\n",
    "\n",
    "        # Just needed for a check in the end\n",
    "        maxSizeOfExistingClusters = np.max(clusterSizes)\n",
    "\n",
    "        #---\n",
    "\n",
    "        # clustersTooSmall is the array of all clusters that are too small.\n",
    "        clustersTooSmall = np.where(np.array(clusterSizes) < self.minClusterSize)[0]\n",
    "        \n",
    "        if len(clustersTooSmall) > 0:\n",
    "            \n",
    "            indicesPerBinNew = copy.deepcopy(indicesPerBin)\n",
    "\n",
    "            # We are searching for the closest other cluster for each cluster that is too small\n",
    "            # As we don't know how many nearest neighbors we need, we are setting k to the number of clusters\n",
    "            nearestClusters = KDTree(centers).query(centers[clustersTooSmall], k = centers.shape[0])[1]\n",
    "\n",
    "            # sizeNearestClusters is an array of shape (len(clustersTooSmall), self.nClusters)\n",
    "            sizeNearestClusters = clusterSizes[nearestClusters]\n",
    "\n",
    "            # Calculating the cumulative sum of the cluster sizes over each row allows us to find out \n",
    "            # which cluster is the first one that is big enough to make the current cluster big enough\n",
    "            clusterSizesCumSum = np.cumsum(sizeNearestClusters, axis = 1)\n",
    "\n",
    "            # argmax returns the first index where the condition is met.\n",
    "            necessaryClusters = (clusterSizesCumSum >= self.minClusterSize).argmax(axis = 1)\n",
    "            \n",
    "            # We are now creating the new indicesPerBin list by extending the indices of the clusters that are too small\n",
    "            for i, cluster in enumerate(clustersTooSmall):\n",
    "                clustersToAdd = nearestClusters[i, 0:necessaryClusters[i] + 1]\n",
    "                    \n",
    "                indicesPerBinNew[cluster] = np.concatenate([indicesPerBin[cluster] for cluster in clustersToAdd])\n",
    "                clusterSizes[cluster] = len(indicesPerBinNew[cluster])\n",
    "\n",
    "                # Following our intended logic, the resulting clusters can't be bigger than minClusterSize + maxSizeOfExistingClusters\n",
    "                if len(indicesPerBinNew[cluster]) > self.minClusterSize + maxSizeOfExistingClusters:\n",
    "                    raise Warning(\"The cluster size is bigger than minClusterSize + maxSizeOfExistingClusters. This should not happen!\")\n",
    "\n",
    "            # indicesPerBin is only turned into a dictionary to be consistent with the other implementations of LevelSetKDEx\n",
    "            self.indicesPerBin = {cluster: np.array(indicesPerBinNew[cluster], dtype = 'uintc') for cluster in range(len(indicesPerBinNew))}\n",
    "            self.clusterSizes = pd.Series(clusterSizes)\n",
    "        \n",
    "        else:\n",
    "            self.indicesPerBin = {cluster: np.array(indicesPerBin[cluster], dtype = 'uintc') for cluster in range(len(indicesPerBin))}\n",
    "            self.clusterSizes = pd.Series(clusterSizes)\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.centers = centers\n",
    "        self.kmeans = kmeans\n",
    "        self.fitted = True\n",
    "\n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X).astype(np.float32)\n",
    "        \n",
    "        if len(yPred.shape) == 1:\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "\n",
    "        clusterPerPred = self.kmeans.assign(yPred)[1]\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        neighborsList = [self.indicesPerBin[cluster] for cluster in clusterPerPred]\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831ecfe",
   "metadata": {},
   "source": [
    "## Level-Set Approach based on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_DT(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. \n",
    "    TBD.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 max_depth: int=8, # Maximum depth of the decision tree used to generate the bins.\n",
    "                 min_samples_leaf: int=100, # Minimum number of samples required to be in a bin.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "\n",
    "        # Check if max_depth is integer\n",
    "        if not isinstance(max_depth, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'max_depth' must be an integer!\")\n",
    "        \n",
    "        # Check if max_depth is bigger than 0\n",
    "        if max_depth <= 0:\n",
    "            raise ValueError(\"'max_depth' must be bigger than 0!\")\n",
    "        \n",
    "        # Check if min_samples_leaf is integer or float\n",
    "        if not isinstance(min_samples_leaf, (int, np.int32, np.int64, float, np.float32, np.float64)):\n",
    "            raise ValueError(\"'min_samples_leaf' must be an integer or float!\")\n",
    "        \n",
    "        # Check if min_samples_leaf is bigger than 0\n",
    "        if min_samples_leaf <= 0:\n",
    "            raise ValueError(\"'min_samples_leaf' must be bigger than 0!\")\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.drf = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: LevelSetKDEx_DT, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to their value. Samples are recursively sorted into bins until each bin contains\n",
    "        `binSize` many samples. For details, checkout the function `generateBins` which does the\n",
    "        heavy lifting.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if max_depth is integer\n",
    "        if not isinstance(self.max_depth, (int, np.int32, np.int64)):\n",
    "            raise ValueError(\"'max_depth' must be an integer!\")\n",
    "        \n",
    "        # Check if min_samples_leaf is integer or float\n",
    "        if not isinstance(self.min_samples_leaf, (int, np.int32, np.int64, float, np.float32, np.float64)):\n",
    "            raise ValueError(\"'min_samples_leaf' must be an integer or float!\")\n",
    "            \n",
    "        if self.min_samples_leaf > y.shape[0]:\n",
    "            raise ValueError(\"'min_samples_leaf' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        tree = DecisionTreeRegressor(max_depth = self.max_depth, min_samples_leaf = self.min_samples_leaf)\n",
    "\n",
    "        tree.fit(X = yPred, y = y)\n",
    "        leafIndicesTrain = tree.apply(yPred)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        self.yTrain = y.ravel()\n",
    "        \n",
    "        self.yPredTrain = yPred\n",
    "        self.tree = tree\n",
    "        self.leafIndicesTrain = leafIndicesTrain\n",
    "        self.fitted = True\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self: LevelSetKDEx_DT, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X)\n",
    "        leafIndicesTest = self.tree.apply(yPred)\n",
    "\n",
    "        weightsDataList = []\n",
    "\n",
    "        for leafIndex in leafIndicesTest:\n",
    "            leafComparison = (self.leafIndicesTrain == leafIndex) * 1\n",
    "            nObsInSameLeaf = np.sum(leafComparison)\n",
    "            weights = leafComparison / nObsInSameLeaf\n",
    "\n",
    "            weightsDataList.append((weights[weights > 0], np.where(weights > 0)[0]))\n",
    "        \n",
    "        #---\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30884783",
   "metadata": {},
   "source": [
    "## LSx Multivariate Gessaman Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ce15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class LevelSetKDEx_multivariate_bin(BaseWeightsBasedEstimator_multivariate, BaseLSx):\n",
    "    \"\"\"\n",
    "    `LevelSetKDEx` turns any point forecasting model into an estimator of the underlying conditional density.\n",
    "    The name 'LevelSet' stems from the fact that this approach interprets the values of the point forecasts\n",
    "    as a similarity measure between samples. \n",
    "    In this version of the LSx algorithm, we are applying the so-called Gessaman rule to create statistically\n",
    "    equivalent blocks of samples. In essence, the algorithm is a multivariate extension of the univariate\n",
    "    LevelSetKDEx algorithm based on bin-building. \n",
    "    We are creating equally sized bins of samples based on the point predictions of the samples specified via `X`\n",
    "    for every coordinate axis. Every bin of one axis is combined with the bins of all other axes resulting in\n",
    "    a total of nBinsPerDim^dim many bins. \n",
    "    Example: Let's say we have 100000 samples, the binSize is given as 20 and the number of dimension\n",
    "    is 3. As the binSize is given as 20, we want to create 5000 bins alltogether. Hence, there have to be\n",
    "    5000^(1/dim) = 5000^(1/3) = 17 bins per dimension. \n",
    "    IMPORTANT NOTE: The getWeights function is not yet finished and has to be completed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 estimator, # Model with a .fit and .predict-method (implementing the scikit-learn estimator interface).\n",
    "                 nBinsPerDim: int=None, # Number of samples belonging to each bin.\n",
    "                 ):\n",
    "        \n",
    "        super(BaseEstimator, self).__init__(estimator = estimator)\n",
    "        \n",
    "        # Check if nBinsPerDim is int\n",
    "        if not isinstance(nBinsPerDim, int):\n",
    "            raise ValueError(\"'binSize' must be an integer!\")\n",
    "        \n",
    "        self.nBinsPerDim = nBinsPerDim\n",
    "        \n",
    "        self.yTrain = None\n",
    "        self.yPredTrain = None\n",
    "        self.indicesPerBin = None\n",
    "        self.lowerBoundPerBin = None\n",
    "        self.fitted = False\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix used by `estimator` to predict `y`.\n",
    "            y: np.ndarray, # 1-dimensional target variable corresponding to the feature matrix `X`.\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Fit `LevelSetKDEx` model by grouping the point predictions of the samples specified via `X`\n",
    "        according to a simple binning rule called Gessaman rule based on the point predictions of the samples.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checks\n",
    "        if self.nBinsPerDim is None:\n",
    "            raise ValueError(\"'binSize' must be specified to fit the LSx estimator!\")\n",
    "        \n",
    "        if self.nBinsPerDim > y.shape[0]:\n",
    "            raise ValueError(\"'binSize' mustn't be bigger than the size of 'y'!\")\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"'X' and 'y' must contain the same number of samples!\")\n",
    "        \n",
    "        # IMPORTANT: In case 'y' is given as a pandas.Series, we can potentially run into indexing \n",
    "        # problems later on.\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.ravel()\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        try:\n",
    "            yPred = self.estimator.predict(X)\n",
    "            \n",
    "        except NotFittedError:\n",
    "            try:\n",
    "                self.estimator.fit(X = X, y = y)                \n",
    "            except:\n",
    "                raise ValueError(\"Couldn't fit 'estimator' with user specified 'X' and 'y'!\")\n",
    "            else:\n",
    "                yPred = self.estimator.predict(X)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "        \n",
    "        #---\n",
    "\n",
    "        \n",
    "\n",
    "        # We have to calculate the size of the bins for every coordinate axis\n",
    "        dim = yPred.shape[1]\n",
    "\n",
    "        for j in range(dim):\n",
    "\n",
    "            yPredDim = yPred[:, j]\n",
    "\n",
    "            if j == 0:\n",
    "                \n",
    "                binSize_firstAxis = int(np.ceil(yPredDim.shape[0] / self.nBinsPerDim))\n",
    "\n",
    "                indicesPerBin, lowerBounds = generateBins(binSize = binSize_firstAxis,\n",
    "                                                          yPred = yPredDim)\n",
    "                \n",
    "                indicesPerBin = {(bin, ): indices for bin, indices in indicesPerBin.items()}\n",
    "                lowerBounds = {(bin, ): [lowerBound] for bin, lowerBound in lowerBounds.items()}\n",
    "\n",
    "            else:\n",
    "                \n",
    "                indicesPerBin_ToAdd = {}\n",
    "                lowerBounds_ToAdd = {}\n",
    "\n",
    "                for bin in indicesPerBin.keys():\n",
    "\n",
    "                    yPredDim_bin = yPredDim[indicesPerBin[bin]]\n",
    "                    binSize_newAxis = int(np.ceil(yPredDim_bin.shape[0] / self.nBinsPerDim))\n",
    "                    \n",
    "                    indicesPerBin_newAxis, lowerBounds_newAxis = generateBins(binSize = binSize_newAxis,\n",
    "                                                                              yPred = yPredDim_bin)\n",
    "                    \n",
    "                    indicesPerBin_ToAdd.update({bin + (bin_new, ): indicesPerBin[bin][indices] for bin_new, indices in indicesPerBin_newAxis.items()})\n",
    "                    lowerBounds_ToAdd.update({bin + (bin_new, ): lowerBounds[bin] + [lowerBound] for bin_new, lowerBound in lowerBounds_newAxis.items()})\n",
    "\n",
    "                indicesPerBin = indicesPerBin_ToAdd\n",
    "                lowerBounds = lowerBounds_ToAdd\n",
    "\n",
    "        # Transform the indices given by indicesPerBin into numpy arrays\n",
    "        indicesPerBin = {bin: np.array(indices) for bin, indices in indicesPerBin.items()}\n",
    "\n",
    "        # Transform the lower bounds given by lowerBounds into a pandas dataframe\n",
    "        lowerBoundsDf = pd.DataFrame(lowerBounds).T\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.yTrain = y\n",
    "        self.yPredTrain = yPred\n",
    "        self.lowerBoundsDf = lowerBoundsDf\n",
    "        self.indicesPerBin = indicesPerBin\n",
    "        self.fitted = True\n",
    "\n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        # __annotations__ = BaseWeightsBasedEstimator.getWeights.__annotations__\n",
    "        __doc__ = BaseWeightsBasedEstimator_multivariate.getWeights.__doc__\n",
    "        \n",
    "        if not self.fitted:\n",
    "            raise NotFittedError(\"This LevelSetKDEx instance is not fitted yet. Call 'fit' with \"\n",
    "                                 \"appropriate arguments before trying to compute weights.\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        yPred = self.estimator.predict(X).astype(np.float32)\n",
    "        \n",
    "        if len(yPred.shape) == 1:\n",
    "            yPred = yPred.reshape(-1, 1)\n",
    "            \n",
    "        #---\n",
    "\n",
    "        # IMPORTANT NOTE: THE CODE BELOW IS NOT FINISHED YET. IT IS JUST A STARTING POINT.\n",
    "\n",
    "        lowerBounds_firstDim = self.lowerBoundsDf.iloc[:, 0]\n",
    "\n",
    "        # Filter lowerBounds_firstDim to only contain unique values\n",
    "        lowerBounds_firstDim = lowerBounds_firstDim.unique()\n",
    "\n",
    "        binPerPred_firstDim = np.searchsorted(a = lowerBounds_firstDim, v = yPred[:, 0], side = 'right') - 1\n",
    "\n",
    "        # The code has to be continued here. We have to find the correct bin for the second dimension, then the third dimension etc.\n",
    "        # It seems like we have to iterate over the observations unfortunately. Of course, there could be ways to do the search in\n",
    "        # batches, but the code would be much more complicated.\n",
    "        for ySingle in yPred:\n",
    "            for j in range(1, yPred.shape[1]):\n",
    "\n",
    "                print(j)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        neighborsList = [self.indicesPerBin[cluster] for cluster in clusterPerPred]\n",
    "        \n",
    "        weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "        \n",
    "        weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                  outputType = outputType, \n",
    "                                                                  y = self.yTrain,\n",
    "                                                                  scalingList = scalingList,\n",
    "                                                                  equalWeights = True)\n",
    "        \n",
    "        return weightsDataList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332347e2-6360-4271-8142-05b9f4d82d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8dc43-8377-41b9-a5a7-aa5904038841",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# import ipdb\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from datasetsDynamic.loadDataYaz import loadDataYaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34541b1d-b3a9-4310-ae57-0918cbd18c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 14,\n",
    "#                                                  daysToCut = 0,\n",
    "#                                                  normalizeDemand = True,\n",
    "#                                                  unstacked = True,\n",
    "#                                                  returnXY = True)\n",
    "\n",
    "# RF = RandomForestRegressor(n_estimators = 10, n_jobs = 1)\n",
    "# RF.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "# # Duplicate XTrain and yTrain m times\n",
    "# m = 1000\n",
    "# XTrain = np.vstack([XTrain for i in range(m)])\n",
    "# yTrain = np.vstack([yTrain for i in range(m)])\n",
    "\n",
    "# print(XTrain.shape)\n",
    "# print(yTrain.shape)\n",
    "\n",
    "# # Add gaussian to XTrain and yTrain\n",
    "# XTrain = XTrain + np.random.normal(0, 0.1, XTrain.shape)\n",
    "# yTrain = yTrain + np.random.normal(0, 0.1, yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d36f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSKDEx = LevelSetKDEx_multivariate_opt(estimator = RF, nClusters = 100, minClusterSize = 20)\n",
    "# LSKDEx.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "# yPred = LSKDEx.estimator.predict(XTest).astype(np.float32)\n",
    "# clusters = LSKDEx.kmeans.assign(yPred)[1]\n",
    "\n",
    "# weightsDataList = LSKDEx.getWeights(X = XTest, outputType='onlyPositiveWeights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers = LSKDEx.centers\n",
    "# yPred = LSKDEx.yPredTrain\n",
    "\n",
    "# distances = cdist(yPred, centers, metric = 'euclidean')\n",
    "\n",
    "# minCenters = np.argmin(distances, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e14eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 20 20 20 30 28 38 22 30 20 36 36 22 38]\n",
      "20\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "# nPosValues = np.array([len(weightsDataList[i][0]) for i in range(len(weightsDataList))])\n",
    "# print(nPosValues)\n",
    "\n",
    "# lenIndices = np.array([len(LSKDEx.indicesPerBin[i]) for i in range(len(LSKDEx.indicesPerBin))])\n",
    "# print(min(lenIndices))\n",
    "# print(max(lenIndices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
