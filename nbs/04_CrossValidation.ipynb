{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800fac0-995f-41b1-b679-bf1eb76d7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b1a36-300d-4661-a2b2-52f48f9647d7",
   "metadata": {},
   "source": [
    "# Cross Validation Functions\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddba5e3-2640-4630-a836-663301c6c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp crossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fea4d3-5da2-412e-9bd6-292e4a0cf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af4132-d4b9-40bd-b05a-0cc1d4ec3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "from sklearn.base import clone\n",
    "\n",
    "from dddex.baseClasses import BaseLSx\n",
    "from dddex.wSAA import SampleAverageApproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5baf3-bf86-48cb-a771-3e6102407c0c",
   "metadata": {},
   "source": [
    "## CV - Pinball Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69881de-9b70-4dec-9a02-ec2483a6572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class QuantileCrossValidation:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 # An object with a `predict` method that must (!) have an argument called `probs`\n",
    "                 # that specifies which quantiles to predict. Further, `estimator` needs\n",
    "                 # a `set_params` and `fit` method.\n",
    "                 estimator, \n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGrid: dict, \n",
    "                 randomSearch: bool=False, # Whether to use randomized search or grid search\n",
    "                 # Number of parameter settings that are sampled if `randomSearch == True`. \n",
    "                 # n_iter trades off runtime vs quality of the solution.\n",
    "                 nIter: int=None,\n",
    "                 # iterable of floats between 0 and 1. These determine the p-quantiles being predicted \n",
    "                 # to evaluate performance of each parameter setting.\n",
    "                 probs: list=[i / 100 for i in range(1, 100, 1)],\n",
    "                 # If True, for each p-quantile a fitted LSF with best binSize to predict it is returned. \n",
    "                 # Otherwise only one LSF is returned that is best over all probs.\n",
    "                 refitPerProb: bool=False, \n",
    "                 n_jobs: int=None, # Number of jobs to run in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None, \n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        # if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "        #     raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")               \n",
    "        \n",
    "        if np.any(np.array(probs) > 1) or np.any(np.array(probs) < 0): \n",
    "            raise ValueError(\"probs must only contain numbers between 0 and 1!\") \n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if randomSearch:\n",
    "            self.parameterGrid = list(ParameterSampler(param_distributions = parameterGrid,\n",
    "                                                       n_iter = nIter,\n",
    "                                                       random_state = random_state).__iter__())\n",
    "            \n",
    "            self.randomSearch = True\n",
    "            self.nIter = nIter\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGrid = ParameterGrid(parameterGrid)\n",
    "            self.randomSearch = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimator = copy.deepcopy(estimator)\n",
    "        self.cvFolds = cvFolds\n",
    "        self.probs = probs\n",
    "        self.refitPerProb = refitPerProb\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestParams_perProb = None\n",
    "        self.bestEstimator = None\n",
    "        self.bestEstimator_perProb = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab529a-a5e0-43fe-9572-cc1debfd096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: QuantileCrossValidation, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScore)(estimator = copy.deepcopy(self.estimator),\n",
    "                                                                         parameterGrid = self.parameterGrid,\n",
    "                                                                         cvFold = cvFold,\n",
    "                                                                         probs = self.probs,\n",
    "                                                                         X = X,\n",
    "                                                                         y = y) for cvFold in self.cvFolds)\n",
    "    \n",
    "    # scoresPerFold = [getFoldScore(estimator = copy.deepcopy(self.estimator),\n",
    "    #                               parameterGrid = self.parameterGrid,\n",
    "    #                               cvFold = cvFold,\n",
    "    #                               probs = self.probs,\n",
    "    #                               y = y,\n",
    "    #                               X = X) for cvFold in self.cvFolds]\n",
    "\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS OVER ALL PROBS\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    self.bestParams = paramsBest\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS PER PROB\n",
    "    paramsBestPerProbSeries = meanCostsDf.idxmin(axis = 0)\n",
    "    paramsBestPerProb = {prob: dict(zip(meanCostsDf.index.names, paramsBestPerProbSeries.loc[prob])) for prob in self.probs}\n",
    "    self.bestParams_perProb = paramsBestPerProb\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # REFITTING ESTIMATORS\n",
    "    if self.refitPerProb:      \n",
    "        \n",
    "        # GET UNIQUE PARAMETER COMBS OF LSx THAT ARE BEST FOR ANY PROB\n",
    "        paramsUnique = pd.DataFrame(paramsBestPerProb.values()).drop_duplicates().to_dict(orient = 'records')\n",
    "        estimatorDict = dict()\n",
    "        \n",
    "        for params in paramsUnique:\n",
    "            estimator = copy.deepcopy(self.estimator)\n",
    "            estimator.set_params(**params)\n",
    "            \n",
    "            estimator.fit(X = X, y = y)\n",
    "            estimatorDict[tuple(params.values())] = estimator\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # DICTIONARY OF THE BEST LSx-ESTIMATORS PER PROB\n",
    "        bestEstimatorPerProb = {prob: estimatorDict[tuple(paramsBestPerProb[prob].values())] for prob in self.probs}\n",
    "        self.bestEstimator_perProb = bestEstimatorPerProb\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    estimator = copy.deepcopy(self.estimator)\n",
    "    estimator.set_params(**paramsBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51812a-41e2-45c7-a653-ea662a3bff33",
   "metadata": {},
   "source": [
    "#### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188bccf-91fa-43b7-ac4c-1959437d6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScore(estimator, parameterGrid, cvFold, probs, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    quantilesDfSAA = SAA_fold.predict(X = XTestFold, probs = probs)\n",
    "    \n",
    "    costsDictSAA = {prob: getPinballLoss(decisions = quantilesDfSAA.loc[:, prob],\n",
    "                                         yTest = yTestFold,\n",
    "                                         prob = prob) for prob in probs}\n",
    "    #---\n",
    "    \n",
    "    # Necessary to ensure compatability with wSAA-models etc.\n",
    "    try:\n",
    "        estimator.refitPointEstimator(X = XTrainFold, \n",
    "                                      y = yTrainFold)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    costsPerParam = defaultdict(dict)\n",
    "\n",
    "    for params in parameterGrid:\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "\n",
    "        estimator.fit(X = XTrainFold,\n",
    "                      y = yTrainFold)\n",
    "\n",
    "        quantilesDf = estimator.predict(X = XTestFold,\n",
    "                                        probs = probs)\n",
    "        \n",
    "        costsDict = {prob: getPinballLoss(decisions = quantilesDf.loc[:, prob],\n",
    "                                          yTest = yTestFold,\n",
    "                                          prob = prob) for prob in probs}\n",
    "        \n",
    "        # We have to capture the special case of costSAA == 0, because then we can't compute the \n",
    "        # Cost-Ratio using the actual definition.\n",
    "        costsRatioDict = dict()\n",
    "        for prob in probs:\n",
    "            \n",
    "            if costsDictSAA[prob] > 0:\n",
    "                costRatio = costsDict[prob] / costsDictSAA[prob]\n",
    "            else:\n",
    "                if costDict[prob] == 0:\n",
    "                    costRatio = 0\n",
    "                else:\n",
    "                    costRatio = 1\n",
    "\n",
    "            costsRatioDict[prob] = costRatio\n",
    "\n",
    "        costsPerParam[tuple(params.values())] = costsRatioDict\n",
    "\n",
    "    #---\n",
    "\n",
    "    costsDf = pd.DataFrame.from_dict(costsPerParam, orient = 'index')\n",
    "    costsDf.index.names = list(params.keys())\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35127f5a-0da0-496a-b35f-15afa09d8865",
   "metadata": {},
   "source": [
    "## CV - Combined - Pinball Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e5b45f-82cc-4d3e-afa9-119fb5ad46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class QuantileCrossValidationLSx:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 estimatorLSx, # A Level-Set based model.\n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with LSx parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridLSx: dict,\n",
    "                 # dict or list of dicts with parameters names (`str`) of the point predictor as keys\n",
    "                 # and distributions or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridEstimator: dict,\n",
    "                 randomSearchLSx: bool=False, # Whether to use randomized search or grid search for the LSx parameters.\n",
    "                 # Whether to use randomized search or grid search for the point predictor parameters.\n",
    "                 randomSearchEstimator: bool=False, \n",
    "                 # Number of parameter settings of the LSx model that are sampled if `randomSearchLSx == True`. \n",
    "                 # LSx parameter settings are usually relatively cheap to evaluate, so all sampled LSx paramater settings\n",
    "                 # are evaluated for each point predictor parameter setting.\n",
    "                 nIterLSx: int=None,\n",
    "                 # Number of parameter settings of the underlying point predictor that are sampled if \n",
    "                 # `randomSearchEstimator == True`. nIterEstimator trades off runtime vs quality of the solution.\n",
    "                 nIterEstimator: int=None,\n",
    "                 # iterable of floats between 0 and 1. These determine the p-quantiles being predicted \n",
    "                 # to evaluate performance of each parameter setting.\n",
    "                 probs: list=[i / 100 for i in range(1, 100, 1)], \n",
    "                 # If True, for each p-quantile a fitted LSF with best binSize to predict it is returned. \n",
    "                 # Otherwise only one LSF is returned that is best over all probs.\n",
    "                 refitPerProb: bool=False, \n",
    "                 n_jobs: int=None, # number of folds being computed in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None,\n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "            raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")\n",
    "            \n",
    "        if len(parameterGridEstimator) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the point predictor. If you want to only evaluate\"\n",
    "                             \"parameter settings of the LSx-estimator itself, use the class `QuantileCrossValidation` instead or\"\n",
    "                             \"provide a fixed parameter setting for the point predictor via `parameterGridEstimator`.\")\n",
    "            \n",
    "        if len(parameterGridLSx) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the LSx model! If you want to only evaluate\"\n",
    "                             \"parameter settings of the point predictor, use standard cross-validation classes or instead\"\n",
    "                             \"provide a fixed parameter setting for the LS model via `parameterGridLSx`.\")\n",
    "            \n",
    "        if np.any(np.array(probs) > 1) or np.any(np.array(probs) < 0): \n",
    "            raise ValueError(\"`probs` must only contain numbers between 0 and 1!\")\n",
    "            \n",
    "        if len(probs) == 0:\n",
    "            raise ValueError(\"`probs` must be specified!\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if randomSearchLSx:\n",
    "            self.parameterGridLSx = list(ParameterSampler(param_distributions = parameterGridLSx,\n",
    "                                                          n_iter = nIterLSx,\n",
    "                                                          random_state = random_state).__iter__())\n",
    "            self.randomSearchLSx = True\n",
    "            self.nIterLsx = nIterLSx\n",
    "            self.random_state = random_state\n",
    "        \n",
    "        else:\n",
    "            self.parameterGridLSx = ParameterGrid(parameterGridLSx)\n",
    "            self.randomSearchLSx = False\n",
    "            \n",
    "        \n",
    "        if randomSearchEstimator:\n",
    "            \n",
    "            self.parameterGridEstimator = list(ParameterSampler(param_distributions = parameterGridEstimator,\n",
    "                                                                n_iter = nIterEstimator,\n",
    "                                                                random_state = random_state).__iter__())\n",
    "            self.randomSearchEstimator = True\n",
    "            self.nIterEstimator = nIterEstimator\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGridEstimator = ParameterGrid(parameterGridEstimator)\n",
    "            self.randomSearchEstimator = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimatorLSx = copy.deepcopy(estimatorLSx)\n",
    "        \n",
    "        self.cvFolds = cvFolds\n",
    "        self.probs = probs\n",
    "        self.refitPerProb = refitPerProb\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestParams_perProb = None\n",
    "        self.bestEstimator = None\n",
    "        self.bestEstimator_perProb = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3539a5-06db-4061-8ec9-611f9e232ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: QuantileCrossValidationLSx, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScoreLSx)(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "                                                                            parameterGridLSx = self.parameterGridLSx, \n",
    "                                                                            parameterGridEstimator = self.parameterGridEstimator,\n",
    "                                                                            cvFold = cvFold,\n",
    "                                                                            probs = self.probs,\n",
    "                                                                            y = y,\n",
    "                                                                            X = X) for cvFold in self.cvFolds)\n",
    "    \n",
    "    # scoresPerFold = [getFoldScoreLSx(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "    #                                  parameterGridLSx = self.parameterGridLSx,\n",
    "    #                                  parameterGridEstimator = self.parameterGridEstimator,\n",
    "    #                                  cvFold = cvFold,\n",
    "    #                                  probs = self.probs,\n",
    "    #                                  y = y,\n",
    "    #                                  X = X) for cvFold in self.cvFolds]\n",
    "\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS OVER ALL PROBS\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    \n",
    "    paramsLSxNames = self.estimatorLSx._get_param_names()\n",
    "    paramsLSxBest = {paramName: value for paramName, value in paramsBest.items() if paramName in paramsLSxNames}\n",
    "    paramsEstimatorBest = {paramName: value for paramName, value in paramsBest.items() if not paramName in paramsLSxNames}\n",
    "    \n",
    "    self.bestParams = paramsBest\n",
    "    self.bestParamsLSx = paramsLSxBest\n",
    "    self.bestParamsEstimator = paramsEstimatorBest\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS PER PROB\n",
    "    paramsBestPerProbSeries = meanCostsDf.idxmin(axis = 0)\n",
    "    paramsBestPerProb = {prob: dict(zip(meanCostsDf.index.names, paramsBestPerProbSeries.loc[prob])) for prob in self.probs}\n",
    "    \n",
    "    paramsLSxBestPerProb = {prob: {paramName: value for paramName, value in paramsBestPerProb[prob].items() \n",
    "                                   if paramName in paramsLSxNames} for prob in self.probs}\n",
    "    paramsEstimatorBestPerProb = {prob: {paramName: value for paramName, value in paramsBestPerProb[prob].items() \n",
    "                                   if not paramName in paramsLSxNames} for prob in self.probs}\n",
    "    \n",
    "    self.bestParams_perProb = paramsBestPerProb\n",
    "    self.bestParamsLSx_perProb = paramsBestPerProb\n",
    "    self.bestParamsEstimator_perProb = paramsBestPerProb\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # REFITTING ESTIMATORS\n",
    "    if self.refitPerProb:\n",
    "        \n",
    "        # GET UNIQUE PARAMETER COMBS OF ESTIMATOR THAT ARE BEST FOR ANY PROB\n",
    "        paramsEstimatorUnique = pd.DataFrame(paramsEstimatorBestPerProb.values()).drop_duplicates().to_dict(orient = 'records')\n",
    "        estimatorDict = dict()\n",
    "        \n",
    "        for params in paramsEstimatorUnique:\n",
    "            estimator = clone(self.estimatorLSx.estimator)\n",
    "            estimator.set_params(**params)\n",
    "            estimator.fit(X = X, y = y)\n",
    "            estimatorDict[tuple(params.values())] = estimator\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # GET UNIQUE PARAMETER COMBS OF LSx THAT ARE BEST FOR ANY PROB\n",
    "        paramsUnique = pd.DataFrame(paramsBestPerProb.values()).drop_duplicates().to_dict(orient = 'records')\n",
    "        \n",
    "        estimatorLSxDict = dict()\n",
    "        for params in paramsUnique:\n",
    "            paramsLSx = {paramName: value for paramName, value in params.items() if paramName in paramsLSxNames}\n",
    "            paramsEstimatorTuple = tuple(value for paramName, value in params.items() if not paramName in paramsLSxNames)\n",
    "            \n",
    "            estimator = estimatorDict[paramsEstimatorTuple]\n",
    "            estimatorLSx = copy.deepcopy(self.estimatorLSx)\n",
    "            \n",
    "            estimatorLSx.set_params(**paramsLSx, \n",
    "                                    estimator = estimator)\n",
    "            \n",
    "            estimatorLSx.fit(X = X, y = y)\n",
    "            estimatorLSxDict[tuple(params.values())] = estimatorLSx\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        # DICTIONARY OF THE BEST LSx-ESTIMATORS PER PROB\n",
    "        bestEstimatorPerProb = {prob: estimatorLSxDict[tuple(paramsBestPerProb[prob].values())] for prob in self.probs}\n",
    "        self.bestEstimator_perProb = bestEstimatorPerProb\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    estimatorLSx = copy.deepcopy(self.estimatorLSx)\n",
    "    \n",
    "    estimator = clone(estimatorLSx.estimator)\n",
    "    estimator.set_params(**paramsEstimatorBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "    \n",
    "    estimatorLSx.set_params(**paramsLSxBest,\n",
    "                            estimator = estimator)\n",
    "    estimatorLSx.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimatorLSx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcd6d0-7e5c-44c4-8916-22179a1d4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(CrossValidationLSx_combined.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971a422-b346-462d-81fd-79044fd43fc9",
   "metadata": {},
   "source": [
    "#### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1a2af-cb0c-4a3e-bc85-8900ac1cbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScoreLSx(estimatorLSx, parameterGridLSx, parameterGridEstimator, cvFold, probs, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    quantilesDfSAA = SAA_fold.predict(X = XTestFold, probs = probs)\n",
    "    \n",
    "    costsDictSAA = {prob: getPinballLoss(decisions = quantilesDfSAA.loc[:, prob],\n",
    "                                         yTest = yTestFold,\n",
    "                                         prob = prob) for prob in probs}\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    costsDfList = list()\n",
    "    \n",
    "    for paramsEstimator in parameterGridEstimator:\n",
    "        \n",
    "        estimatorLSx.refitPointEstimator(X = XTrainFold, \n",
    "                                         y = yTrainFold,\n",
    "                                         **paramsEstimator)\n",
    "\n",
    "        #---\n",
    "\n",
    "        costsPerParamLSx = defaultdict(dict)\n",
    "\n",
    "        for paramsLSx in parameterGridLSx:\n",
    "\n",
    "            estimatorLSx.set_params(**paramsLSx)\n",
    "\n",
    "            estimatorLSx.fit(X = XTrainFold,\n",
    "                             y = yTrainFold)\n",
    "\n",
    "            quantilesDf = estimatorLSx.predict(X = XTestFold,\n",
    "                                                 probs = probs)\n",
    "\n",
    "            #---\n",
    "            \n",
    "            costsDict = {prob: getPinballLoss(decisions = quantilesDf.loc[:, prob],\n",
    "                                              yTest = yTestFold,\n",
    "                                              prob = prob) for prob in probs}\n",
    "        \n",
    "            # We have to capture the special case of costSAA == 0, because then we can't compute the \n",
    "            # Cost-Ratio using the actual definition.\n",
    "            costsRatioDict = dict()\n",
    "            for prob in probs:\n",
    "\n",
    "                if costsDictSAA[prob] > 0:\n",
    "                    costRatio = costsDict[prob] / costsDictSAA[prob]\n",
    "                else:\n",
    "                    if costDict[prob] == 0:\n",
    "                        costRatio = 0\n",
    "                    else:\n",
    "                        costRatio = 1\n",
    "                        \n",
    "                costsRatioDict[prob] = costRatio\n",
    "            \n",
    "            costsPerParamLSx[tuple(paramsLSx.values())] = costsRatioDict\n",
    "\n",
    "        #---\n",
    "        \n",
    "        costsDf = pd.DataFrame.from_dict(costsPerParamLSx, orient = 'index')\n",
    "        \n",
    "        paramsLSxNames = list(paramsLSx.keys())\n",
    "        costsDf.index.names = paramsLSxNames\n",
    "\n",
    "        costsDf = costsDf.reset_index(drop = False)\n",
    "        for paramName, value in paramsEstimator.items():\n",
    "            costsDf[paramName] = value\n",
    "\n",
    "        paramNames = paramsLSxNames + list(paramsEstimator.keys())\n",
    "        costsDf = costsDf.set_index(paramNames)\n",
    "        \n",
    "        costsDfList.append(costsDf)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    costsDf = pd.concat(costsDfList, axis = 0)\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81b7de-bc53-4a11-a063-c8c5346ec221",
   "metadata": {},
   "source": [
    "## CV - Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aee1df-61c3-43ba-97d8-5f359b618753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DensityCrossValidation:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 # An object with a `predict` method that must (!) have an argument called `probs`\n",
    "                 # that specifies which quantiles to predict. Further, `estimator` needs\n",
    "                 # a `set_params` and `fit` method.\n",
    "                 estimator, \n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGrid: dict, \n",
    "                 randomSearch: bool=False, # Whether to use randomized search or grid search\n",
    "                 # Number of parameter settings that are sampled if `randomSearch == True`. \n",
    "                 # n_iter trades off runtime vs quality of the solution.\n",
    "                 nIter: int=None,\n",
    "                 p: int=1, # The order of the wasserstein distance to evaluate each hyperparameter settings.\n",
    "                 n_jobs: int=None, # Number of jobs to run in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None, \n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        # if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "        #     raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")               \n",
    "        \n",
    "        if not isinstance(p, int):\n",
    "            raise ValueError(\"`p` must be an integer between 1 and inf!\")\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        if randomSearch:\n",
    "            self.parameterGrid = list(ParameterSampler(param_distributions = parameterGrid,\n",
    "                                                       n_iter = nIter,\n",
    "                                                       random_state = random_state).__iter__())\n",
    "            \n",
    "            self.randomSearch = True\n",
    "            self.nIter = nIter\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGrid = ParameterGrid(parameterGrid)\n",
    "            self.randomSearch = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimator = copy.deepcopy(estimator)\n",
    "        self.cvFolds = cvFolds\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestEstimator = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d308d6-5319-4701-a696-7e758e98370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: DensityCrossValidation, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScore_wasserstein)(estimator = copy.deepcopy(self.estimator),\n",
    "    #                                                                                  parameterGrid = self.parameterGrid,\n",
    "    #                                                                                  cvFold = cvFold,\n",
    "    #                                                                                  p = self.p,\n",
    "    #                                                                                  X = X,\n",
    "    #                                                                                  y = y) for cvFold in self.cvFolds)\n",
    "    \n",
    "    scoresPerFold = [getFoldScore_wasserstein(estimator = copy.deepcopy(self.estimator),\n",
    "                                              parameterGrid = self.parameterGrid,\n",
    "                                              cvFold = cvFold,\n",
    "                                              p = self.p,\n",
    "                                              y = y,\n",
    "                                              X = X) for cvFold in self.cvFolds]\n",
    "    \n",
    "    # RESULTS\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    # BEST PARAMETER SETTING\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    self.bestParams = paramsBest\n",
    "    \n",
    "    # REFITTING ESTIMATOR WITH BEST PARAMETERS\n",
    "    estimator = copy.deepcopy(self.estimator)\n",
    "    estimator.set_params(**paramsBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7b904-8f36-43ec-8fea-a13881f85ca4",
   "metadata": {},
   "source": [
    "#### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8c349-039d-4364-a8ca-336c2ebf124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScore_wasserstein(estimator, parameterGrid, cvFold, p, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    densitiesSAA = SAA_fold.getWeights(X = XTestFold, outputType = 'onlyPositiveWeightsValues')\n",
    "    \n",
    "    wassersteinDistSAA = getWassersteinDistances(densities = densitiesSAA,\n",
    "                                                 yTest = yTestFold,\n",
    "                                                 p = p).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # Necessary to ensure compatability with wSAA-models etc.\n",
    "    try:\n",
    "        estimator.refitPointEstimator(X = XTrainFold, \n",
    "                                      y = yTrainFold)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    costsPerParam = defaultdict(dict)\n",
    "\n",
    "    for params in parameterGrid:\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "\n",
    "        estimator.fit(X = XTrainFold,\n",
    "                      y = yTrainFold)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        densities = estimator.getWeights(X = XTestFold,\n",
    "                                         outputType = 'onlyPositiveWeightsValues')\n",
    "        \n",
    "        wassersteinDist = getWassersteinDistances(densities = densities, \n",
    "                                                  yTest = yTestFold, \n",
    "                                                  p = p).sum()\n",
    "        \n",
    "        if wassersteinDistSAA > 0:\n",
    "            wassersteinRatio = wassersteinDist / wassersteinDistSAA\n",
    "        else:\n",
    "            if wassersteinDist == 0:\n",
    "                wassersteinRatio = 0\n",
    "            else:\n",
    "                wassersteinRatio = 1\n",
    "                \n",
    "        costsPerParam[tuple(params.values())] = {'wassersteinRatio': wassersteinRatio}\n",
    "\n",
    "    #---\n",
    "    # s = pd.Series(list(d.values()),index=pd.MultiIndex.from_tuples(d.keys()))\n",
    "    costsDf = pd.DataFrame.from_dict(costsPerParam, orient = 'index')\n",
    "    costsDf.index.names = list(params.keys())\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1cee6-9806-4618-8540-4e46a751076c",
   "metadata": {},
   "source": [
    "## CV - Combined - Wasserstein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10607ae7-135c-4534-9026-3327a13be0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DensityCrossValidationLSx:\n",
    "    \"\"\"\n",
    "    Class to efficiently tune the `binSize` parameter of all Level-Set based models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 estimatorLSx, # A Level-Set based model.\n",
    "                 cvFolds, # An iterable yielding (train, test) splits as arrays of indices.\n",
    "                 # dict or list of dicts with LSx parameters names (`str`) as keys and distributions\n",
    "                 # or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridLSx: dict,\n",
    "                 # dict or list of dicts with parameters names (`str`) of the point predictor as keys\n",
    "                 # and distributions or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "                 # method for sampling (such as those from scipy.stats.distributions).\n",
    "                 parameterGridEstimator: dict,\n",
    "                 randomSearchLSx: bool=False, # Whether to use randomized search or grid search for the LSx parameters.\n",
    "                 # Whether to use randomized search or grid search for the point predictor parameters.\n",
    "                 randomSearchEstimator: bool=False, \n",
    "                 # Number of parameter settings of the LSx model that are sampled if `randomSearchLSx == True`. \n",
    "                 # LSx parameter settings are usually relatively cheap to evaluate, so all sampled LSx paramater settings\n",
    "                 # are evaluated for each point predictor parameter setting.\n",
    "                 nIterLSx: int=None,\n",
    "                 # Number of parameter settings of the underlying point predictor that are sampled if \n",
    "                 # `randomSearchEstimator == True`. nIterEstimator trades off runtime vs quality of the solution.\n",
    "                 nIterEstimator: int=None,\n",
    "                 p: int=1, # The order of the wasserstein distance to evaluate each hyperparameter settings.\n",
    "                 n_jobs: int=None, # number of folds being computed in parallel.\n",
    "                 # Pseudo random number generator state used for random uniform sampling of parameter candidate values.\n",
    "                 # Pass an int for reproducible output across multiple function calls.\n",
    "                 random_state: int=None,\n",
    "                 ):\n",
    "        \n",
    "        # CHECKS  \n",
    "        if not isinstance(estimatorLSx, (BaseLSx)):\n",
    "            raise ValueError(\"'estimatorLSx' has to be a 'LevelSetKDEx', 'LevelSetKDEx_NN_new' or a 'LevelSetKDEx_kNN' object!\")\n",
    "            \n",
    "        if len(parameterGridEstimator) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the point predictor. If you want to only evaluate\"\n",
    "                             \"parameter settings of the LSx-estimator itself, use the class `QuantileCrossValidation` instead or\"\n",
    "                             \"provide a fixed parameter setting for the point predictor via `parameterGridEstimator`.\")\n",
    "            \n",
    "        if len(parameterGridLSx) == 0:\n",
    "            raise ValueError(\"No parameter candidates have been specified for the LSx model! If you want to only evaluate\"\n",
    "                             \"parameter settings of the point predictor, use standard cross-validation classes or instead\"\n",
    "                             \"provide a fixed parameter setting for the LS model via `parameterGridLSx`.\")\n",
    "            \n",
    "        if not isinstance(p, int):\n",
    "            raise ValueError(\"`p` must be an integer between 1 and inf!\")\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        if randomSearchLSx:\n",
    "            self.parameterGridLSx = list(ParameterSampler(param_distributions = parameterGridLSx,\n",
    "                                                          n_iter = nIterLSx,\n",
    "                                                          random_state = random_state).__iter__())\n",
    "            self.randomSearchLSx = True\n",
    "            self.nIterLsx = nIterLSx\n",
    "            self.random_state = random_state\n",
    "        \n",
    "        else:\n",
    "            self.parameterGridLSx = ParameterGrid(parameterGridLSx)\n",
    "            self.randomSearchLSx = False\n",
    "            \n",
    "        \n",
    "        if randomSearchEstimator:\n",
    "            \n",
    "            self.parameterGridEstimator = list(ParameterSampler(param_distributions = parameterGridEstimator,\n",
    "                                                                n_iter = nIterEstimator,\n",
    "                                                                random_state = random_state).__iter__())\n",
    "            self.randomSearchEstimator = True\n",
    "            self.nIterEstimator = nIterEstimator\n",
    "            self.random_state = random_state\n",
    "            \n",
    "        else:\n",
    "            self.parameterGridEstimator = ParameterGrid(parameterGridEstimator)\n",
    "            self.randomSearchEstimator = False\n",
    "            \n",
    "        #---\n",
    "        \n",
    "        self.estimatorLSx = copy.deepcopy(estimatorLSx)\n",
    "        \n",
    "        self.cvFolds = cvFolds\n",
    "        self.p = p\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.bestParams = None\n",
    "        self.bestEstimator = None\n",
    "        self.cvResults = None\n",
    "        self.cvResults_raw = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc2135-693e-4287-b9a0-c4a45bd5f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch\n",
    "def fit(self: DensityCrossValidationLSx, \n",
    "        X: np.ndarray, # Feature matrix (has to work with the folds specified via `cvFolds`)\n",
    "        y: np.ndarray, # Target values (has to work with the folds specified via `cvFolds`)\n",
    "        ): \n",
    "    \n",
    "    # Making sure that X and y are arrays to ensure correct subsetting via implicit indices.\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    scoresPerFold = Parallel(n_jobs = self.n_jobs)(delayed(getFoldScoreLSx_wasserstein)(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "                                                                                        parameterGridLSx = self.parameterGridLSx, \n",
    "                                                                                        parameterGridEstimator = self.parameterGridEstimator,\n",
    "                                                                                        cvFold = cvFold,\n",
    "                                                                                        p = self.p,\n",
    "                                                                                        y = y,\n",
    "                                                                                        X = X) for cvFold in self.cvFolds)\n",
    "    \n",
    "    # scoresPerFold = [getFoldScoreLSx_wasserstein(estimatorLSx = copy.deepcopy(self.estimatorLSx),\n",
    "    #                                              parameterGridLSx = self.parameterGridLSx,\n",
    "    #                                              parameterGridEstimator = self.parameterGridEstimator,\n",
    "    #                                              cvFold = cvFold,\n",
    "    #                                              p = self.p,\n",
    "    #                                              y = y,\n",
    "    #                                              X = X) for cvFold in self.cvFolds]\n",
    "\n",
    "    self.cvResults_raw = scoresPerFold\n",
    "    meanCostsDf = sum(scoresPerFold) / len(scoresPerFold)\n",
    "    self.cvResults = meanCostsDf\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    # BEST PARAMETER SETTINGS OVER ALL PROBS\n",
    "    meanCostsPerParam = meanCostsDf.mean(axis = 1)\n",
    "    paramsBest = meanCostsPerParam.index[np.argmin(meanCostsPerParam)]\n",
    "    paramsBest = dict(zip(meanCostsPerParam.index.names, paramsBest))\n",
    "    \n",
    "    paramsLSxNames = self.estimatorLSx._get_param_names()\n",
    "    paramsLSxBest = {paramName: value for paramName, value in paramsBest.items() if paramName in paramsLSxNames}\n",
    "    paramsEstimatorBest = {paramName: value for paramName, value in paramsBest.items() if not paramName in paramsLSxNames}\n",
    "    \n",
    "    self.bestParams = paramsBest\n",
    "    self.bestParamsLSx = paramsLSxBest\n",
    "    self.bestParamsEstimator = paramsEstimatorBest\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    estimatorLSx = copy.deepcopy(self.estimatorLSx)\n",
    "    \n",
    "    estimator = clone(estimatorLSx.estimator)\n",
    "    estimator.set_params(**paramsEstimatorBest)\n",
    "    estimator.fit(X = X, y = y)\n",
    "    \n",
    "    estimatorLSx.set_params(**paramsLSxBest,\n",
    "                            estimator = estimator)\n",
    "    estimatorLSx.fit(X = X, y = y)\n",
    "\n",
    "    self.bestEstimator = estimatorLSx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568ec34-a9b7-4fb0-8ef8-d989565dd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(CrossValidationLSx_combined.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43405f85-b449-4604-b6a6-3f5963b55a7f",
   "metadata": {},
   "source": [
    "#### Scores for Single Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0b146-803c-42bb-90d9-da652e9d0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function evaluates the newsvendor performance for different bin sizes for one specific fold.\n",
    "# The considered bin sizes\n",
    "\n",
    "def getFoldScoreLSx_wasserstein(estimatorLSx, parameterGridLSx, parameterGridEstimator, cvFold, p, X, y):\n",
    "    \n",
    "    indicesTrain = cvFold[0]\n",
    "    indicesTest = cvFold[1]\n",
    "    \n",
    "    yTrainFold = y[indicesTrain]\n",
    "    XTrainFold = X[indicesTrain]\n",
    "    \n",
    "    yTestFold = y[indicesTest]\n",
    "    XTestFold = X[indicesTest]\n",
    "    \n",
    "    #---\n",
    "\n",
    "    SAA_fold = SampleAverageApproximation()\n",
    "    SAA_fold.fit(y = yTrainFold)\n",
    "\n",
    "    densitiesSAA = SAA_fold.getWeights(X = XTestFold, outputType = 'onlyPositiveWeightsValues')\n",
    "    \n",
    "    wassersteinDistSAA = getWassersteinDistances(densities = densitiesSAA,\n",
    "                                                 yTest = yTestFold,\n",
    "                                                 p = p).sum()\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    costsDfList = list()\n",
    "    \n",
    "    for paramsEstimator in parameterGridEstimator:\n",
    "        \n",
    "        estimatorLSx.refitPointEstimator(X = XTrainFold, \n",
    "                                         y = yTrainFold,\n",
    "                                         **paramsEstimator)\n",
    "\n",
    "        #---\n",
    "\n",
    "        costsPerParamLSx = defaultdict(dict)\n",
    "\n",
    "        for paramsLSx in parameterGridLSx:\n",
    "\n",
    "            estimatorLSx.set_params(**paramsLSx)\n",
    "\n",
    "            estimatorLSx.fit(X = XTrainFold,\n",
    "                             y = yTrainFold)\n",
    "\n",
    "            densities = estimatorLSx.getWeights(X = XTestFold,\n",
    "                                                outputType = 'onlyPositiveWeightsValues')\n",
    "        \n",
    "            wassersteinDist = getWassersteinDistances(densities = densities, \n",
    "                                                      yTest = yTestFold, \n",
    "                                                      p = p).sum()\n",
    "\n",
    "            if wassersteinDistSAA > 0:\n",
    "                wassersteinRatio = wassersteinDist / wassersteinDistSAA\n",
    "            else:\n",
    "                if wassersteinDist == 0:\n",
    "                    wassersteinRatio = 0\n",
    "                else:\n",
    "                    wassersteinRatio = 1\n",
    "\n",
    "            costsPerParamLSx[tuple(paramsLSx.values())] = {'wassersteinRatio': wassersteinRatio}\n",
    "\n",
    "        #---\n",
    "        \n",
    "        costsDf = pd.DataFrame.from_dict(costsPerParamLSx, orient = 'index')\n",
    "        \n",
    "        paramsLSxNames = list(paramsLSx.keys())\n",
    "        costsDf.index.names = paramsLSxNames\n",
    "\n",
    "        costsDf = costsDf.reset_index(drop = False)\n",
    "        for paramName, value in paramsEstimator.items():\n",
    "            costsDf[paramName] = value\n",
    "\n",
    "        paramNames = paramsLSxNames + list(paramsEstimator.keys())\n",
    "        costsDf = costsDf.set_index(paramNames)\n",
    "        \n",
    "        costsDfList.append(costsDf)\n",
    "        \n",
    "    #---\n",
    "    \n",
    "    costsDf = pd.concat(costsDfList, axis = 0)\n",
    "    \n",
    "    return costsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fada52-3351-4323-b0f0-d529a995de89",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb64052-b2ba-4cad-969b-8ee841266d9f",
   "metadata": {},
   "source": [
    "### Get Pinball Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eee9cb-0c6c-4628-8fc3-f9539b57fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def getPinballLoss(decisions, yTest, prob):\n",
    "    \n",
    "    underageIndices = yTest > decisions\n",
    "    \n",
    "    underageCosts = prob * (yTest[underageIndices] - decisions[underageIndices]).sum()\n",
    "    overageCosts = (1 - prob) * (decisions[~underageIndices] - yTest[~underageIndices]).sum()\n",
    "    \n",
    "    return underageCosts + overageCosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff88d88-4610-47d4-9158-1134fa87ff2e",
   "metadata": {},
   "source": [
    "### Get Wasserstein Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5677535-b374-4d0e-94a3-b0cd7f0cabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def getWassersteinDistances(densities, yTest, p):\n",
    "    \n",
    "    wassersteinDists = list()\n",
    "    \n",
    "    for i in range(yTest.shape[0]):\n",
    "        y = yTest[i]\n",
    "        values = densities[i][1]\n",
    "        probs = densities[i][0]\n",
    "\n",
    "        if len(values.shape) == 1:\n",
    "            values = values.reshape(-1, 1)\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        wassersteinDists.append(np.sum(probs * np.sum(np.abs(values - y)**p, axis = 1))**(1/p))\n",
    "\n",
    "    return np.array(wassersteinDists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfac211-df8d-43cf-894f-b183eb3692c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# def getWassersteinDistances(densities, yTest, p):\n",
    "    \n",
    "#     # CHECK IF TARGET VALUES ARE ONE- OR MULTI-DIMENSIONAL\n",
    "#     # In the case of one-dimensional arrays, a faster algorithm that\n",
    "#     # doesn't iterate over the test observations can be used.\n",
    "#     if len(densities[0][1].shape) == 1:\n",
    "        \n",
    "#         lenList = list()\n",
    "#         valuesList = list()\n",
    "#         probsList = list()\n",
    "        \n",
    "#         for probs, values in densities:\n",
    "#             lenList.append(values.shape[0])\n",
    "#             probsList.append(probs)\n",
    "#             valuesList.append(values)\n",
    "            \n",
    "#         lenMax = max(lenList)   \n",
    "#         insertionCheck = np.arange(lenMax) < np.array(lenList)[:, None]\n",
    "\n",
    "#         probsArray = np.zeros(shape = (len(probsList), lenMax))\n",
    "#         valuesArray = np.zeros(shape = (len(valuesList), lenMax))\n",
    "\n",
    "#         probsArray[insertionCheck] = np.concatenate(probsList, axis = 0)\n",
    "#         valuesArray[insertionCheck] = np.concatenate(valuesList, axis = 0)\n",
    "#         yTest = yTest.reshape(-1, 1)\n",
    "\n",
    "#         wassersteinDists = np.sum(probsArray * np.abs(valuesArray - yTest)**p, axis = 1)**(1/p)\n",
    "        \n",
    "#         return wassersteinDists\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     else:\n",
    "#         wassersteinDists = list()\n",
    "    \n",
    "#         for i in range(yTest.shape[0]):\n",
    "#             y = yTest[i]\n",
    "#             values = densities[i][1]\n",
    "#             probs = densities[i][0]\n",
    "\n",
    "#             if len(values.shape) == 1:\n",
    "#                 values = values.reshape(-1, 1)\n",
    "#                 y = y.reshape(-1, 1)\n",
    "\n",
    "#             wassersteinDists.append(np.sum(probs * np.sum(np.abs(values - y)**p, axis = 1))**(1/p))\n",
    "\n",
    "#         return np.array(wassersteinDists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a628cfb-028a-4d7e-ae23-5e459995d6e3",
   "metadata": {},
   "source": [
    "### Grouped Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558baa5-750c-4809-9c73-81d22ff3603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function creates the cross-validation folds for every time series. Usually you'd want all test-observations \n",
    "# of each fold to refer to the same time period, but this is impossible to ensure in the case of the two-step models,\n",
    "# because the regression of the non-zero observations will always contain data of different time points. For that\n",
    "# reason, we refrain from trying to ensure this consistency.\n",
    "# Instead we organize our splits such that we always move a fixed amount of observations into the future from split\n",
    "# to split for every time series. This fixed amount of observations is currently set to the test length of the\n",
    "# corresponding time series.\n",
    "\n",
    "# In case this function is supposed to be used in the two-step case, data simply has to be filtered before hand\n",
    "# to only contain positive demand observations.\n",
    "\n",
    "def groupedTimeSeriesSplit(data, kFolds, testLength, groupFeature, timeFeature):\n",
    "    \n",
    "    # We reset the index because we have to access 'group.index' later on and\n",
    "    # want to make sure that we return the implicit numerical indices for our splits.\n",
    "    data = data.reset_index(drop = True)\n",
    "\n",
    "    dataGrouped = data.groupby(groupFeature)\n",
    "    splitNumbers = np.flip(np.array(range(kFolds)))\n",
    "    \n",
    "    foldsList = list()\n",
    "\n",
    "    for i in splitNumbers:\n",
    "\n",
    "        trainIndicesList = list()\n",
    "        valIndicesList = list()\n",
    "\n",
    "        valIndicesDict = dict()\n",
    "\n",
    "        for name, group in dataGrouped:\n",
    "\n",
    "            timeMin = int(group[timeFeature].min())\n",
    "            timeMax = int(group[timeFeature].max())\n",
    "\n",
    "            validationTimeMax = timeMax - i * testLength\n",
    "            trainTimeMax = validationTimeMax - testLength\n",
    "\n",
    "            trainTimesGroup = np.array(range(timeMin, trainTimeMax + 1))\n",
    "            valTimesGroup = np.array(range(trainTimeMax + 1, validationTimeMax + 1))\n",
    "\n",
    "            trainIndicesCheck = [timePoint in trainTimesGroup for timePoint in group[timeFeature]]\n",
    "            valIndicesCheck = [timePoint in valTimesGroup for timePoint in group[timeFeature]]\n",
    "\n",
    "            trainIndicesGroup = group.index[trainIndicesCheck]\n",
    "            valIndicesGroup = group.index[valIndicesCheck]\n",
    "\n",
    "            trainIndicesList.append(trainIndicesGroup)\n",
    "            valIndicesList.append(valIndicesGroup)\n",
    "\n",
    "        trainIndices = np.concatenate(trainIndicesList)\n",
    "        valIndices = np.concatenate(valIndicesList)\n",
    "        fold = (trainIndices, valIndices)\n",
    "\n",
    "        foldsList.append(fold)\n",
    "\n",
    "    return foldsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92782a-5fef-40fa-a501-01b0f5e99186",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4618d1-5d08-422c-a933-de29a16fc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# from dddex.levelSetKDEx_univariate import *\n",
    "# from dddex.loadData import *\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# import time\n",
    "# import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a84312-b5f5-4bbe-8a3b-8df14742ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, XTrain, yTrain, XTest, yTest = loadDataBakery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00188f25-e9c4-4ee0-9cfc-efc5d0c6cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM = LGBMRegressor(n_jobs = 1)\n",
    "# LGBM.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec950c1-d6d3-4f72-90db-d54ddffdf6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataTrain = data[data.label == 'train']\n",
    "\n",
    "# cvFolds = groupedTimeSeriesSplit(data = dataTrain, \n",
    "#                                  kFolds = 3, \n",
    "#                                  testLength = 28, \n",
    "#                                  groupFeature = 'id', \n",
    "#                                  timeFeature = 'dayIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac2395-da9b-4384-926b-3a6e15d9e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGridEstimator = {'num_leaves': [20, 40, 60, 80, 100, 150],\n",
    "#                       'max_depth': [-1, 3, 4, 5, 6, 7],\n",
    "#                       'min_child_samples': [2, 4, 6, 8, 10, 13, 16, 20, 25, 50, 75, 100, 150, 200, 300, 400],\n",
    "#                       'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "#                       'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "#                       'subsample': [0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1],\n",
    "#                       'colsample_bytree': [0.05, 0.1, 0.2, 0.3, 0.5, 0.75, 1]}\n",
    "\n",
    "# CVEstimator = RandomizedSearchCV(estimator = LGBM,\n",
    "#                                  cv = cvFolds,\n",
    "#                                  param_distributions = paramGridEstimator,\n",
    "#                                  scoring = 'neg_mean_squared_error',\n",
    "#                                  n_iter = 100,\n",
    "#                                  refit = True,\n",
    "#                                  n_jobs = len(cvFolds),\n",
    "#                                  return_train_score = True,\n",
    "#                                  verbose = 0)\n",
    "\n",
    "# CVEstimator.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99d3bd-5951-4043-bd80-bc8a30b6cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSKDEx = LevelSetKDEx(estimator = CVEstimator.best_estimator_)\n",
    "# LSKDEx = LevelSetKDEx(estimator = LGBM)\n",
    "\n",
    "# paramGridLSx = {'binSize': [10, 20, 35, 50, 75, 100, 150, 200, 250, 400, 600, 800, 1000],\n",
    "#                 'weightsByDistance': [True, False]}\n",
    "\n",
    "# CV = DensityCrossValidation(estimator = LSKDEx,\n",
    "#                             cvFolds = cvFolds,\n",
    "#                             parameterGrid = paramGridLSx,\n",
    "#                             n_jobs = len(cvFolds),\n",
    "#                             randomSearch = True,\n",
    "#                             nIter = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a7337-fddd-4390-9a1a-5bba3a2ebd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = ParameterSampler(param_distributions = paramGridLSx, n_iter = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc17feb-677a-4cbd-a18c-506ba4d2376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(ps.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bcbea6-959a-44e2-aef0-38eb3b01ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d897af8-d8fd-4170-a1ed-7702d2ea2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV.cvResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8d681-5c45-414e-b162-c84a42f65569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# import ipdb\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "# from dddex.loadData import *\n",
    "# from dddex.levelSetKDEx_univariate import LevelSetKDEx, BaseLSx\n",
    "# from dddex.wSAA import RandomForestWSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31d24d-457b-4473-9574-f5db4124cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataYaz(returnXY = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0126734-feeb-4e91-9ac3-31b269f591a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# LGBM = LGBMRegressor(boosting_type = 'gbdt',\n",
    "#                      n_jobs = 1)\n",
    "\n",
    "# LGBM.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef94795-2496-4528-9776-abe1833df5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# LSKDEx = LevelSetKDEx(estimator = LGBM, binSize = 100, weightsByDistance = False)\n",
    "# LSKDEx.fit(XTrain, yTrain)\n",
    "\n",
    "# densities = LSKDEx.getWeights(XTest, outputType = 'onlyPositiveWeightsValues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef340dbc-5c5c-4cf7-8d22-97647b08d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# dataTrain = data[data.label == 'train']\n",
    "\n",
    "# cvFolds = groupedTimeSeriesSplit(data = dataTrain, \n",
    "#                                  kFolds = 3, \n",
    "#                                  testLength = 28, \n",
    "#                                  groupFeature = 'id', \n",
    "#                                  timeFeature = 'dayIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab47f8-d170-4278-82c4-6805dcc3b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# CV = QuantileCrossValidationLSx(estimatorLSx = LSKDEx,\n",
    "#                                 cvFolds = cvFolds,\n",
    "#                                 parameterGridLSx = {'binSize': [10, 100, 400],\n",
    "#                                                    'weightsByDistance': [True, False]},\n",
    "#                                 parameterGridEstimator = {'max_depth': [3, 4],\n",
    "#                                                           'n_estimators': [150, 210]},\n",
    "#                                 randomSearchEstimator = True,\n",
    "#                                 nIterEstimator = 2,\n",
    "#                                 random_state = 44,\n",
    "#                                 probs = [0.01, 0.49, 0.5, 0.999],\n",
    "#                                 refitPerProb = True)\n",
    "\n",
    "# CV.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "# CV2 = QuantileCrossValidation(estimator = LSKDEx,\n",
    "#                               cvFolds = cvFolds,\n",
    "#                               parameterGrid = {'binSize': [10, 100, 400],\n",
    "#                                                'weightsByDistance': [True, False]},\n",
    "#                               randomSearch = False,\n",
    "#                               nIter = 1,\n",
    "#                               random_state = 4444,\n",
    "#                               probs = [0.01, 0.49, 0.5, 0.999],\n",
    "#                               refitPerProb = True)\n",
    "\n",
    "# CV2.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0bd343-a8e5-4ee5-975d-c12eccf61be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# RF = RandomForestWSAA()\n",
    "\n",
    "# CV3 = QuantileCrossValidation(estimator = RF,\n",
    "#                              cvFolds = cvFolds,\n",
    "#                              parameterGrid = {'max_depth': [3, 4],\n",
    "#                                               'n_estimators': [150, 210]},\n",
    "#                              probs = [0.01, 0.49, 0.5, 0.999],\n",
    "#                              refitPerProb = True)\n",
    "\n",
    "# CV3.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e77c34-ca94-4dd9-9d81-b276a0b64617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# CV4 = DensityCrossValidation(estimator = LSKDEx,\n",
    "#                             cvFolds = cvFolds,\n",
    "#                             parameterGrid = {'binSize': [10, 100, 400],\n",
    "#                                              'weightsByDistance': [True, False]},\n",
    "#                             randomSearch = True,\n",
    "#                             nIter = 5,\n",
    "#                             random_state = 44,\n",
    "#                             p = 2)\n",
    "\n",
    "# CV4.fit(XTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56734d6a-2731-4b6b-bd44-7e90f3d0cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# CV5 = DensityCrossValidationLSx(estimatorLSx = LSKDEx,\n",
    "#                                cvFolds = cvFolds,\n",
    "#                                parameterGridLSx = {'binSize': [10, 100, 400],\n",
    "#                                                   'weightsByDistance': [True, False]},\n",
    "#                                parameterGridEstimator = {'max_depth': [3, 4],\n",
    "#                                                          'n_estimators': [150, 210]},\n",
    "#                                randomSearchEstimator = True,\n",
    "#                                nIterEstimator = 2,\n",
    "#                                random_state = 44,\n",
    "#                                p = 2)\n",
    "\n",
    "# CV5.fit(X = XTrain, y = yTrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
