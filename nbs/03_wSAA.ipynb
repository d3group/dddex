{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9df2e7-fdc8-44e7-b16c-d8b15ed01db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbaad6-3667-46e6-b25e-efa0822452ae",
   "metadata": {},
   "source": [
    "# wSAA\n",
    "\n",
    "> Module description for wSAA classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c687b2c-941e-4e2a-9855-aa227ccb8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp wSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1faac0-5c0d-4c70-80e9-7d959811b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e786a-e401-43d8-91e9-97c175eba23a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f92e6-67ad-492b-9779-7b9acfc3d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from lightgbm.sklearn import LGBMModel\n",
    "from dddex.baseClasses import BaseWeightsBasedEstimator\n",
    "from dddex.utils import restructureWeightsDataList, restructureWeightsDataList_multivariate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bd1d-ceaa-4b36-a42e-aaa2e4062c22",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b775c4-ca57-4539-a0b8-e282bdf963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class RandomForestWSAA(RandomForestRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix\n",
    "            y: np.ndarray, # Target values\n",
    "            **kwargs):\n",
    "\n",
    "        super().fit(X = X, \n",
    "                    y = y, \n",
    "                    **kwargs)\n",
    "        \n",
    "        self.yTrain = y\n",
    "        \n",
    "        self.leafIndicesTrain = self.apply(X)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        leafIndicesDf = self.apply(X)\n",
    "        \n",
    "        weightsDataList = list()\n",
    "\n",
    "        for leafIndices in leafIndicesDf:\n",
    "            leafComparisonMatrix = (self.leafIndicesTrain == leafIndices) * 1\n",
    "            nObsInSameLeaf = np.sum(leafComparisonMatrix, axis = 0)\n",
    "\n",
    "            # It can happen that RF decides that the best strategy is to fit no tree at\n",
    "            # all and simply average all results (happens when min_child_sample is too high, for example).\n",
    "            # In this case 'leafComparisonMatrix' mustn't be averaged because there has been only a single tree.\n",
    "            if len(leafComparisonMatrix.shape) == 1:\n",
    "                weights = leafComparisonMatrix / nObsInSameLeaf\n",
    "            else:\n",
    "                weights = np.mean(leafComparisonMatrix / nObsInSameLeaf, axis = 1)\n",
    "\n",
    "            weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "            weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "        #---\n",
    "\n",
    "        # Check if self.yTrain is a 2D array with more than one column.\n",
    "        if len(self.yTrain.shape) > 1:\n",
    "            if self.yTrain.shape[1] > 1:\n",
    "\n",
    "                if not outputType in ['all', 'onlyPositiveWeights', 'summarized']:\n",
    "                    raise ValueError(\"outputType must be one of 'all', 'onlyPositiveWeights', 'summarized' for multivariate y.\")\n",
    "                \n",
    "                weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "                                                                        outputType = outputType, \n",
    "                                                                        y = self.yTrain, \n",
    "                                                                        scalingList = scalingList,\n",
    "                                                                        equalWeights = False) \n",
    "            \n",
    "        else:\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                        outputType = outputType, \n",
    "                                                        y = self.yTrain, \n",
    "                                                        scalingList = scalingList,\n",
    "                                                        equalWeights = False)\n",
    "            \n",
    "        \n",
    "                \n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self: BaseWeightsBasedEstimator, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ): \n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "        return super(MetaEstimatorMixin, self).predict(X = X,\n",
    "                                                       probs = probs, \n",
    "                                                       scalingList = scalingList)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self,\n",
    "                     X: np.ndarray, # Feature Matrix\n",
    "                     **kwargs):\n",
    "        \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "        return super().predict(X = X,\n",
    "                               **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfff03a",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export \n",
    "\n",
    "# # We attempt here to speed up the computation of the weights by interpreting every single\n",
    "# # tree as a lookup table. This way we don't have to compare the leaf-Indices arrays of each\n",
    "# # training sample and each test sample.\n",
    "# # Unfortunately, despite the fact that this strategy works very well for a single tree,\n",
    "# # it doesn't work for the whole forest because the structure of the output of the lookup \n",
    "# # tables per tree makes it difficult to aggregate the received weights per tree \n",
    "# # over all trees.\n",
    "\n",
    "# class RandomForestWSAA2(RandomForestRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "#     def fit(self, \n",
    "#             X: np.ndarray, # Feature matrix\n",
    "#             y: np.ndarray, # Target values\n",
    "#             **kwargs):\n",
    "\n",
    "#         super().fit(X = X, \n",
    "#                     y = y, \n",
    "#                     **kwargs)\n",
    "        \n",
    "#         self.yTrain = y\n",
    "        \n",
    "#         leafIndices = self.apply(X)\n",
    "\n",
    "#         indicesPerBinPerTree = list()\n",
    "\n",
    "#         for indexTree in range(self.n_estimators):\n",
    "#             leafIndicesPerTree = leafIndices[:, indexTree]\n",
    "\n",
    "#             indicesPerBin = defaultdict(list)\n",
    "\n",
    "#             for index, leafIndex in enumerate(leafIndicesPerTree):\n",
    "#                 indicesPerBin[leafIndex].append(index)\n",
    "\n",
    "#             indicesPerBinPerTree.append(indicesPerBin)\n",
    "        \n",
    "#         self.indicesPerBinPerTree = indicesPerBinPerTree\n",
    "\n",
    "        \n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def getWeights(self, \n",
    "#                    X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "#                    # Specifies structure of the returned density estimates. One of: \n",
    "#                    # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "#                    outputType: str='onlyPositiveWeights', \n",
    "#                    # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "#                    # density of each sample for scaling purposes.\n",
    "#                    scalingList: list=None, \n",
    "#                    ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "#         __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "#         #---\n",
    "        \n",
    "#         leafIndicesPerTree = self.apply(X)\n",
    "        \n",
    "#         weightsDataList = list()\n",
    "\n",
    "#         for leafIndices in leafIndicesPerTree:\n",
    "            \n",
    "#             weights = np.zeros(self.yTrain.shape[0])\n",
    "\n",
    "#             for indexTree in range(len(leafIndices)):\n",
    "#                 indicesPosWeight = self.indicesPerBinPerTree[indexTree][leafIndices[indexTree]]\n",
    "\n",
    "#                 weightsNew = np.zeros(self.yTrain.shape[0])\n",
    "#                 np.put(weightsNew, indicesPosWeight, 1 / len(indicesPosWeight))\n",
    "                \n",
    "#                 weights = weights + weightsNew\n",
    "\n",
    "#             weights = weights / len(leafIndices)\n",
    "\n",
    "#             weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "#             weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "#         #---\n",
    "\n",
    "#         # Check if self.yTrain is a 2D array with more than one column.\n",
    "#         if len(self.yTrain.shape) > 1:\n",
    "#             if self.yTrain.shape[1] > 1:\n",
    "\n",
    "#                 if not outputType in ['all', 'onlyPositiveWeights', 'summarized']:\n",
    "#                     raise ValueError(\"outputType must be one of 'all', 'onlyPositiveWeights', 'summarized' for multivariate y.\")\n",
    "                \n",
    "#                 weightsDataList = restructureWeightsDataList_multivariate(weightsDataList = weightsDataList, \n",
    "#                                                                         outputType = outputType, \n",
    "#                                                                         y = self.yTrain, \n",
    "#                                                                         scalingList = scalingList,\n",
    "#                                                                         equalWeights = False) \n",
    "            \n",
    "#         else:\n",
    "#             weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "#                                                         outputType = outputType, \n",
    "#                                                         y = self.yTrain, \n",
    "#                                                         scalingList = scalingList,\n",
    "#                                                         equalWeights = False)\n",
    "            \n",
    "        \n",
    "                \n",
    "\n",
    "#         return weightsDataList\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def predict(self: BaseWeightsBasedEstimator, \n",
    "#                 X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "#                 probs: list, # Probabilities for which quantiles are computed.\n",
    "#                 outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "#                 # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "#                 # of each sample to rescale values.\n",
    "#                 scalingList: list=None, \n",
    "#                 ): \n",
    "        \n",
    "#         __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "#         return super(MetaEstimatorMixin, self).predict(X = X,\n",
    "#                                                        probs = probs, \n",
    "#                                                        scalingList = scalingList)\n",
    "    \n",
    "#     #---\n",
    "    \n",
    "#     def pointPredict(self,\n",
    "#                      X: np.ndarray, # Feature Matrix\n",
    "#                      **kwargs):\n",
    "#         \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "#         return super().predict(X = X,\n",
    "#                                **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811aafa",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da815ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class RandomForestWSAA_LGBM(LGBMRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix\n",
    "            y: np.ndarray, # Target values\n",
    "            **kwargs):\n",
    "\n",
    "        super().fit(X = X, \n",
    "                    y = y, \n",
    "                    **kwargs)\n",
    "        \n",
    "        self.yTrain = y\n",
    "        \n",
    "        self.leafIndicesTrain = self.pointPredict(X, pred_leaf = True)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        leafIndicesDf = self.pointPredict(X, pred_leaf = True)\n",
    "        \n",
    "        weightsDataList = list()\n",
    "\n",
    "        for leafIndices in leafIndicesDf:\n",
    "            leafComparisonMatrix = (self.leafIndicesTrain == leafIndices) * 1\n",
    "            nObsInSameLeaf = np.sum(leafComparisonMatrix, axis = 0)\n",
    "\n",
    "            # It can happen that RF decides that the best strategy is to fit no tree at\n",
    "            # all and simply average all results (happens when min_child_sample is too high, for example).\n",
    "            # In this case 'leafComparisonMatrix' mustn't be averaged because there has been only a single tree.\n",
    "            if len(leafComparisonMatrix.shape) == 1:\n",
    "                weights = leafComparisonMatrix / nObsInSameLeaf\n",
    "            else:\n",
    "                weights = np.mean(leafComparisonMatrix / nObsInSameLeaf, axis = 1)\n",
    "\n",
    "            weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "            weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain, \n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = False)\n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self: BaseWeightsBasedEstimator, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ): \n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "        return super(LGBMModel, self).predict(X = X,\n",
    "                                             probs = probs, \n",
    "                                             scalingList = scalingList)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self,\n",
    "                     X: np.ndarray, # Feature Matrix\n",
    "                     **kwargs):\n",
    "        \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "        return super().predict(X = X,\n",
    "                               **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471bba6-50ec-49a9-980d-1c10f4361938",
   "metadata": {},
   "source": [
    "## SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b21e7-4c15-4c6d-8ebd-236740ec71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SampleAverageApproximation(BaseWeightsBasedEstimator):\n",
    "    \"\"\"SAA is a featureless approach that assumes the density of the target variable is given\n",
    "    by assigning equal probability to each historical observation of said target variable.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.yTrain = None\n",
    "        \n",
    "    #---\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"SAA()\"\n",
    "    __repr__ = __str__ \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: SAA, \n",
    "            y: np.ndarray, # Target values which form the estimated density function based on the SAA algorithm.\n",
    "            ):\n",
    "        self.yTrain = y\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray=None, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        # If no scaling is necessary, we can simply compute the data of the weights for a single observation and\n",
    "        # then simply duplicate it.\n",
    "        if (X is None) or (scalingList is None) or (outputType == 'onlyPositiveWeights'):\n",
    "            \n",
    "            neighborsList = [np.arange(len(self.yTrain))]\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "            \n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "            \n",
    "            if not X is None:\n",
    "                weightsDataList = weightsDataList * X.shape[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            neighborsList = [np.arange(len(self.yTrain))] * X.shape[0]\n",
    "\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self: SampleAverageApproximation, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ) -> np.ndarray: \n",
    "        \n",
    "        \"\"\"\n",
    "        Predict p-quantiles based on a reweighting of the empirical distribution function.\n",
    "        In comparison to all other weights-based approaches, SAA only needs to compute\n",
    "        the quantile predictions for one observation and then simply duplicate them.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # CHECKS\n",
    "        if isinstance(probs, int) or isinstance(probs, float):\n",
    "            if probs >= 0 and probs <= 1:\n",
    "                probs = [probs]\n",
    "            else:\n",
    "                raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")           \n",
    "                 \n",
    "        if any([prob > 1 or prob < 0 for prob in probs]):\n",
    "            raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")\n",
    "            \n",
    "        try:\n",
    "            probs = np.array(probs)\n",
    "        except:\n",
    "            raise ValueError(\"Can't convert `probs` to 1-dimensional array.\")\n",
    "        \n",
    "        #---\n",
    "\n",
    "        distributionData = self.getWeights(X = None,\n",
    "                                           outputType = 'cumulativeDistribution',\n",
    "                                           scalingList = None)        \n",
    "\n",
    "        # A tolerance term of 10^-8 is substracted from prob to account for rounding errors due to numerical precision.\n",
    "        quantileIndices = np.searchsorted(a = distributionData[0][0], v = probs - 10**-8, side = 'left')\n",
    "        quantiles = distributionData[0][1][quantileIndices]\n",
    "        \n",
    "        quantilesDf = pd.DataFrame([quantiles])\n",
    "        quantilesDf.columns = probs\n",
    "        \n",
    "        quantilesDf_duplicated = pd.concat([quantilesDf] * X.shape[0], axis = 0).reset_index(drop = True)\n",
    "        \n",
    "        if not scalingList is None:\n",
    "            quantilesDf_duplicated = (quantilesDf_duplicated.T * np.array(scalingList)).T\n",
    "        \n",
    "        return quantilesDf_duplicated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f57a0f-e4d4-4655-858c-d6319f087936",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# import lightgbm as lgb\n",
    "# from dddex.loadData import *\n",
    "# from datasetsDynamic.loadDataYaz import loadDataYaz\n",
    "# import ipdb\n",
    "# import inspect\n",
    "# from sklearn.base import RegressorMixin\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataBakery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50a254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72300, 227)\n",
      "(72300, 7)\n"
     ]
    }
   ],
   "source": [
    "# #| hide\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 14,\n",
    "#                                                  daysToCut = 0,\n",
    "#                                                  normalizeDemand = True,\n",
    "#                                                  unstacked = True,\n",
    "#                                                  returnXY = True)\n",
    "\n",
    "# # RF = RandomForestRegressor(n_estimators = 10, n_jobs = 1, max_depth = 3)\n",
    "# # RF.fit(X = XTrain, y = yTrain)\n",
    "\n",
    "# # Duplicate XTrain and yTrain m times\n",
    "# m = 100\n",
    "# XTrain = np.vstack([XTrain for i in range(m)])\n",
    "# yTrain = np.vstack([yTrain for i in range(m)])\n",
    "\n",
    "# print(XTrain.shape)\n",
    "# print(yTrain.shape)\n",
    "\n",
    "# # Add gaussian to XTrain and yTrain\n",
    "# XTrain = XTrain + np.random.normal(0, 0.1, XTrain.shape)\n",
    "# yTrain = yTrain + np.random.normal(0, 0.1, yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# RFWSAA = RandomForestWSAA(n_estimators = 10, n_jobs = 1, max_depth = 4)\n",
    "# RFWSAA.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a01f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# RFWSAA2 = RandomForestWSAA2(n_estimators = 10, max_depth = 4, n_jobs = 1)\n",
    "# RFWSAA2.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4515e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 2.92 s, total: 18.3 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# weights = RFWSAA.getWeights(X = XTrain[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58be9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 1.44 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# weights2 = RFWSAA2.getWeights(X = XTrain[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF.apply(XTrain).shape\n",
    "\n",
    "# indicesPerBinPerTree = list()\n",
    "\n",
    "# for indexTree, tree in enumerate(RF.estimators_):\n",
    "#     leafIndicesTrain = tree.apply(XTrain)\n",
    "\n",
    "#     indicesPerBin = defaultdict(list)\n",
    "\n",
    "#     for index, leafIndex in enumerate(leafIndicesTrain):\n",
    "#         indicesPerBin[leafIndex].append(index)\n",
    "\n",
    "#     indicesPerBinPerTree.append(indicesPerBin)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2880-1b9e-4d07-ab84-4ba8a2bef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| hide\n",
    "\n",
    "# RF = RandomForestWSAA_LGBM(max_depth = 2,\n",
    "#                            n_estimators = 10,\n",
    "#                            n_jobs = 1,\n",
    "#                            boosting_type = 'rf',\n",
    "#                            subsample_freq = 1,\n",
    "#                            subsample = 0.9)                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
