{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9df2e7-fdc8-44e7-b16c-d8b15ed01db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfbaad6-3667-46e6-b25e-efa0822452ae",
   "metadata": {},
   "source": [
    "# wSAA\n",
    "\n",
    "> Module description for wSAA classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c687b2c-941e-4e2a-9855-aa227ccb8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp wSAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1faac0-5c0d-4c70-80e9-7d959811b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "# from nbdev.qmd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e786a-e401-43d8-91e9-97c175eba23a",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f92e6-67ad-492b-9779-7b9acfc3d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastcore.docments import *\n",
    "from fastcore.test import *\n",
    "from fastcore.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from dddex.baseClasses import BaseWeightsBasedEstimator\n",
    "from dddex.utils import restructureWeightsDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331bd1d-ceaa-4b36-a42e-aaa2e4062c22",
   "metadata": {},
   "source": [
    "## wSAA - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b775c4-ca57-4539-a0b8-e282bdf963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class RandomForestWSAA(RandomForestRegressor, BaseWeightsBasedEstimator):\n",
    "    \n",
    "    def fit(self, \n",
    "            X: np.ndarray, # Feature matrix\n",
    "            y: np.ndarray, # Target values\n",
    "            **kwargs):\n",
    "\n",
    "        super().fit(X = X, \n",
    "                    y = y, \n",
    "                    **kwargs)\n",
    "        \n",
    "        self.yTrain = y\n",
    "        \n",
    "        n_jobs = copy.deepcopy(self.get_params()['n_jobs'])\n",
    "        \n",
    "        self.set_params(n_jobs = 1)\n",
    "        \n",
    "        self.leafIndicesTrain = self.apply(X)\n",
    "        \n",
    "        self.set_params(n_jobs = n_jobs)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        n_jobs = copy.deepcopy(self.get_params()['n_jobs'])\n",
    "        \n",
    "        self.set_params(n_jobs = 1)\n",
    "        \n",
    "        leafIndicesDf = self.apply(X)\n",
    "        \n",
    "        self.set_params(n_jobs = n_jobs)\n",
    "        \n",
    "        #---\n",
    "        \n",
    "        weightsDataList = list()\n",
    "\n",
    "        for leafIndices in leafIndicesDf:\n",
    "            leafComparisonMatrix = (self.leafIndicesTrain == leafIndices) * 1\n",
    "            nObsInSameLeaf = np.sum(leafComparisonMatrix, axis = 0)\n",
    "\n",
    "            # It can happen that RF decides that the best strategy is to fit no tree at\n",
    "            # all and simply average all results (happens when min_child_sample is too high, for example).\n",
    "            # In this case 'leafComparisonMatrix' mustn't be averaged because there has been only a single tree.\n",
    "            if len(leafComparisonMatrix.shape) == 1:\n",
    "                weights = leafComparisonMatrix / nObsInSameLeaf\n",
    "            else:\n",
    "                weights = np.mean(leafComparisonMatrix / nObsInSameLeaf, axis = 1)\n",
    "\n",
    "            weightsPosIndex = np.where(weights > 0)[0]\n",
    "\n",
    "            weightsDataList.append((weights[weightsPosIndex], weightsPosIndex))\n",
    "\n",
    "        #---\n",
    "\n",
    "        weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                     outputType = outputType, \n",
    "                                                     y = self.yTrain, \n",
    "                                                     scalingList = scalingList,\n",
    "                                                     equalWeights = False)\n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self : BaseWeightsBasedEstimator, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                outputAsDf: bool=True, # Determines output. Either a dataframe with probs as columns or a dict with probs as keys.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ): \n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.predict.__doc__\n",
    "        \n",
    "        return super(MetaEstimatorMixin, self).predict(X = X,\n",
    "                                                       probs = probs, \n",
    "                                                       scalingList = scalingList)\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def pointPredict(self,\n",
    "                     X: np.ndarray, # Feature Matrix\n",
    "                     **kwargs):\n",
    "        \"\"\"Original `predict` method to generate point forecasts\"\"\"\n",
    "        \n",
    "        return super().predict(X = X,\n",
    "                               **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db18d0f-bdb7-4f08-a8cf-ac07a49e7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a240e3b-5acc-4f82-8f4d-29391f5692b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe067-f51e-4038-ae00-06bafa7ca014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(RandomForestWSAA.getWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471bba6-50ec-49a9-980d-1c10f4361938",
   "metadata": {},
   "source": [
    "## SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b21e7-4c15-4c6d-8ebd-236740ec71e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseWeightsBasedEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSampleAverageApproximation\u001b[39;00m(\u001b[43mBaseWeightsBasedEstimator\u001b[49m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124;03m\"\"\"SAA is a featureless approach that assumes the density of the target variable is given\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    by assigning equal probability to each historical observation of said target variable.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseWeightsBasedEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "class SampleAverageApproximation(BaseWeightsBasedEstimator):\n",
    "    \"\"\"SAA is a featureless approach that assumes the density of the target variable is given\n",
    "    by assigning equal probability to each historical observation of said target variable.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.yTrain = None\n",
    "        \n",
    "    #---\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"SAA()\"\n",
    "    __repr__ = __str__ \n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def fit(self: SAA, \n",
    "            y: np.ndarray, # Target values which form the estimated density function based on the SAA algorithm.\n",
    "            ):\n",
    "        self.yTrain = y\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def getWeights(self, \n",
    "                   X: np.ndarray=None, # Feature matrix for which conditional density estimates are computed.\n",
    "                   # Specifies structure of the returned density estimates. One of: \n",
    "                   # 'all', 'onlyPositiveWeights', 'summarized', 'cumDistribution', 'cumDistributionSummarized'\n",
    "                   outputType: str='onlyPositiveWeights', \n",
    "                   # Optional. List with length X.shape[0]. Values are multiplied to the estimated \n",
    "                   # density of each sample for scaling purposes.\n",
    "                   scalingList: list=None, \n",
    "                   ) -> list: # List whose elements are the conditional density estimates for the samples specified by `X`.\n",
    "        \n",
    "        __doc__ = BaseWeightsBasedEstimator.getWeights.__doc__\n",
    "        \n",
    "        # If no scaling is necessary, we can simply compute the data of the weights for a single observation and\n",
    "        # then simply duplicate it.\n",
    "        if (X is None) or (scalingList is None) or (outputType == 'onlyPositiveWeights'):\n",
    "            \n",
    "            neighborsList = [np.arange(len(self.yTrain))]\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "            \n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "            \n",
    "            if not X is None:\n",
    "                weightsDataList = weightsDataList * X.shape[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            neighborsList = [np.arange(len(self.yTrain))] * X.shape[0]\n",
    "\n",
    "            weightsDataList = [(np.repeat(1 / len(neighbors), len(neighbors)), np.array(neighbors)) for neighbors in neighborsList]\n",
    "\n",
    "            weightsDataList = restructureWeightsDataList(weightsDataList = weightsDataList, \n",
    "                                                         outputType = outputType, \n",
    "                                                         y = self.yTrain,\n",
    "                                                         scalingList = scalingList,\n",
    "                                                         equalWeights = True)\n",
    "\n",
    "        return weightsDataList\n",
    "    \n",
    "    #---\n",
    "    \n",
    "    def predict(self : SampleAverageApproximation, \n",
    "                X: np.ndarray, # Feature matrix for which conditional quantiles are computed.\n",
    "                probs: list, # Probabilities for which quantiles are computed.\n",
    "                # Optional. List with length X.shape[0]. Values are multiplied to the predictions\n",
    "                # of each sample to rescale values.\n",
    "                scalingList: list=None, \n",
    "                ) -> np.ndarray: \n",
    "        \n",
    "        \"\"\"\n",
    "        Predict p-quantiles based on a reweighting of the empirical distribution function.\n",
    "        In comparison to all other weights-based approaches, SAA only needs to compute\n",
    "        the quantile predictions for one observation and then simply duplicate them.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # CHECKS\n",
    "        if isinstance(probs, int) or isinstance(probs, float):\n",
    "            if probs >= 0 and probs <= 1:\n",
    "                probs = [probs]\n",
    "            else:\n",
    "                raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")           \n",
    "                 \n",
    "        if any([prob > 1 or prob < 0 for prob in probs]):\n",
    "            raise ValueError(\"The values specified via 'probs' must lie between 0 and 1!\")\n",
    "            \n",
    "        try:\n",
    "            probs = np.array(probs)\n",
    "        except:\n",
    "            raise ValueError(\"Can't convert `probs` to 1-dimensional array.\")\n",
    "        \n",
    "        #---\n",
    "\n",
    "        distributionData = self.getWeights(X = None,\n",
    "                                           outputType = 'cumulativeDistribution',\n",
    "                                           scalingList = None)        \n",
    "\n",
    "        # A tolerance term of 10^-8 is substracted from prob to account for rounding errors due to numerical precision.\n",
    "        quantileIndices = np.searchsorted(a = distributionData[0][0], v = probs - 10**-8, side = 'left')\n",
    "        quantiles = distributionData[0][1][quantileIndices]\n",
    "        \n",
    "        quantilesDf = pd.DataFrame([quantiles])\n",
    "        quantilesDf.columns = probs\n",
    "        \n",
    "        quantilesDf_duplicated = pd.concat([quantilesDf] * X.shape[0], axis = 0).reset_index(drop = True)\n",
    "        \n",
    "        if not scalingList is None:\n",
    "            quantilesDf_duplicated = (quantilesDf_duplicated.T * np.array(scalingList)).T\n",
    "        \n",
    "        return quantilesDf_duplicated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6774387-0636-452e-842e-bc998a834395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc67de5-0fcb-47b3-82dd-908cbedaae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9520933-6480-4e96-bace-8b9f164cd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e77e0d-5cb8-4c29-b356-0f0e61bccde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(SampleAverageApproximation.getWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79762a-1427-49d7-b8b2-42ecde48e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ccab9-da3c-4bb1-ad35-9cfcdade92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f57a0f-e4d4-4655-858c-d6319f087936",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f055de-89ce-4943-8242-8f434ee8a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from dddex.loadData import *\n",
    "\n",
    "# data, XTrain, yTrain, XTest, yTest = loadDataYaz(testDays = 14, \n",
    "#                                                  returnXY = True,\n",
    "#                                                  daysToCut = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd2880-1b9e-4d07-ab84-4ba8a2bef226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# RF = RandomForestWSAA(max_depth = 2,\n",
    "#                       n_estimators = 10,\n",
    "#                       n_jobs = 1)\n",
    "\n",
    "# RF.fit(X = XTrain, y = yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533bbef-fe12-4600-85a7-07cd9dc05fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# RF.predict(XTest, probs = [0.5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
